,company,job_description,job_link,job_location,job_title,search_position
0,PCS Global Tech,"ResponsibilitiesAnalyze and organize raw dataBuild data systems and pipelinesEvaluate business needs and objectivesInterpret trends and patternsConduct complex data analysis and report on resultsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesCombine raw information from different sourcesExplore ways to enhance data quality and reliabilityIdentify opportunities for data acquisitionDevelop analytical tools and programsCollaborate with data scientists and architects on several projectsRequirementsPrevious experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, and segmentation techniquesKnowledge of programming languages (e.g. Java and Python)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT, or similar field; a Master’s is a plusJob Types: Full-time, ContractSalary: $70,000.00 - $80,000.00 per yearJob Types: Full-time, ContractSalary: $70,000.00 - $80,000.00 per yearBenefits:Dental insuranceEmployee assistance programHealth insuranceProfessional development assistanceRelocation assistanceVision insuranceSchedule:8 hour shiftMonday to FridayWork Location: Multiple Locations",https://www.indeed.com/company/PCS-GLOBAL-TECH/jobs/Entry-Level-Opportunity-c407b519368d0ed9?fccid=252beafbd99a6037&vjs=3,"Boston, MA+1 location",Entry level Opportunity :: Data Engineer Entry level Opportunity,Data Engineer
1,First Soft Solutions [ Direct],"Data Engineer (or) Analyst with hands on experience working on various data sources which include Sybase, Splunk and SQL.Candidate must have experience with ETLProficient in a scripting language such as Python, Perl or shellDevelop procedures and functions (Automate) of ETL from various data sources using Python.Contribute to architecture and automation discussions across Network Operations.Support and maintain current build-out of Operational Intelligence system, including three environments and over 350 virtual machinesExperience in AI/ML is a huge plusJob Types: Full-time, ContractPay: $35.00 - $77.00 per hourSchedule:Monday to FridayAbility to commute/relocate:Greenwood Village, CO: Reliably commute or planning to relocate before starting work (Preferred)Experience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/rc/clk?cmp=First-Soft-Solutions-%5B-Direct%5D&ti=Data+Engineer&jk=0d764b476f634788&fccid=6dd0aeac1ec043d4&vjs=3,"Greenwood Village, CO+1 location",Data Engineer,Data Engineer
2,TekwissenGroup,"Overview: TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. Our client is a company operating a marketplace for consumers, sellers, and content creators. It offers merchandise and content purchased for resale from vendors and those offered by third-party sellers.Job Description: Client is looking for a Data Engineer (DE) to join the Robotics Advanced Technology (AT) the Digital Information Services (DIS) team. AT is responsible for delivering automated solutions to various process paths within the Client Fulfillment Network.The Digital Information Services (DIS) team provides visibility to strategic planning , development, and operational insights of AT products throughout their lifecycle.We reinforce the rhythm of the business and use data as an instrument to inspire confidence and enable our customers to make effective, data-driven decisions in real-time.The DE will support in the design and implementation of data solutions that automate and enable various value streams within the AT business.Responsibilities include: Create new and refine on existing data pipelines, including ETL or ELT practicesManipulate and mine data from database tables, e.g. Redshift, MySQL, using SQLFacilitate in data gap analysis and stitching/blending exercisesBuild and improve existing data streams to ensure data integrity and efficient analysesAutomate human intervention via scriptingAssist in developing reporting and visualizing data insightsProvide fact-based insights into variances and trendsOnboard and maintain tables to internal tools/platformsCreate and refine on respective process documentation as changes are realizedCommunicate announcements regarding new processes and enhancementsMonitor, respond, and resolve trouble ticketing queueIdentify opportunities for improving customer experienceStory Behind the Need – Business Group & Key Projects: Group: Business solutions services team. Deploy maintain and integrated suite of services. Enable folks with AT to do their job. Research, procure, developBI Verticals with in team. Deploy automates solutions to Client network. Research $ development. Prototyping, decide if things are released to productions. Automation in business process. Newish team. Introducing technologies to business.Reason/motivation for request: Ramping up quickly, need help with work load. Immediate need for reporting stream built & deployed to stake holders.Typical Day in the Role: Interaction with team/Day to Day: Team is nontraditional. New management.Leading different verticals, Application services, BI,Maintain new services AWS systems working.Help with requirements gathering & release managementCodependent teamWork individually.Will work closely with HMAttend workshopsWill interact with Product & program managementCompelling Story & Candidate Value Proposition: Role interesting: Impact & influence AT industryNew InnovationBuilding data for advanced technologyEnable business to make right data driven decisions.Assist associates make safer technologyReduce unsafe machine/technology usage in various job functions.Possible conversionCandidate Requirements: REQUIRED SKILLS Builder, Design, Architecting (SME) – No support for this skill set.Data Engineer at least 5 years.Pull information from systems Study & stage infoELT or ETL (proficient)Advanced data modeling.ScriptingAutomatingUsing BI tools & visualization tool, (show data, metrics,Tableau, Power BI, Client quick site. Able to switch from tool to tool.Visualize data & analyze dataData base architecting, Client redshift & my sequelBachelor's degree in Computer Science, Engineering, Mathematics, or a related technical disciplinePreferred Skills: Agile methodologySoftware developmentInfluence leadership & managementReportingCommunication skillsAble to mentor others on data management and engineering.Leadership principles: OwnershipInvent & Simplify: Find creative ways to simplify dataAre right a lotLearn & Be curious: learn about business & how to benefit customersDeliver resultsThink Big: Understand next steps we could be taking, be impactful.Experience: 5+ years of experience as a Data Engineer, BI Engineer, or similar roleTop 3 must-have hard skills: Design, architect, implement - 5 years’ experienceSQL Expert – A lot of query work (5 years)Visualization – assisting with reporting.Knowledge of BI tools (3 years)TekWissen® Group is an equal opportunity/affirmative action Employer (m/f/d/v) supporting workforce diversity. Job Types: Full-time, ContractPay: Up to $66.00 per hourEducation:Bachelor's (Preferred)Experience:Data Engineer, BI Engineer, or similar role: 1 year (Preferred)SQL Expert – A lot of query work: 1 year (Preferred)BI tools: 1 year (Preferred)ELT or ETL (proficient): 1 year (Preferred)Database architecting, Client redshift & my sequel: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/TekWissenGroup/jobs/Data-Engineer-404dfe48d4fc1402?fccid=37a6d008193135ea&vjs=3,"Seattle, WA",Data Engineer II,Data Engineer
3,Common Room Job Posts,"About Us:

Common Room brings companies closer to their communities. Today's most successful companies are driven by thriving communities of end users. As a result, companies of all sizes are investing in their communities, but software hasn't caught up yet. Until now. We're building the first community intelligence platform that identifies the people, topics, and organizations that matter most. With Common Room, teams can finally realize the true potential of their communities and build stronger relationships with their users than ever before.

We've raised over $50 million from top-tier investors including Greylock, Index, and Madrona to build the community platform for modern SaaS companies. Additionally, we're backed by 25+ operators from community-first organizations such as Figma, Stripe, Airtable, Slack, Notion, Loom, and more.

You + Common Room? You'd be joining a team that revels in asking hard questions, collaborating gladly, and making decisions quickly—a team that values simplicity, passion, trust, each other, and our customers above all.

So hello! Please, knock on our door. We'd love to meet you.
Why we need you:

As with any modern organization, data is our lifeblood, and you are its guardian. You own Common Room's data infrastructure (data warehouse, ETL pipeline, event data management, data model development and deployment, etc) that enables us to develop data-driven insights for our customers, find innovative applications of customers' community data, and to develop data-driven business and product insights.You collaborate effectively with engineers, PMs, designers, and leadership to develop technology and process solutions to ensure that our data infrastructure functions reliably and can be leveraged for building innovative solutions for our customers.

Outside of this, you also identify and participate in important company-building initiatives that a fast growing startup needs.

You would enjoy being a member of our team if you:

Have 5+ years of experience, or consider yourself either a stable anchor or an untapped genius ready to flourish
Revel in your craft, are excited by and capable of establishing the technical data strategy for our company (data warehouse, ETL pipeline, event data management, data model deployment, etc)
Collaborate effectively with other experienced engineers, designers, and PMs
Are experienced with building at startup speed, or excited to experience and embrace it
You have good judgement on tradeoffs and tools needed to solve the problem, and don't over index on trendy/fashionable tech. You generally prefer supported, stable technologies
Are motivated by customer problems, and find joy in creating simple, elegant solutions that avoid unnecessary complexity
Bring your authentic self to work, and engage in candid + respectful feedback
Build sustainably, and are capable of managing your time and priorities as best suits your needs
Enjoy the journey as much as the destination, but never lose sight of the destination
Demonstrate pride, ownership, and accountability for your work, and expect the same from those you work with

Our current tech stack:

We choose tools that help solve the problem at hand efficiently. For our current set of problems, we prefer to primarily use TypeScript with Node.js. We use TypeGraphQL for our API endpoints, Apollo/Express as our web server, and Postgres (hosted on RDS) as our database. We use React for front-end, and AWS for our infrastructure needs.
Our values:

Be Customer-centric - We work backwards from the needs of our customers. The crisp articulation of customer value guides our decisions.
Strive for Simplicity - We choose simplicity over complexity whenever possible. We seek to identify and understand the essential quality of what we are building.
Craft Excellence - We take pride in our craft and are relentless in our pursuit of excellence. But we also know real artists ship.
We're In this Together - We measure personal success by the success of our customers and teammates. Relationships matter, and the strongest ones are built on the foundations of trust, enablement, and transparency.

Our benefits:

Our investment in caring for our employees and their families is a key part of our values and culture at Common Room:

Competitive base compensation with meaningful equity ownership
Health insurance including medical, dental, and vision
We pay 100% of your employee premium and 50% of your premium for any dependents
Long Term Care insurance
Unlimited Paid Time Off
8 Paid Company Holidays
Work from home policy including a laptop and support for your home office needs
401(k) self contribution
Commuter Benefits
Paid Family Leave
Opportunity to join a diverse, passionate, and fun team at a pivotal time in the company's lifecycle
Remote friendly (west coast only)

Common Room provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",https://www.indeed.com/rc/clk?jk=3b850523bd80fbc7&fccid=b2422f21833e4dad&vjs=3,"Remote in Portland, OR+2 locations",Data Engineer,Data Engineer
4,Lamda Inc,"Plunk is a proptech company that has built the first AI and Machine Learning platform to revolutionize the way homeowners, home experts, and investors value and invest in residential real estate. We combine the most advanced analytical methods, new innovations, and computer vision to turn residential real estate into a mainstream investment class. Our team is made up of data scientists, technologists, engineers, product managers, designers, storytellers, and most importantly–we are a team passionate about empowering all participants in the $180 trillion worldwide residential real estate ecosystem.
For more information, visit https://www.getplunk.com/
We are looking for a passionate developer who loves to tackle new challenges, learn new platforms and technologies, and work alongside other developers and leaders in the company.
The Plunk Data team is developing new analytical approaches to indexing and understanding residential real estate, with the goal of empowering homeowners to make the best decisions about their biggest investment. Data Variety, Quality, Recency and Completeness is the lifeblood of this effort!
The Role
This data engineering professional will help take advantage of new and existing data sources in support of Plunk’s analytical products. We are looking for stellar candidates who love data of all kinds, are masters are of building efficient, scalable, operable data pipelines, and are ready to take ownership in the following areas:
Data ingestion pipelines for existing and new data sources
Monitoring frameworks to protect data quality and consistency
Pipelines to populate Plunk’s proprietary data schema from sources
Data modeling design and data architecture skills to support reporting and analytics requirements
Thought leadership in relevant tooling, architecture, and processes to scale data management 100x or more!
Requirements
As an early-stagep, we are looking for team members who are ready and willing to take on a variety of challenges and bring an abiding curiosity about the “what’s” and “why’s” of the data and technology. Specific qualifications we are looking for:
Degree in Computer Science, Mathematics, Statistics, or other data-intensive discipline with substantive engineering experience.
7+ years demonstrated development experience using SQL, Java, Scala, and/or Python
5+ years demonstrated experience in data management (structured and unstructured) and modern database technologies
Demonstrated experience developing data pipelines to support machine learning or other analytical solutions
Experience working with AWS services
Ability and willingness to work in a lightweight Agile/Scrum development process
Ability to work collaboratively through ambiguity, with urgency, patience, and good humor
Bonus Points & Other Considerations
Demonstrated experience with Spark or other distributed data-processing frameworks
Experience working with real estate, economic, geospatial and/or demographic data
Good “data savvy”, rooted in strong foundational knowledge of statistical methods and probability
Benefits:
Significant, early-startup Incentive Stock Option (ISO) Equity package
One of the best Full medical, dental, and vision insurance plans available!
Unlimited PTO
We encourage applications from all qualified candidates regardless of regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.",https://www.indeed.com/rc/clk?jk=8f6f3e8613519f85&fccid=4dbd73bad3bc8a71&vjs=3,"Bellevue, WA",Data Engineer,Data Engineer
5,US Internal Revenue Service,"Help
This job is open to
Career transition (CTAP, ICTAP, RPL)
Federal employees who meet the definition of a ""surplus"" or ""displaced"" employee.
Military spouses
The public
U.S. Citizens, Nationals or those who owe allegiance to the U.S.
Clarification from the agency
Open to U.S. Citizens and Nationals and ICTAP eligibles in the commuting area.",https://www.indeed.com/rc/clk?jk=d8450e86c0c973b7&fccid=39ca88c90a37b09b&vjs=3,"Fresno, CA",Computer Engineer (Data Systems),Data Engineer
6,Kin Insurance,"The world has changed. Why hasn't insurance?

Kin's mission is to reimagine home insurance For Every New Normal. While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same.

Kin is proud to be one of BuiltIn Chicago's 2021 and 2022 Best Mid Sized Companies to work for, and Forbes 2021 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.
So, what's the role?

Data is central to Kin's operations and success. As a data engineer, you will be part of a data management team that supports and enables our product, operations, analytics, and data science teams, amongst others. As we scale, you will be integral in how we manage, structure, and store our data, as well as develop new solutions related to data architecture and ETL pipelines.
A day in the life could include:

Creating, designing, and maintaining ETL pipelines
Working with data science and BI teams to create data sets to be used in various projects
Participating in daily stands and weekly retros
Collaborating with cross-functional team members
Providing subject matter expertise and support

I've got the skills… but do I have the necessary ones?

3+ years of data engineering experience
Experience with the entire ETL pipeline: Data Integration tools, Databases, Big Data Platforms, and cloud based data platforms.
Exposure to and support of data visualization tools, such as Looker.
Experience in building from the ground up a modern next generation data warehouse platform.

Oh, and don't worry, we've got you covered!

Medical, Dental and Vision Insurance (including 100% employer-paid plans)
Flexible PTO policy
Very generous equity options and 401K
Parental Leave
Continuing education and professional development
Disability and Life Insurance
The excitement of joining a high-growth Insurtech company and seeing your work make an impact
About Kin

In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we're making that a reality.

Our approach to the industry makes us unique, and the people at Kin help us excel. We're a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it's how we treat each other. That's one of the many reasons we've been recognized as a great place to work by Built In, Forbes, and Fast Company.


EEOC Statement

Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don't discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",https://www.indeed.com/rc/clk?jk=412ee8f2444a9610&fccid=0910ded94ff651cf&vjs=3,"Remote in Chicago, IL+1 location",Data Engineer,Data Engineer
7,Warwick Investment Group,"FIRM INTRODUCTION
Warwick Group is a private equity firm that invests in real assets. We are a small but fast-growing team with ~$2 bn. in managed assets. At Warwick we strive to couple innovative techniques in data mining and machine learning to make intelligent data-driven investment decisions.
Data is deeply embedded in the culture at Warwick. Every department uses data in different varieties and volumes. We rely on public and private data – lots of it – to improve asset identification and management and continuously evaluate risk.
As a Data Engineer within the Data Science & Engineering team, you will take an essential role in building scalable, repeatable, highly available and secure data pipelines. This is a unique opportunity to work with a talented group founded with the goal of applying cutting-edge technology to rich data for improving investment strategies.
We're looking for a talented Data Engineer with extensive experience in building scalable data pipelines to join us as a foundational member of the team and to provide leadership in all areas that relate to data integration and data management.
JOB RESPONSIBILTIES
Facilitate acquisition of data from a variety of different sources and analyzing the data for correctness with input from downstream users
Work with various structured and unstructured data systems (relational, file based, HDFS/ADLS) and their performance tuning requirements
Develop automation tools where appropriate, with a focus on collecting, parsing and managing large sets of discrete data sources
Data-modeling and master data management
Building data services for applications and machine learning/analytics pipelines using GraphQL, Apollo, Hasura, or REST AI.
Review business requirements, recommending technological solutions that can be integrated and deployed in the environment
Identify key opportunities for redesign of systems infrastructure to meet end user needs
QUALIFICATIONS
Bachelor’s / Master’s Degree (preferably in Computer science or equivalent)
5+ years of work experience in Data Engineering or another relevant field
Solid understanding of database design principles and the system development life cycle
Ability to communicate and understand end user or business requirements
Strong familiarity with Microsoft SQL Server, Sybase, Oracle, ETL Tasks, SSIS, Database Development and Administration
Experience in data acquisition from REST API, SFTP, Cloud Storage, web scraping
Experience with self-service data delivery platforms like Dremio or equivalent
Familiarity with orchestration using Airflow/Prefect, SQL Server
Development in SQL, Python, JavaScript, and Open-Source technologies
Experience working in secure cloud platforms Azure, AWS, GCP
Comfortable working in both Windows and Linux environments",https://www.indeed.com/rc/clk?jk=156467993e35d562&fccid=5f91a5a521416588&vjs=3,"Oklahoma City, OK 73116 73116",Data Engineer,Data Engineer
8,Twitch,"About Us

Launched in 2011, Twitch is a global community that comes together each day to create multiplayer entertainment: unique, live, unpredictable experiences created by the interactions of millions. We bring the joy of co-op to everything, from casual gaming to world-class esports to anime marathons, music, and art streams. Twitch also hosts TwitchCon, where we bring everyone together to celebrate, learn, and grow their personal interests and passions. We're always live at Twitch. Stay up to date on all things Twitch on LinkedIn, Twitter and on our Blog.

About the Role

You will shape the way that performance is measured, establish how we transform our data, and scale analytics methods and tools to support our growing business, leading the way for high-quality, high velocity decisions.

We're looking for an experienced data engineer to join our Monetization Data Engineering team, which is focused on making trusted, reusable data assets for data consumers in Ads, Commerce, and throughout Twitch to use. Your responsibilities will include creating and maintaining pipelines to batch transform billions of records into our data warehouses which act as sources of truth across the company; working with data producers to improve data coverage and ensure complete data; partnering with other data tool creators driving data quality and maintaining data across interfaces for Twitch staff to consume in different formats. In the process, you will work with technical and non-technical staff members throughout Twitch.

You Will:
Delight data consumers throughout Twitch by ensuring they have the data they need to inform decisions, where and when they need it
Own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their questions
Prioritize projects from a diverse set of stakeholders to maximize impact
Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; develop and maintain data quality monitoring and assurance framework; and improve the processes for developing new ones, raising the level of quality expected from our work
Improve data discovery: create data exploration tools and promote adoption of data sources across the company
Improve business, engineering, and data processes through data architecture, engineering, testing, and operational excellence best practices
You Have:
3+ years of experience in data engineering, software engineering, or other related roles
3+ years using relational database concepts with a working knowledge of SQL, SQL tuning, data modeling best principles, OLAP, Big Data technologies
3+ years of experience creating data pipelines from data sources, in collaboration with diverse stakeholders
Experience with development best practices, including query optimization, version control, code reviews, and documentation
Experience with an orchestration platform: Airflow, Luigi, Step Functions.
Experience with Amazon Web Services: Redshift, S3, Glue, EMR, Athena
Experience with Python
Bonus Points
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k), Maternity & Parental Leave
Flexible PTO
Commuter Benefits
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.),
Breakfast, Lunch & Dinner Served Daily
Free Snacks & Beverages

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",https://www.indeed.com/rc/clk?jk=e3b86effaa319851&fccid=e21f6affbb4bb0b2&vjs=3,"San Francisco, CA+1 location",Data Engineer,Data Engineer
9,Piper Companies,"Zachary Piper Solutions is currently searching for a remote Data Engineer to work as a Full Time Employee supporting a development/data driven engagement.

2 to 4 years of experience working within a NoSQL database.

Position Requirements:
Build and maintain data pipelines to ingest data from multiple sources, create scripts/modules for data harmonization and backend processing
Develop REST APIs to support complex queries against the database and user interactions within applications
Develop in adherence with policies and procedures to ensure the security of the application
Research and introduce new technologies and best practices as needed to solve business problems and improve existing solutions
Collaborate in an Agile framework through the entire SDLC to gather requirements, design, build, and deploy web applications that allow for complex user interaction with advanced dynamic content
Develop using Git for source code management and collaboration
Require Qualifications and Skills:
Bachelor’s in Computer Science or related field, preferred, or equivalent combination experience of education and experience
2-5 years of professional experience
1+ year experience working with NoSQL database technologies
1+ years of experience with Server-Side JavaScript
Understanding of the SDLC and familiarity with modern Agile development methodologies
Ability to articulate coding rationale and problem-solving thought process
Excellent verbal and written communication skills
Ability to work independently, be self-motivated, and innovative
Strong collaboration skills and ability to work well in a team environment
Strongly Preferred Qualifications & Skills
Experience with multi-model NoSQL databases such as MarkLogic, ArangoDB, OrientDB, etc.
Experience with Apache NiFi or similar ETL tool
Compensation: Salary between $75,000 - $95,000 based on experience. Full Benefits will be provided including: PTO, Holidays, Medical, Dental, Vision, education reimbursement and much MORE!",https://www.indeed.com/rc/clk?jk=bfe8db14149ad982&fccid=fc68da685e8aa986&vjs=3,+9 locationsRemote,Data Engineer (100% Remote),Data Engineer
10,Radiant Digital,"About Us:
Radiant Digital delivers technology consulting and business solutions for commercial and government clients.
Our flexible delivery model allows us to provide end-to-end solution delivery, single project execution, and, or strategic resources.
CMMI Maturity Level III and ISO 9001 – 2015 certified.
Responsibilities:
Position: Data Engineer
Location : Remote

Client : CFPB

Skill Requirements
5+ years of experience in the following:Experience managing code in GitHubExperience building applications in PythonExcellent communication skills and ability to work with customers, senior management and other technical teams.Excellent documentation and organizational skills.1+ years of experience in the following:Experience deploying applications using AnsibleExperience using Mesos containersExperience managing Citus infrastructure and deploying with AnsibleExperience using AWS tools and services
Technical skills
AWS, GitHub, Python

This is a remote position.",https://www.indeed.com/rc/clk?jk=278ea6cc19c91cc7&fccid=86872fa3ab4e6621&vjs=3,"Remote in Vienna, VA 22180 22180+1 location",Data Engineer,Data Engineer
11,Twitch,"About Us

Launched in 2011, Twitch is a global community that comes together each day to create multiplayer entertainment: unique, live, unpredictable experiences created by the interactions of millions. We bring the joy of co-op to everything, from casual gaming to world-class esports to anime marathons, music, and art streams. Twitch also hosts TwitchCon, where we bring everyone together to celebrate, learn, and grow their personal interests and passions. We're always live at Twitch. Stay up to date on all things Twitch on LinkedIn, Twitter and on our Blog.

About the Role

Twitch is building the future of interactive entertainment. The Data Platform team develops and operates the platform that enables Twitch's data-informed decision-making. This ranges from the machine learning models that power Twitch Search to empowering self-serve analytics for business staff. Our data pipeline receives over 220 billion events a day, and we forward live data to hundreds of systems as well as powering decisions at all levels of Twitch. We provide tools to ingest, store, transform, move, and understand data.

As a software engineer on Data Platform, you will deliver the next generation of Twitch data technology. Using Go and Python, you will build tools which are reliable and scale. You'll review and shape the work of your peers, and participate in a high performing engineering team.

You can work in San Francisco, CA, Irvine, CA; Seattle, WA; New York, NY; and Vancouver, BC.

You Will:
Develop new capabilities in our data warehouses and pipelines.
Improve the reliability, flexibility, and scalability of our existing tools.
Collaborate on our vision as we scale to our next petabyte.
You Have:
1+ years with software engineering experience (intern or part-time)
Minimum of Bachelor's degree in Computer Science or prior relevant engineering experiences
Experience with data structures and algorithms.
Proficient experience in one of the following languages: Go, Ruby, Java, and C++
Bonus Points
Familiar with AWS ecosystem
Experience with Distributed systems
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages)

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Workers in New York City who perform in-person work or interact with the public in the course of business must show proof they have been fully vaccinated against COVID or request and receive approval for a reasonable accommodation, including medical or religious accommodation.",https://www.indeed.com/rc/clk?jk=e7a0f0dcc31fb57e&fccid=e21f6affbb4bb0b2&vjs=3,"Seattle, WA+2 locations",Software Engineer - Data,Data Engineer
12,Factspan Inc,"Education/Experience· Bachelor's degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience.· 6+ years of work experience in Data Engineering, ETL Development and Data Analytics.· 6+ years of hands-on experience using SQL and scripting language such as Unix Shell or Python· 5+ years of hands-on experience developing on a Linux platform.· 4+ years of hands-on experience working in traditional RDBMS such as Oracle, DB2.· 1+ years of hands-on experience working in Cloud technology· 1+ years of hands-on experience working scripting language such as PythonCommunication Skills· Excellent written and verbal communication and presentation skills· Ability to collaborate with internal and cross functional teamsMathematical Skills (if applicable)· Basic math functions such as addition, subtraction, multiplication, division and analytical skills.Reasoning Ability· Must be able to work independently with minimal supervision, strategic thinking and organizational planning skillsPhysical Demands (if applicable)· This position involves regular ambulating, sitting, hearing, and talking.· May occasionally involve stooping, kneeling, or crouching.Other Skills· Experience with Spark, PySpark, Zeppelin and Jupyter Notebook is nice to have· Demonstrated experience implementing and automating ETL processes on large data sets· Experience with report development and supporting data requirements for reporting· Ability to multi-task and meet deadlines.Ability to work with diverse teams and multiple technologiesWork Hours· Ability to work on weekends and any Two weekdays will be off.Job Type: Full-timePay: $83,160.00 - $180,000.00 per yearSchedule:8 hour shiftDay shiftWeekend availabilityExperience:Python: 4 years (Preferred)SQL: 4 years (Preferred)Work Location: RemoteSpeak with the employer+91 +12066934361",https://www.indeed.com/company/Factspan-INC/jobs/Data-Engineer-61eb15ee6e2587f4?fccid=5c8b7a3ab18498ba&vjs=3,Remote,Data Engineer,Data Engineer
13,"Enterprise Engineering Solutions, Inc.","ScopePerson should be strong in writing Scope Query. Deal with Cosmos Structured Streams.C#PythonFunctions, loops, String handling functions, Python Data structures (tuple, List, Data Dictionary etc ), Panda libraries , Dataframes etcJob Types: Full-time, ContractPay: $46.00 - $90.00 per hourSchedule:10 hour shift8 hour shiftApplication Question(s):Are you US Citizen or US Green Card Holder? Please answer in Yes or NoExperience:Scope management: 3 years (Preferred)SQL: 3 years (Preferred)C#: 3 years (Preferred)Python: 3 years (Preferred)Pandas: 3 years (Preferred)Work Location: Multiple Locations",https://www.indeed.com/company/Enterprises-Engineering-Solutions/jobs/Data-Engineer-003a50154c6268a9?fccid=2a8464104b0db383&vjs=3,"Seattle, WA",Data Engineer,Data Engineer
14,Factspan Analytics,"CANDIDATE SHOULD BE FLEXIBLE WORKING OVER THE WEEKENDS Required: Experience with cloud based platforms (i.e. Azure, GPC, AWS) – Azure PreferredSnowflake SQL/Spark - Data engineering pipelineExposure/understanding of deployment practices using containerization (i.e. Kubernetes, Docker)Experience in Python, SQLStrong critical thinking, communication, and problem-solving skillsPreferred: Experience building automated data pipelinesExperience performing data analysis and data explorationExperience working in an agile delivery environmentBonus: Exposure/understanding of DevOps best practice and CICD (i.e. Jenkins)Experience working in multi-developer environment, using version control (i.e. Git)Experience with big data frameworks (i.e. Hadoop and Spark) – Azure Data Bricks PreferredJob Type: Full-timeSalary: $130,000.00 - $160,000.00 per yearBenefits:Dental insuranceHealth insurancePaid time offVision insuranceSchedule:Weekend availabilitySupplemental Pay:Bonus payExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Factspan-Analytics/jobs/Data-Engineer-68a9bcc32210c9c4?fccid=84d82b6ff2f37bc8&vjs=3,Remote,Data Engineer - Remote,Data Engineer
15,"Enterprise Engineering Solutions, Inc.","ScopePerson should be strong in writing Scope Query. Deal with Cosmos Structured Streams.C#PythonFunctions, loops, String handling functions, Python Data structures (tuple, List, Data Dictionary etc ), Panda libraries , Dataframes etcJob Types: Full-time, ContractPay: $46.00 - $90.00 per hourSchedule:10 hour shift8 hour shiftApplication Question(s):Are you US Citizen or US Green Card Holder? Please answer in Yes or NoExperience:Scope management: 3 years (Preferred)SQL: 3 years (Preferred)C#: 3 years (Preferred)Python: 3 years (Preferred)Pandas: 3 years (Preferred)Work Location: Multiple Locations",https://www.indeed.com/company/Enterprises-Engineering-Solutions/jobs/Data-Engineer-003a50154c6268a9?fccid=2a8464104b0db383&vjs=3,"Seattle, WA",Data Engineer,Data Engineer
16,Piper Companies,"Zachary Piper Solutions is currently searching for a remote Data Engineer to work as a Full Time Employee supporting a development/data driven engagement.

2 to 4 years of experience working within a NoSQL database.

Position Requirements:
Build and maintain data pipelines to ingest data from multiple sources, create scripts/modules for data harmonization and backend processing
Develop REST APIs to support complex queries against the database and user interactions within applications
Develop in adherence with policies and procedures to ensure the security of the application
Research and introduce new technologies and best practices as needed to solve business problems and improve existing solutions
Collaborate in an Agile framework through the entire SDLC to gather requirements, design, build, and deploy web applications that allow for complex user interaction with advanced dynamic content
Develop using Git for source code management and collaboration
Require Qualifications and Skills:
Bachelor’s in Computer Science or related field, preferred, or equivalent combination experience of education and experience
2-5 years of professional experience
1+ year experience working with NoSQL database technologies
1+ years of experience with Server-Side JavaScript
Understanding of the SDLC and familiarity with modern Agile development methodologies
Ability to articulate coding rationale and problem-solving thought process
Excellent verbal and written communication skills
Ability to work independently, be self-motivated, and innovative
Strong collaboration skills and ability to work well in a team environment
Strongly Preferred Qualifications & Skills
Experience with multi-model NoSQL databases such as MarkLogic, ArangoDB, OrientDB, etc.
Experience with Apache NiFi or similar ETL tool
Compensation: Salary between $75,000 - $95,000 based on experience. Full Benefits will be provided including: PTO, Holidays, Medical, Dental, Vision, education reimbursement and much MORE!",https://www.indeed.com/rc/clk?jk=bfe8db14149ad982&fccid=fc68da685e8aa986&vjs=3,+9 locationsRemote,Data Engineer (100% Remote),Data Engineer
17,Class Valuation,"Class Valuation is looking for a Data Engineer!!
Essential Tasks/Qualifications:
Experience with NoSQL and SQL based persistence layers
Experience with ETL at scale within large data streaming-based architectures
Functional SQL skills and willingness to expand existing skillset
Experience with cloud-native (AWS and Azure) and their subsequent tooling (Kinesis, Athena, Firehose, Replication, etc.)
Demonstrated ability to work independently and efficiently under tight deadlines
Comfortable working and collaborating within remote teams and maintaining a presence
Understands and is willing to contribute to a large Data warehouse, Datamart, and Data Lake architectures in the cloud
Some exposure to data cubes, projections, model development, KPIs, business intelligence, reporting, data frames, etc.
Comfortable with 'owning' a feature/user story from requirements to delivery
Comfortable working in a highly collaborative environment interfacing back end, DevOps, data, and QA engineers
Comfortable interfacing with multiple internal stakeholder groups including but not limited to: Analytic teams, Data Science teams, Operations teams, and Technology teams
Willing to learn, contribute, and solve problems in a large data environment using both traditional data sets and blob data at scale (>100 Terabytes)
Understands collective ownership of patterns within one’s space
Values personal traits including but not limited to a sense of ownership and pride, personal integrity, humility, customer-focused and collaborative
Comfortable working on multiple pieces spanning different products at once and understanding relevance and prioritization as it pertains to release schedules
Experience with git, pull requests, SCM patterns and practices, SCRUM, and the Spotify development process
Here are just a few of the amazing benefits we have to offer:
100% Remote Opportunity!
Paid Training!
Competitive Pay!
EAP/Health & Wellness Benefits!
Paid Time Off/Floating Holidays!
Opportunities for Growth/Advancement!!!
***Bonus***You’ll have an exciting & fun career with a top AMC (Appraisal Management Company)! Don’t know what an AMC is or what we do? No problem! We offer a unique virtual training program to get you up to speed & you’ll have AMAZING team members along the way!
LEARN MORE ABOUT OUR PATH TO APPRAISAL MODERNIZATION HERE
FROM SCOT ROSE: https://www.instagram.com/tv/CZrjWNYsacN/?utm_medi
Learn more about our 3D Scan Team here: https://www.classvaluation.com/3d-property-scans/
Our recruiting team is eager to speak with you! APPLY TODAY!!! Want to learn more about us? Connect here:
https://www.facebook.com/classvaluation
https://www.linkedin.com/company/class-valuation/
https://www.instagram.com/classvaluation/
Classvaluation.com
Industry Leading AMC Since 2009
#QualityWithoutQuestion",https://www.indeed.com/rc/clk?jk=02659eb50c286fe7&fccid=7c56b687774ca8f9&vjs=3,"Troy, MI 48083 48083",Data Engineer,Data Engineer
18,Factspan Inc,"Education/Experience· Bachelor's degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience.· 6+ years of work experience in Data Engineering, ETL Development and Data Analytics.· 6+ years of hands-on experience using SQL and scripting language such as Unix Shell or Python· 5+ years of hands-on experience developing on a Linux platform.· 4+ years of hands-on experience working in traditional RDBMS such as Oracle, DB2.· 1+ years of hands-on experience working in Cloud technology· 1+ years of hands-on experience working scripting language such as PythonCommunication Skills· Excellent written and verbal communication and presentation skills· Ability to collaborate with internal and cross functional teamsMathematical Skills (if applicable)· Basic math functions such as addition, subtraction, multiplication, division and analytical skills.Reasoning Ability· Must be able to work independently with minimal supervision, strategic thinking and organizational planning skillsPhysical Demands (if applicable)· This position involves regular ambulating, sitting, hearing, and talking.· May occasionally involve stooping, kneeling, or crouching.Other Skills· Experience with Spark, PySpark, Zeppelin and Jupyter Notebook is nice to have· Demonstrated experience implementing and automating ETL processes on large data sets· Experience with report development and supporting data requirements for reporting· Ability to multi-task and meet deadlines.Ability to work with diverse teams and multiple technologiesWork Hours· Ability to work on weekends and any Two weekdays will be off.Job Type: Full-timePay: $83,160.00 - $180,000.00 per yearSchedule:8 hour shiftDay shiftWeekend availabilityExperience:Python: 4 years (Preferred)SQL: 4 years (Preferred)Work Location: RemoteSpeak with the employer+91 +12066934361",https://www.indeed.com/company/Factspan-INC/jobs/Data-Engineer-61eb15ee6e2587f4?fccid=5c8b7a3ab18498ba&vjs=3,Remote,Data Engineer,Data Engineer
19,Factspan Analytics,"CANDIDATE SHOULD BE FLEXIBLE WORKING OVER THE WEEKENDS Required: Experience with cloud based platforms (i.e. Azure, GPC, AWS) – Azure PreferredSnowflake SQL/Spark - Data engineering pipelineExposure/understanding of deployment practices using containerization (i.e. Kubernetes, Docker)Experience in Python, SQLStrong critical thinking, communication, and problem-solving skillsPreferred: Experience building automated data pipelinesExperience performing data analysis and data explorationExperience working in an agile delivery environmentBonus: Exposure/understanding of DevOps best practice and CICD (i.e. Jenkins)Experience working in multi-developer environment, using version control (i.e. Git)Experience with big data frameworks (i.e. Hadoop and Spark) – Azure Data Bricks PreferredJob Type: Full-timeSalary: $130,000.00 - $160,000.00 per yearBenefits:Dental insuranceHealth insurancePaid time offVision insuranceSchedule:Weekend availabilitySupplemental Pay:Bonus payExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Factspan-Analytics/jobs/Data-Engineer-68a9bcc32210c9c4?fccid=84d82b6ff2f37bc8&vjs=3,Remote,Data Engineer - Remote,Data Engineer
20,HARAMAIN SYSTEMS INC.,"Role: Big Data Engineers (Multiple openings)
Location :Remote (EST or CST time zones)
Duration :Long Term
Big Data Engineers, any who also have some DBA experience a bonus.
They have built the logic but need help building the tuning and analytics for the 7000+ dealer locations
All backend work
Qualifications:

BigData: Hadoop / Spark (preferably with Scala). Looking for Engineers with hands on experience.
Must have Strong Experience on Spark and Hive SQL.
Knowledge on Database architectures of RDBMS.
Experience in building Data-pipeline, complex SQL and ETL
Experience with Data-formats like Parquet, CSV, JSON etc.
Data-storage architectures like HDFS, S3 and Hive.
Data-transformations concepts including Partitioning, Shuffling,
Translate, load and present disparate datasets in multiple formats/sources including JSON, text, SQS.
Data-processing constructs like hive joins, MapReduce.

This is a remote position.",https://www.indeed.com/rc/clk?jk=67f9aecb65c08646&fccid=def5c3e96031049d&vjs=3,"Remote in Seattle, WA 98101 98101+1 location",Big data engineer,Data Engineer
21,"Esolvit Inc.,","Title: Data EngineerLocation: Washington, DCJob Description:Develop analytics-based solutions that produce quantitative and qualitative business insights. Work with partners as necessary to integrate systems and data quickly and effectively, regardless of technical challenges or business environments.Reporting to the Lead Business Analyst and delivering to Arcadis Gen plans using Gen tools and processes, the Data Engineer mode ls master data for loading into MLR consulting with the customer on design, translation, interpolation, testing and data sign off.Must havesExperience of Scrum methodologyExperience building asset-based data models in ExcelEAM experience with multiple asset classesUnderstanding of Maximo XMLFirst class stakeholder managementNice to haves: Experience using project collaboration tools (Atlassian: JIRA & Confluence, Slack, Teams, etc)Maximo Functional ExperienceData engineering experience with a major Rail organisationRequired Skills: 5 Years of Data Engineering5 Years of IBM Maximo Data MigrationJob Types: Full-time, ContractPay: $100,000.00 - $169,606.00 per yearBenefits:Dental insuranceHealth insuranceLife insuranceReferral programVision insuranceSchedule:Monday to FridayExperience:IBM Maximo Data Migration: 6 years (Required)Data Engineering: 5 years (Required)Work Location: One location","https://www.indeed.com/company/Esolvit-Inc.,/jobs/Data-Engineer-5e44e030db37b8bd?fccid=2cabaea7be285a10&vjs=3","Washington, DC",Data Engineer (IBM Maximo) -(only US Citizens),Data Engineer
22,iconvergence,"Minimum 8 -10 years Must4-5 years AWS and Python experience RequiredData engineering with a similar skill set as mentioned below is needed.Experienced Data EngineerUnderstanding the new requirements and business rules.Implementing the logic using Data sourcing, Data Movement and loading into Data lake using the tools like AWS s3, AWS Glue transform stored data by python scripts and loading into s3 buckets, loading into optimized layer and snowflake data bases.Extracting data from AWS S3, which was loaded by ETL TeamProcessing data using Glue to analyze and transform data into Snowflake,.Processing large sets of structured and semi-structured data using Pyspark Data Frames/RDDDeveloping jobs or scripts required for Data sourcing, Data Movement and Data engineering.Updating JIRA Cards with daily updatesParticipating in Daily Status calls and Sprint meetingsJob Types: Full-time, ContractPay: $91,027.00 - $150,000.00 per yearSchedule:8 hour shiftAbility to commute/relocate:Austin, TX: Reliably commute or planning to relocate before starting work (Required)Experience:AWS: 4 years (Required)pYTHON: 4 years (Required)Glue: 2 years (Required)Snowflake: 1 year (Required)Work Location: One location",https://www.indeed.com/company/iConvergenceSolutions/jobs/Data-Engineer-b75aeb68ce8d9a45?fccid=8048f09ea2d1f72f&vjs=3,"Austin, TX",Data Engineer,Data Engineer
23,Devcare Solutions,"The Voice/Data Engineer (VDE) directs and participates in all activities related to the selection and installation of telephone facilities and special on-premises equipment that will meet the customer's communication requirements.Responsible for all technology and connectivity involving telecommunications and data networks.Typically specializes in telephony and data interfaces and systems that have proprietary functions within the communications area of a corporation/business.General wiring excluded, the Voice/Data Engineer ensures that any specialized conduit or wiring is properly deployed and installed according to code.Expert in audio/visual, teleconferencing, and voice mail equipment.Often specialized or is certified in a particular piece of equipment.Please note - this position is 100% onsite.Job Types: Full-time, ContractSalary: $35.00 - $40.00 per hourSchedule:8 hour shiftAbility to commute/relocate:Trenton, NJ: Reliably commute or planning to relocate before starting work (Required)Experience:VoIP Avaya Platform Experience - (System Manager): 1 year (Required)Avaya Voice Mail Aura: 1 year (Required)Verizon CMAC Centrex and POS line support: 1 year (Preferred)Avaya CMS Supervisor: 1 year (Preferred)AirWatch - Cellular Management System: 1 year (Preferred)Legacy ISDN technologies: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Devcare-Solutions/jobs/Voice-Data-Engineer-fb4e6eb5a6a50057?fccid=debba90512f725cc&vjs=3,"Trenton, NJ",Voice/Data Engineer,Data Engineer
24,Prexel LLC,"Job Description: In this role, you will be responsible for developing ETL processes for on-prem Hadoop and AWS technologies.You will address data migration issues, including validation, clean-up, and gaps, and must understand the importance of data dictionaries.Responsibilities: Design and develop ongoing data ingestion using Python and Spark.Use programming skills to create big data pipelines that can deal with both structured and unstructured data.Support other teams by effectively using big data tools and providing guidance for writing efficient code in SQL, Python, and/or R.Analyze and load data for use by a larger audience.Analyze databases to improve data access speed.Analyze data provided by project consultants and make it available in our databaseDevelop strategies for data acquisitions, archive, recovery, and database implementation.Prepare data for econometricians for further statistical analysis.Design and develop databases, data warehouses, and multidimensional databases.Lead and direct data management work of others as applicable.Qualifications: Bachelor’s degree required.1-3 yrs of experience with big data tools, including Apache Spark or Hadoop.2+ years of developing data ingestion and transformation processes associated with big data technologies (e.g., Hadoop, Spark, SQL, etc.)Hands-on Python, SQL, and/or R coding experience.Experience with data modeling concepts.Experience troubleshooting Spark-related failures.Previous experience working with cloud (e.g., AWS) is a plus.Experience with object-oriented scripting languages: Python (preferred) and/or Java.Data tools knowledge, including hands-on experience with ETL (Extract-Transform-Load) software products.Experience with Databricks is a plus.Experience with performance tuning of ETL jobs.Experience with data modeling, integration, and warehousing.Ability to communicate technical issues to non-technical staff.Ability to think laterally with a wide degree of creativity.May require more than 40 hours per week to perform the essential duties of the position.Benefits: Competitive compensation and benefits including flexible work schedules, tuition reimbursement up to $50,000, low healthcare premiums, and much more!Open culture where your voice is heard, your input is sought, and your contributions are rewarded.Fun and engaging culture including frequent social events.Amenities including standing desks, fitness center, rooftop terrace, espresso, fresh fruit, pool table, and table tennis.Employee-driven community outreach featuring fundraising events (e.g., trivia, game shows, cooking competitions, etc.), volunteer opportunities, and matching funds.Investment in your career through training programs, assigned mentor and peer coach, and frequent feedback.Networking opportunities through employee interest groups, Women’s Network, International Network, and Diversity-Inclusion Council.Job Type: Full-timePay: $130,000.00 - $160,000.00 per yearBenefits:401(k)Dental insuranceHealth insurancePaid time offTuition reimbursementVision insuranceSchedule:8 hour shiftExperience:Spark: 3 years (Preferred)Working with Large Data: 2 years (Preferred)Troubleshooting and backend work of Spark: 2 years (Preferred)Python: 2 years (Preferred)Hadoop: 3 years (Preferred)Work Location: One location",https://www.indeed.com/company/Prexel-LLC/jobs/Data-Engineer-c5e6d3d01068cedc?fccid=6bf0836d264f2fb4&vjs=3,"Washington, DC",Data Engineer,Data Engineer
25,PlayStation Global,"Why PlayStation?

PlayStation isn't just the Best Place to Play — it's also the Best Place to Work. Today, we're recognized as a global leader in entertainment producing The PlayStation family of products and services including PlayStation®5, PlayStation®4, PlayStation®VR, PlayStation®Plus, PlayStation™Now, acclaimed PlayStation software titles from PlayStation Studios, and more.

PlayStation also strives to create an inclusive environment that empowers employees and embraces diversity. We welcome and encourage everyone who has a passion and curiosity for innovation, technology, and play to explore our open positions and join our growing global team.

The PlayStation brand falls under Sony Interactive Entertainment, a wholly-owned subsidiary of Sony Corporation.

Senior Data Engineer - Analytics and Visualization

San Francisco, CA

Data platform engineering team is looking for a flexible, skilled, highly driven Senior Data Engineer with strong technical abilities, outstanding communication skills and ability to mentor the team to achieve outstanding results. The individual will help lead development and operations for our real time data analytic and visualization platform that will be used for our next generation console, PlayStation 5!

Responsibilities
You will play a senior role in design, build, code review, deploy, and support software for our Real Time Data Analytics and Visualization Platform.
You will be part of a team building streaming pipeline solutions to reshape data and ingest into the platform in near real time.
This position requires extensive hands-on technical domain expertise in data infrastructure and visualization within the cloud (preferably AWS) ecosystem.
Collaborate with the team to lead Technology product evaluations, POC execution and platform implementation for the Visualization platform.
You will participate in product road-map discussions and identify key areas for improvements for the analytics and visualization platform.
Take on technical ownership of a work stream, be the subject matter expert and managing risks and issues
Mentor junior engineers to ensure we all deliver awesome!
Qualifications
BS Degree in Engineering, Computer Science or equivalent experience.
5+ years' experience in software development, design and analysis using Scala, Java, or Python
Proficient in using visualization technology (such as Domo, Tableau, Quicksight, imply etc.) from data discovery, dashboard prototyping, to productionize large scale visual analytics platform implementation, performance tuning and optimization.
Strong foundation in data engineering, data structures and software design.
Experience with real time data streaming technologies such as Kafka, Kinesis
Experience with OLAP/MOLAP database technologies such as Apache Druid, Pinot, Clickhouse
Experience with container technologies, such as Docker, Kubernetes, AWS EKS
Hands-on experience in cloud-based web services, at enterprise scale, AWS is preferable.
Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members.
Strong verbal and written communication skills and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams.
Strong sense of ownership and passion to provide world class customer experience.

Equal Opportunity Statement:

Sony is an Equal Opportunity Employer. All persons will receive consideration for employment without regard to gender (including gender identity, gender expression and gender reassignment), race (including colour, nationality, ethnic or national origin), religion or belief, marital or civil partnership status, disability, age, sexual orientation, pregnancy or maternity, trade union membership or membership in any other legally protected category.

We strive to create an inclusive environment, empower employees and embrace diversity. We encourage everyone to respond.",https://www.indeed.com/rc/clk?jk=c584f32c01e7d59d&fccid=ad3f1759d3ff042b&vjs=3,"San Diego, CA+3 locations",Senior Data Engineer – Analytics & Visualization,Data Engineer
26,AR SYSTEMS,"Skills: databricks, spark, aws , bigdata, hadoop, java or scala or pythonExperience in Software Engineering and best practices commensurate to the duties outlines above.Experience with different programming languages - C++, Java, Python, Scala - along with ability to pick up new languages.Experience with cloud technologies – specifically within the AWS ecosystem – DevOps, Databricks, Airflow, YAML, Spark, S3.Past demonstrable programming work - Exposure to big data technologies - Hadoop, Hive, Kafka.Strong expertise with DevOps tools and approaches, and engineering products to be both reliable and adaptable.Experience of deploying and managing large-scale, cloud-based solutions.Experience creating benchmark tests, designing for scalability and performance, and designing/integrating large-scale systems.Ability to perform analysis of business problems and technical environments.Proven track record of success in challenging the status quo, implementing new ideas and designs with a practical orientation.Ability to think strategically and implement iteratively. Ability to estimate financial impact of design/architecture alternatives.Strong teamwork and collaboration skills.Solid oral and written communication skills.Track record and a passion for being a team player. You take the lead when needed and also coach and develop others when needed.Be passionate about resolving user pain points through great design.Be open to receiving feedback and constructive criticism.Be passionate about all things big data of design and innovation. Research and showcase knowledge in the industry’s latest trends and technologies.Job Types: Full-time, ContractSalary: $70.00 - $80.00 per hourSchedule:Monday to FridayExperience:Big data: 10 years (Preferred)databricks: 5 years (Preferred)Work Location: One location",https://www.indeed.com/company/AR-systems-inc/jobs/Big-Data-Engineer-d3a3ed6c5f3b7ef0?fccid=6420329222ba8199&vjs=3,"Glastonbury, CT+1 location",Big Data Engineer,Data Engineer
27,Modernagile Technologies,"Role – Data EngineerLocation – Remote Job – work in PST Time Zone (SFO,CA / Carlsbad, CA)ContractJob Description : 1. 8 to 12 years of total experience2. Data lead position – hands-on is required3. Hands-on experience in building Data Pipeline, designing and building data architecture on AWS.4. Must Have - AWS Stack with Glue, EMR, Spark S3 etc.5. Good to have - Snowflake stack with SnowPipe, Stored proc, Tasks, Streams, JSON processing etc, AirflowResponsibilities : -1. Work with customer in PST timezone2. Understand the current system, architecture, design3. Articulate requirements, work closely with customer4. Resolve queries and guide customer team on solution5. Work as SPOC for customer and Persistent team6. Onsite coordination, work with offshore architect and team7. Complementary to offshore architectBest Regards, Karun| Sr.Technical RecruiterE-Mail:  karun At fisecglobal.net | Website: http://www.fisecglobal.netJob Types: Full-time, ContractPay: $65.00 - $68.00 per hourSchedule:8 hour shiftDay shiftMonday to FridayExperience:Data Engineer: 9 years (Required)Lead: 1 year (Required)AWS Stack with Glue, EMR, Spark S3 etc: 3 years (Required)Snowflake: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Modernagile-Technologies/jobs/Data-Engineer-5cd295ca9a20194f?fccid=c96c1083e675f6d1&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
28,Next Level Performance,"Position DescriptionSeeking a talented, experienced Data Engineer to join our data team, which is responsible for the company’s data operations. Responsibilities include database and report development, database administration, data feed integration, and data warehousing and analytics support. Additionally, the engineer will need to work with client teams to help develop reports to be used internally and for customers. We leverage various Microsoft technologies for our software offerings and operate within the Azure cloud computing ecosystemKey Responsibilities: Work with development and product owners to design and deploy schema changes for various applicationsFollow and help develop schema guidelines to ensure clear and consistent coding practicesLearn and support NXL legacy applicationsWork with client services to provide product support for customer data integrationsConvert legacy SSIS feeds into Azure Data Factory jobsWork with client services to understand reporting needs so corresponding data structures can be built in the Data WarehousePerform analytics on data base queries to provide feedback and possible solutions on potential performance bottlenecksPerform routine maintenance and clean up on data to ensure efficiency on the databaseAdministration of the servers to provide access to databases and routinely reviewing access and permissions to ensure system securityRequirements: B.S. in Computer Science or related discipline5+ years of extensive SQL Server/ Azure SQL experienceFamiliarity working with reporting tools such as PowerBI or TableauStrong database development/administration skillsHighly proficient with T-SQLScripting experienceObject-oriented programming experience with a language such as C# for authoring jobs, consuming services, etc.Experience/familiarity with SSRS/SSISExperience with REST APIsFamiliarity with Azure Data Factory or similar middlewareFocused on qualityWrites clean, highly maintainable SQL/codeLoves to work with dataAuthorized to work in the USPerformance Measurements Delivery of client projects on timeTakes initiative to ensure a high quality of workDirect Report(s) n/aJob Type: Full-timePay: $70,000.00 - $90,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:8 hour shiftMonday to FridaySupplemental Pay:Bonus payCOVID-19 considerations:To combat Covid we have a flexible remote work policy. When employees do come into the office then we request the use of masks and proof of vaccinationExperience:Rest API: 5 years (Preferred)SQL: 5 years (Required)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Dittman-Incentive-Marketing/jobs/Data-Engineer-cc799fdddc45efbf?fccid=520ee01031869942&vjs=3,"New Brunswick, NJ 08901 08901",Data Engineer,Data Engineer
29,Modernagile Technologies,"Role – Data EngineerLocation – Remote Job – work in PST Time Zone (SFO,CA / Carlsbad, CA)ContractJob Description : 1. 8 to 12 years of total experience2. Data lead position – hands-on is required3. Hands-on experience in building Data Pipeline, designing and building data architecture on AWS.4. Must Have - AWS Stack with Glue, EMR, Spark S3 etc.5. Good to have - Snowflake stack with SnowPipe, Stored proc, Tasks, Streams, JSON processing etc, AirflowResponsibilities : -1. Work with customer in PST timezone2. Understand the current system, architecture, design3. Articulate requirements, work closely with customer4. Resolve queries and guide customer team on solution5. Work as SPOC for customer and Persistent team6. Onsite coordination, work with offshore architect and team7. Complementary to offshore architectBest Regards, Karun| Sr.Technical RecruiterE-Mail:  karun At fisecglobal.net | Website: http://www.fisecglobal.netJob Types: Full-time, ContractPay: $65.00 - $68.00 per hourSchedule:8 hour shiftDay shiftMonday to FridayExperience:Data Engineer: 9 years (Required)Lead: 1 year (Required)AWS Stack with Glue, EMR, Spark S3 etc: 3 years (Required)Snowflake: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Modernagile-Technologies/jobs/Data-Engineer-5cd295ca9a20194f?fccid=c96c1083e675f6d1&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
30,NR Consulting LLC,"Job Tittle: Data Engineer
Duration:6 Months
Location:Valley Forge,PA


Job Description:
Azure, SQL, Azure Data Factory, Data Orchestration, Data Bricks (preferred)",https://www.indeed.com/rc/clk?jk=4759a5aff578e811&fccid=3800c35bb29ac508&vjs=3,"Valley Forge, PA+6 locations",Data Engineer,Data Engineer
31,Brahma Consulting Group,"Title: Azure Data Platform EngineerLocation: Milpitas, CA (Remote)Duration: 6 monthsOverall skill setGeneral support and project work.Problem solving and analytics with the ability to research information and deduct solutions based on wide array of information they are sourcing.An enterprise level understanding of how system interconnect as it applies to BI and data analytics in an Azure Environment.Project work:Prior development experience with ability to integrate with Secret management systems (Vault), Message Brokers, Enterprise Search Index, API Gateway etc.Build CI/CD pipeline in Azure integrating CheckMarx, OSA and AppScan Standard toolsExperience in designing pipelines in Azure devops and hands on powershellArea:Databricks:An understanding of Clusters, libraries, job clusters, and how databricks is leverage in ETL/ELT.Understanding of python and how it is used for data transformation and integration with other systems.Understanding of Azure Databricks at the level of managing and configuring the product/workspace, and working knowledge of PySpark codeAzure Data factory:Previous experience with Azure Data Factory, and understanding of concepts including linked services, triggers, jobs, monitor, etc.Understanding of CICD devops and integrating ADFUnderstanding of how to configure the 60 plus inputs and outputs of ADF.Job Types: Full-time, ContractPay: $65.00 - $70.00 per hourSchedule:Monday to FridayExperience:Azure: 8 years (Preferred)Terraform: 4 years (Required)DevOps: 7 years (Required)Infrastructure: 7 years (Required)Work Location: Remote",https://www.indeed.com/company/Brahma-Consulting-Group/jobs/Azure-Data-Platform-Engineer-457296dc45f55f20?fccid=15965e7ec9a1df3d&vjs=3,Remote,Azure Data Platform Engineer,Data Engineer
32,Glassdoor,"Why Glassdoor?

Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.

Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; Uniontown, OH; London, UK; and Dublin, Ireland.

We are looking for a dedicated Data Engineer to join our growing Data Engineering team. The ideal candidate has experience in building scalable data pipelines that enable decision making, analytics, data science and data products. You must have strong, hands-on technical expertise in big data technologies and the ability to adapt to emerging technologies and fashion robust, scalable solutions.

We embrace a wide variety of technologies and work very closely with data scientists and business stakeholders to deliver impactful end to end solutions. If you are interested in a fast paced environment, the latest technologies, and fun data problems, come join us!

Compensation: $111,200.00 - $166,800.00

At Glassdoor, operating transparently is at the heart of everything we do. That's why we let job seekers know that salary ranges may differ for jobs depending on where these positions are filled. It's no secret that the cost of living and other economic considerations vary widely across the country and the world.

At Glassdoor we apply a ""tier"" system to promote consistency in level of compensation for where roles are filled in different locations. The tiers are as follows:

Tier 1 encompasses the highest salary range for states where large metropolitan areas are located (ie: San Francisco, New York City, Washington DC, & etc)
Tier 2 encompasses salary ranges 15% less than those in Tier 1 for states with mid-sized cities (ie: Chicago, Dallas, Portland, etc)
Tier 3 encompasses salary ranges 20% less than those in Tier 1 for states with smaller
cities (including our Ohio office, Nevada, etc)

You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
What you'll do:

Design and develop big data applications using a variety of different technologies.
Develop logical and physical data models for big data platforms.
Automate workflows using Apache Airflow.
Write data pipelines using Apache Hive, Apache Spark, Apache Kafka.
Create data ingress solutions on AWS using various technologies and techniques
Provide ongoing maintenance and enhancements to existing systems, and participate in rotational on-call support.
Learn our business domain and technology infrastructure quickly and share your knowledge freely and proactively with others in the team.

What you'll bring:

1+ years of hands-on experience with developing data warehouse solutions and data products.
6+ months of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc.
1+ years of hands-on experience in modeling and designing schema for data lakes or for RDBMS platforms.
Experience with programming languages: Python, Java, Scala, etc.
Experience with scripting languages: Perl, Shell, etc.
Practice working with, processing, and leading large data sets.
Excellent verbal and written communication skills.
Bachelor's Degree in Computer Science or equivalent experience.
Strong desire to add to our culture of diversity, equity and inclusion.

Nice To Have

Exposure to test driven development and automated testing frameworks.
Background in Scrum/Agile development methodologies.
Capable of delivering on multiple competing priorities with little supervision.
Experience working with REST APIs, Streaming APIs, or other Data Ingress techniques.
Familiarity with AWS or GCS technologies.
Benefits for U.S. based, regular, full-time employees:

Generous Restricted Stock Units (RSU): ***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.

You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.

Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, along with 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!

Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year

Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.

Our Company Values and Commitments

Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback survey responses are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.

Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.

US & Ontario Only

Where legally permitted, Glassdoor requires all employees who report to a workplace, or travel and/or attend in-person meetings, including client visits, to be fully vaccinated against COVID-19. For positions that can only be performed at a Glassdoor office, candidates must be fully vaccinated against COVID-19 and provide acceptable proof of vaccination before their first day of employment as a condition of employment. Glassdoor will consider requests for reasonable accommodation as required under applicable law. To qualify as being fully vaccinated against COVID-19, two weeks should have elapsed since receiving the second dose (or any government recommended booster shot) in a two-dose COVID-19 vaccine series, or since receiving a single dose (or any government recommended booster shot) in a single dose COVID-19 vaccine.",https://www.indeed.com/rc/clk?jk=26fd928cab3f4704&fccid=5a969b35c0256a8a&vjs=3,"San Francisco, CA+9 locations",Data Engineer,Data Engineer
33,CGATE Corporation,"Seeking a Data Engineer ASAP. This is 100% REMOTEDesired Skills and Experience: 2-4 years of professional Data engineering experienceStrong SQL and Python developmentExperience in production developmentAny cloud experience (AWS preferred)The ideal candidate will be extremely comfortable in SQL but also eager to learn new things. This platform is in the development stages. This engineer will be adding enhancements and different modules to this platform. This team runs in 2 week sprints- you will be given 2-3 tickets at one time and will be heads down development in SQL, Python, devJob Types: Full-time, Part-time, Contract, TemporaryPay: $40.00 - $60.00 per hourSchedule:8 hour shiftWork Location: Remote",https://www.indeed.com/company/CGATE/jobs/Data-Engineer-4761f6e29096a583?fccid=d73a76df2b43e110&vjs=3,Remote,Data Engineer,Data Engineer
34,Samiti Technology Inc,"Soft Skills: *Fluent in Spanish and EnglishGood time management skillsExperience in a customer facing role and interacting with clientsTechnical Skills: *Strong BigQuery and related GCP experience, certifications are a plusCloud Data EngineeringJava, Hadoop and Spark experience are a plusStrong SQL Optimization skills for BigQuerySoftware Engineering (Python)Modern Data Warehousing DesignData Analytics & Visualization (Tableau, PowerBI, Looker, etc)Project timeline:  4-6 monthsLocation:  LATAM Preferred, open to other regions. Must work CST hours.Job Types: Full-time, ContractSalary: $80.00 - $100.00 per hourSchedule:8 hour shiftExperience:Big Query: 4 years (Required)Tableau: 3 years (Required)Python: 7 years (Required)Data Developers: 5 years (Required)Microsoft Power BI: 3 years (Required)Spanish: 10 years (Required)Google Cloud Platform: 6 years (Required)Work Location: Remote",https://www.indeed.com/company/Samiti-Technology-Inc/jobs/Big-Data-Engineer-82c7d37a878e9cb4?fccid=df2cbdde79de42cc&vjs=3,Remote,Big Data Engineer,Data Engineer
35,Beacon Systems,"Position: Data EngineerLocation: Washington, DC.Contract: 1 Year + 4 Yearly extensions Minimum Qualifications:  Bachelor’s Degree in Related field.Skill Requirements: 3+ years of experience in the following:Experience managing code in GitHubExperience building applications in PythonExcellent communication skills and ability to work with customers, senior management, and other technical teams.Excellent documentation and organizational skills.1+ years of experience in the following:Experience deploying applications using AnsibleExperience using Mesos containersExperience managing Cites infrastructure and deploying with AnsibleExperience using AWS tools and servicesTechnical skills:  AWS, GitHub, PythonJob Types: Full-time, ContractSchedule:Monday to FridayWork Location: One location",https://www.indeed.com/company/Beacon-Systems/jobs/Data-Engineer-6f2878c29fc26671?fccid=c74c89aab3ed2e39&vjs=3,"Washington, DC",Data Engineer,Data Engineer
36,OwnBackup,"The JobData loss can be devastating. Whether it's caused by human error, bad code, rogue integrations, or malicious intent, all companies are at risk. That's why so many companies choose OwnBackup, the #1 cloud data protection platform on the Salesforce AppExchange and global leader in SaaS data protection. With nearly 4,000 customers, we are ranked on the Forbes Cloud 100 as one of the world's top private cloud companies and have raised nearly $500 million in funding from AIkeon Capital, B Capital Group, BlackRock Private Equity Partners, Insight Partners and others.We are hiring a Senior Big Data Engineer to work on our Data Platform to help us build a robust, scalable and performant solution to severe the demanding BI needs of our clients.Our tech stack is Python. As a SaaS platform we operate in a cloud native environment. Our products are developed on AWS and we support AWS and Azure. Our plans include containerization (Docker, Kubernetes) of our products to help us scale and meet demand from a fast growing client base. We aspire to remain cloud agnostic to support a varied client base. We believe in a culture of continuous improvement and constant refactoring.About YouAre a curious engineer who loves to discover how things work and determine how to improve themEnjoy learning new technologies with a view to defining optimal solutionsEnjoy working with data and solving for peta-scale problemsAre self motivated and have excellent analytical, problem solving and trouble-shooting skillsEnjoy working collaboratively with product and engineering teams to deliver value to clientsYour Work Experience8+ years of experience in software engineeringA minimum of 2 years' experience with Python4+ years of experience architecting products and solutionsExcellent understanding of data storage and data retrieval concepts and keen interest in cloud based systemsExperience with Big Data systems, Cloud Lake Formation designs and patternsExperience with Columnar data stores, optimized row-column data formats, parquet, ORC, etcExperience working with Scrum\Agile methodologyExperience developing in a SaaS environmentExperience with AWS highly desirableDeliver full features into production environmentYour Day-to-Day RoleWrite code with the highest standards with a strong focus on data privacy, and security controlsResearch, experiment with, and deliver big data, cloud data lake solutions with a strong security postureParticipate in design meetingsLead epics end-to-end - working with Product and the R&D teamConduct code reviews and always strive for qualityCoach junior members of the teamWork in highly dynamic agile environmentWhat We OfferUnlimited PTOGenerous Healthcare plans401(k) savings planCatered lunches in the office 5 days a weekA full fitness center in the officeShuttle bus service to and from New York City and Jersey CityImportant DetailsThis is a full-time position. The ideal candidate will work out of our Englewood Cliffs, NJ office to maximize collaboration and interaction with employees. OwnBackup is a global team, and as we're looking for top-talent, for the right candidate there can be an opportunity to work remotely in the following states: FL, GA, MA, NJ, PA.New employees also have the opportunity to attend our award-winning new hire bootcamp, which customizes the onboarding experience by role, provides new employees with invaluable hands-on training within their first few weeks at the company, and gives employees the chance to meet their new colleagues in-person.Creating an environment where employees thrive also means making sure every employee feels accepted. As we scale to help all types of companies protect precious data, our team must reflect the diversity we serve. OwnBackup is an Equal Opportunity Employer and we believe that every employee in the company brings a unique perspective that they can and should contribute in order to make an impact every day. We strive to be one team, one culture, and one family that builds trust through transparency. We do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, age, national origin, protected veteran status or disability status.A Bit About UsHave a look at our website and read through the AppExchange reviews to get to know OwnBackup a little better. Founded in 2015, OwnBackup is backed by top-tier venture capital firms and has experienced 100% year-over-year growth, establishing early market dominance in a Salesforce ecosystem that includes over 150,000 customers. And while we've experienced tremendous growth providing backup and recovery solutions for Salesforce customers, we are now offering our world-class data protection solution across other clouds, like Microsoft Dynamics 365. At OwnBackup, our vision is to empower customers to own and protect their data on any cloud platform.Ownbackup will consider qualified candidates in the following locations: Englewood Cliffs, NJ- OfficeFlorida- RemoteGeorgia- RemoteMassachusetts- RemotePennsylvania- Remote#LI-OnsiteJob Type: Full-time",https://www.indeed.com/company/OwnBackup/jobs/Senior-Big-Data-Engineer-a5b0b195e44957ec?fccid=633e9a44f65f08fc&vjs=3,"Englewood Cliffs, NJ",Senior Big Data Engineer,Data Engineer
37,Meridian Cooperative,"Meridian Cooperative is seeking a Data Engineer in Implementations to be part of the success in our Vision 2025 mission. This position may be remote or hybrid.About Us: Meridian Cooperative was formed in 1976 by a group of Electric Membership Cooperatives with a vision for a single enterprise solution provider to serve data processing, IT, and operational needs to cooperatives, public utility districts, and municipal utilities. Through carefully curated acquisitions and partnerships, Meridian has unified multiple leading-edge companies under its umbrella in order to truly execute that vision. Today, the Meridian Co-op Suite serves over 500 utilities across the country with industry leading enterprise software solutions.Role & Responsibilities: As part of Meridian Cooperative’s Implementation Services organization, the ideal candidate is a customer oriented, self motivated and intellectually curious team member who uses subject matter expertise to independently extract, transform and load billing and accounting data on a customer-by-customer basis for proprietary utility software applications.The candidate will be responsible for the data component of the implementation life cycle for utilities of low to moderate complexity.In addition to extracting transforming and loading customer data, the ideal candidate will also have the ability to analyze, identify, troubleshoot, research and resolve issues throughout the implementation life cycle including after the customer is live on the new software platform.The data analyst works effectively with other team members to perform in depth data analysis to write detailed project specifications which meet the customers business needs.The data analyst is responsible for identifying and working to resolve performance gaps and issues.The data analyst candidate uses organizational tools effectively in conjunction with sound judgement to independently make reasonable assumptions and decisions.Leads quality assurance testing throughout the implementation lifecycle utilizing both SQL as well as application knowledge to perform testing.Create SQL scripts and be able to analyze data for transfer and comparison.Delegates tasks to other team members as appropriate. With minimal oversight supports the implementation life cycle for the cross functional or interdepartmental teams.Exhibits excellent customer service skills with both internal and external customers, which includes cross functional teams.Ability to collaborate effectively in a team environment.Effectively communicates both verbally and nonverbally to both technical and non-technical internal and external customers.Possible travel to client sites (5%-10%)Skills: Must have: SQL, Relational Databases, Troubleshooting issues in a Client/Server environment, logistical analysis, AlteryxBachelor's Degree; with 3-4 years of equivalent professional experienceGuided by our Vision 2025 strategy, our leaders have developed 5 stretch goals to achieve a best-in-class organization for our members, customers, and employees.It is a priority that our future team member embraces these goals and demonstrate the behaviors to guide the company to realize success:Act as One Unified CompanyPut Our Members/Customers First in All We DoBe Known for Our Innovative Products and PeopleGrow our Reputation and Market ShareCompete on CultureWe offer:Competitive medical benefits package including health, dental, vision, and life401(k) with up to 9% in company contributions after one year$4,000 for tuition reimbursementWellness reimbursement plan and volunteer days*Unfortunately, we cannot sponsor employment Visas now or in the future for this position.Job Type: Full-timePay: $75,000.00 - $85,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveProfessional development assistanceReferral programRetirement planTuition reimbursementVision insuranceSchedule:8 hour shiftMonday to FridayApplication Question(s):What is your desired salary/income expectation?Education:Bachelor's (Preferred)Experience:SQL: 3 years (Preferred)Creating SQL Scripts: 1 year (Preferred)Alteryx: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Southeaster-Data-Cooperative-(SEDC)/jobs/Data-Engineer-a3aa702644f87305?fccid=0cb03f5e3e41c329&vjs=3,Remote,Data Engineer (SQL),Data Engineer
38,Warren County Public Schools,"Position Type:
 Technology/Information Systems Technician

Date Posted:
 2/21/2022

Location:
 TECHNOLOGY OFFICE

Closing Date:

 Open until filled


Salary Schedule - 170
Hours per day 7.5 hrs/day",https://www.indeed.com/rc/clk?jk=ad6a2b217d124be2&fccid=827f964ffcdffa63&vjs=3,"Bowling Green, KY 42101 42101",Data Engineer,Data Engineer
39,Technoviz LLC,"Job Description: Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destructionEnsures quality of technical solutions as data moves across multiple zones and environmentsProvides insight into the changing data environment, data processing, data storage and utilization requirements for the company, and offer suggestions for solutionsEnsures managed analytic assets to support the company’s strategic goals by creating and verifying data acquisition requirements and strategyDevelops, constructs, tests, and maintains architecturesAligns architecture with business requirements and use programming language and toolsIdentifies ways to improve data reliability, efficiency, and qualityConducts research for industry and business questionsDeploys sophisticated analytics programs, machine learning, and statistical methods to efficiently implement solutionsPrepares data for predictive and prescriptive modeling and find hidden patterns using data; in support of the Data Science teamUses data to discover tasks that can be automatedCreates data monitoring capabilities for each business process and works with data consumers on updatesAligns data architecture to the solution architecture; contributes to overall solution architectureDevelops patterns for standardizing the environment technology stackHelps maintain the integrity and security of company dataJob Title : RemoteJob Types: Full-time, ContractPay: $40.00 - $50.00 per hourSchedule:8 hour shiftExperience:Data Engineering: 8 years (Preferred)Data Programming languages: 5 years (Preferred)Data Developers: 5 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Technoviz-LLC/jobs/Data-Engineer-eb5dd0d5553c9d13?fccid=e355f62f90b3ee84&vjs=3,Remote,Data Engineer,Data Engineer
40,Appian Infotech,"Job Title:  Data Engineer (Data engineer – Splunk and Elastic Search)Job Location: Minneapolis, MN (Day one on-site)Experience Required 6+ yearsLong term contract C2C and W2COVID Vaccination RequiredJob Description : The Application System Administrator/Cloud Engineer will be responsible for working with Splunk/Elastic systems including vendor managed environment (SaaS) in Azure and AWS.· Supports the installation, configuration, and maintenance of purchased applications or application frameworks. Participates in the design configuration, problem diagnosis, maintenance, and installation of application systems and/or user group profiles. Validates system functionality after changes. Participates in testing and installing new software releases and application system upgrades. Analyzes, monitors, and fine-tunes the application system to achieve optimum performance levels. Works with internal infrastructure teams and vendors to resolve issues with hardware and software. Maintains a comprehensive operating system hardware and software configuration database/library of all supporting documentation to ensure data integrity. Participates with application system problem resolution by working with application developers, vendors and internal infrastructure team members to troubleshoot. Ability to perform periodic on-call and production support for the Splunk / ElasticSearch environmentsBasic Qualifications- Bachelor's degree or equivalent work experience- At least 6 years’ experience with developing and implementing applicationsPreferred Skills/Experience:- Usage of dashboard tools such as Tableau, Splunk, Elastic Kibana, Grafana, Microsoft Power BI- Kafka, Kubernetes and Public cloud (Azure and AWS)- Understanding of data formats such as XML and JSON- Experience with ETL processes (retrieving data from sources, transforming the data, storing data)- Experience with data structures, data types, indexes, databases- Experience architecting /engineering solutions involving large amount of log volumes and associated Monitoring/observability- Scripting experience such as Perl, Python, Powershell - Advanced Microsoft Excel skills-Modeling: data, process, events, objects -IT standards, procedures, policy -Change control -System development life cycle -Application testingJob Types: Full-time, ContractSalary: $45.00 - $50.00 per hourSchedule:8 hour shiftApplication Question(s):Experience Splunk AWS and Elastic SearchExperience:Informatica: 5 years (Preferred)Data warehouse: 5 years (Preferred)Work Location: One location",https://www.indeed.com/company/Appian-Infotech/jobs/Data-Engineer-e0ccabced7facaa2?fccid=c4316b0dde48e3f6&vjs=3,"Minneapolis, MN 55111 55111",Data Engineer,Data Engineer
41,ASCENDING,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.
Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.
Requirements:
Minimum 4 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.
Thanks for applying!",https://www.indeed.com/rc/clk?jk=ac44d7ea3e08969a&fccid=d2ec638c4f1a5bd5&vjs=3,"Remote in Rockville, MD",Data Engineer,Data Engineer
42,Qorvo,"Data Engineer
Experience Level: Recent College Grad
Job Type: Full-Time
Location:
NC - Greensboro (HQ), US
Requisition ID: 3464
SUMMARY
This position, Data Engineer, supports business analysts, product developers, manufacturing engineers, and data scientists by developing and maintaining an automated data pipeline that unifies data acquired from multiple sources. Data engineers work collaboratively with both producers and consumers of data to define requirements and develop strategies for data acquisition and usability.

RESPONSIBILITIES
Use SQL, Python / Pandas, Apache Spark and other data manipulation tools to develop, construct, test, and maintain databases and big data processing systems in support of analytics and applications.
Unify data from disparate sources while assessing and improving data quality and usability. Prepare data for use in diagnostic, predictive, and prescriptive modeling.
Collaborate with BI users, applications developers, and data scientists to establish and refine data requirements.
Implement data movement and transformations in the context of an automated data pipeline. Support pipeline operations.
Assist data scientists in feature engineering and model deployment.
Qualfied candidates will have 0-2 years' experience and a minimum of Bachelors Degree in an IT related field of studies.

MAKE A DIFFERENCE AT QORVO

We are Qorvo. We do more than create innovative RF solutions for the mobile, defense and infrastructure markets – we are a place to innovate and shape the future of wireless communications. It starts with our employees. As a unified global team, we bring a commitment to excellence, growth and a passion for creating what's next. Explore the possibilities with us.

We are an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcome all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, military or veteran status, physical or mental disability, genetic information, and/or any other status protected by law.

Qorvo is an E-Verify Employer. For more information, please see the Right to Work and E-Verify Participation posters.",https://www.indeed.com/rc/clk?jk=e3c6e9b353a6e94b&fccid=f0f57785b56c2161&vjs=3,"Greensboro, NC 27409 27409+1 location",Data Engineer,Data Engineer
43,Paramount,"Paramount (NASDAQ: PARA; PARAA) is a leading global media and entertainment company that creates premium content and experiences for audiences worldwide. Driven by iconic studios, networks and streaming services, its portfolio of consumer brands includes CBS, Showtime Networks, Paramount Pictures, Nickelodeon, MTV, Comedy Central, BET, Paramount+, Pluto TV and Simon & Schuster, among others. The company delivers the largest share of the U.S. television audience and boasts one of the industry's most important and extensive libraries of TV and film titles. In addition to offering innovative streaming services and digital video products, Paramount provides powerful capabilities in production, distribution and advertising solutions.

Overview :
We are seeking a curious and motivated Data Engineer to join our Data Strategy & Operations team at Paramount. This position supports the data ingestion needs within Corporate Research. The role services Brands across the Paramount portfolio and works across data from Streaming, Social, Digital and external platforms. This person will develop and support data pipelines to pull data through various ingestion methods (API, feed, S3, Snowflake, etc.) based on business needs. They will also work closely with datasets which are then used as inputs into downstreaming data flows and dashboarding/reporting.

Day-to-Day Responsibilities:
Maintain pre-existing data pipelines
Develop & support new ingestion pipelines
Expand upon existing processes to pull in new metrics/data needs based on business requirements
Troubleshoot failures, data delays, and data anomalies to ensure data accuracy.
Monitor jobs and find opportunities for efficiency in scheduling, dependencies, and functionality.
Optimize data processes to ensure data timeliness and completeness.

Qualifications:
Undergraduate degree
2+ years’ of working experience with Python - strong internship and project experience will be considered
Demonstrated ability to import, clean, transform, and aggregate data with the purpose of translating large data sources into meaningful datasets for decision making purposes.
Familiarity with data transfer methods/protocols required (including but not limited to APIs, feeds, S3, S/FTP, Google Cloud Storage, querying data out of data warehouses, snowflake, etc.)
AWS or GCP knowledge
Familiar with ETL tools
Demonstrated usage of engineering best practices.
Curiosity about all things data related

Nice to have:
Experience in Linux/UNIX environments, SQL & R
Experience contributing and collaborating via Git.
A broad understanding of datasets from streaming, social, and digital platforms including but not limited to Adobe Analytics, Facebook, YouTube, Twitter, Instagram, etc.
Familiar with SVOD & AVOD datasets.
Experience with RESTful services
Familiar with Domo or other BI tools.

Paramount is an equal opportunity employer (EOE) including disability/vet.

At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status.

If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to viacomaccommodations@viacom.com. Only messages left for this purpose will be returned.",https://www.indeed.com/rc/clk?jk=e1634a52b75084b5&fccid=4f8daa48ef966ab5&vjs=3,"New York, NY 10036 10036+4 locations",Data Engineer,Data Engineer
44,Orgspire Inc,"Work Authorization : Only Independent ContractorKey Skills:  Azure , DatabricksJob Types: Full-time, ContractPay: $55.00 - $60.00 per hourSchedule:8 hour shiftExperience:Azure: 4 years (Required)Work Location: Remote",https://www.indeed.com/company/Orgspire-Inc/jobs/Data-Engineer-657023b4fea4a39f?fccid=48b059808c2b2e29&vjs=3,Remote,Data Engineer,Data Engineer
45,NECI,"About NECINECI, an Emerson Impact Partner, is the leading Digital Automation solutions provider transforming manufacturing, lab operations, process development and process control across a range of process industries in New England. NECI relentlessly seeks to drive the outcomes that ‘change the game’ for our clients and is seeking team members to join in our mission.The RoleThe Data Integration Engineer is a customer facing individual capable of executing projects in the Data Historian space. This individual will be responsible for coordinating with customer site contacts and implementing Data Historian solutions in a challenging real-time process environment.Project execution tasks include the installation and configuration of complete OSIsoft PI systems. This ranges from the initial PI Server Installation to the configuration of data collection from a wide variety of possible 3rd party data sources.He/she will be responsible for implementing the configuration and/or the custom software components necessary to meet the project requirements for the projects to which he/she is assigned.How You’ll Be SuccessfulAbility to troubleshoot Windows Server/Activity Directory/Domain IssuesWilling to keep up with and learn the ever evolving OSIsoft Product SuiteAble to work in both team environments as well as unsupervised in a customer environmentAbility to collect requirements for the data integration of process control systems/equip and configure the appropriate interface as requiredFamiliarity of other Data Historian Systems outside of OSIsoft PICooperate with others to uphold NECI’s Total Customer Commitment.Have prior experience utilizing PI Organizational and Visualization tools such as PI Asset Framework and PI Coresight/Vision.Ability to quickly troubleshoot and resolve issues.What You’ll DoInstallation, Configuration, and maintenance of new or existing OSIsoft PI SystemsPI Tag and Asset Framework ConfigurationPI Data Analyzation/Visualization utilizing PI Asset Framework and CoreSight/VisionWhat You’ll BringBachelor of Science in Computer Engineering, Science, or Chemical Engineering2 to 3 years’ experience in a combination of process control, automation, software development or information technology management in pharmaceuticals, biotechnology or other life sciences industries.Demonstrated success in identifying requirements and subsequent development and implementation of PI Installation and Integration ActivitiesCompletion of the following OSIsoft training or demonstrable experience - PI System Architecture, Planning, and Implementation, Building PI System Assets and Analytics with PI AF, PI System Administration for IT Professionals.Here’s What You’ll GetHighly competitive Medical and Dental InsuranceFlexible Spending Accounts for medical expenses and dependent care expensesUnlimited PTO policy401(k)Employee Stock Ownership Plan (ESOP)Employee Referral BonusProfessional Development ReimbursementNECI paid Basic Life & Short-Term, Long-Term Disability InsuranceVoluntary Benefits - Accident, Term Life, Whole Life InsuranceFlu Shot Clinic & First Aid/AED/CPR ClassesMust be currently authorized to work in the United States.Follow NECIFacebookLinkedInGlassdoorPolicy on Third-Party Unsolicited Resume Submissions:  Please note that any third-party unsolicited resume submissions will immediately become the property of NECI. NECI will not pay any fee to a submitting employment agency, person, or entity unless a signed agreement is established.Please Note: NECI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, disability, protected veteran status or any other characteristic protected by law.Job Type: Full-time",https://www.indeed.com/company/NECI/jobs/Data-Integration-Engineer-4ab7dd843e91e228?fccid=bd7c40b08330c9d1&vjs=3,"Mansfield, MA 02048 02048+1 location",Data Integration Engineer,Data Engineer
46,Miller Nash Graham & Dunn,"Data Engineer

If you are a skilled problem-solver who loves to build things that are useful for others, we want you to be a part of our team! This remote position is responsible for leveraging large volumes of data to provide critical financial reports and analysis to the firm’s partners in an effort to ensure the continued financial health of the firm.
Some responsibilities include:

Automating work processes that have historically been manual in nature.
Performing statistical analysis.
Data mining and query authoring (SQL).
Creating data visualizations.
Creating, monitoring, and updating automated delivery schedules.
Verifying the integrity of data used, and more.

For more information about this opportunity and to apply, visit our Job Board: MN Job Board",https://www.indeed.com/rc/clk?jk=824025f8b16cb70a&fccid=01944cf9cea74457&vjs=3,"Remote in Portland, OR 97204 97204",Data Engineer,Data Engineer
47,Millennial Tech,"Our company is looking for an on-site Data Center Engineer to help us build and manage our multiple active data centers. As part of our data center team, you will assist the lead engineer with daily tasks. In addition, you will perform quality assurance tests and troubleshoot hardware issues to help us improve and maintain our data center infrastructure. Our ideal candidate has experience in wiring and hardware and can ensure that our standards are maintained, and all procedures are followed to guarantee uninterrupted operations in our 24/7 environment. Aid all groups with equipment in the Datacenter. We strongly prefer candidates with a background in hardware and cabling, and it is essential that you have a firm grasp of network wiring principles and architecture.
Duties and Responsibilities
Design and manage data center hardwareTroubleshoot issues as they occurMonitor power and cooling for issuesOrganize and clean any work areasIdentify reliability risksLabel and clean cabling for new installsUpdate wire lists and rack layoutsCreate operational proceduresProvide technical support for several groupsRemove and replace drives as needed

Requirements and Qualifications
Willing to cover many sites as neededExperience working with IT systemsStrong organizational skillsNetwork and power capacity planning experienceA solid understanding of network and data cablingAnalytical skillsTroubleshooting skillsAbility to lift 50LbsParticipate in a 7X24 on-call rotation.",https://www.indeed.com/rc/clk?jk=d1c3ec7340931cae&fccid=33f331c5e7f1da46&vjs=3,"Boston, MA 02163 02163 (Allston-Brighton area)",Data Center Engineer,Data Engineer
48,Twitch,"About Us

Launched in 2011, Twitch is a global community that comes together each day to create multiplayer entertainment: unique, live, unpredictable experiences created by the interactions of millions. We bring the joy of co-op to everything, from casual gaming to world-class esports to anime marathons, music, and art streams. Twitch also hosts TwitchCon, where we bring everyone together to celebrate, learn, and grow their personal interests and passions. We're always live at Twitch. Stay up to date on all things Twitch on LinkedIn, Twitter and on our Blog.

About the Role

Twitch is building the future of interactive entertainment. The Data Platform team develops and operates the platform that enables Twitch's data-informed decision-making. This ranges from the machine learning models that power Twitch Search to empowering self-serve analytics for business staff. Our data pipeline receives over 220 billion events a day, and we forward live data to hundreds of systems as well as powering decisions at all levels of Twitch. We provide tools to ingest, store, transform, move, and understand data.

As a DevOps Engineer on Data Infrastructure, you will deliver the next generation of Twitch data technology. You will work on platforms which are reliable and scale and automate processes to allow the team to operate more efficiently. You will work with AWS, provisioning infrastructure via code and building automated CICD pipelines. You will report to the Engineering Manager for Data Platform. We're based in San Francisco, but are open to you working from Vancouver, BC, Seattle, WA, or Los Angeles, CA.

You Will:
Be a strong engineering voice, advising and driving the Data Platform team's technical vision and strategy.
Improve operational practices for distributed, high-throughput, and low-latency data infrastructure systems with a strong focus on availability, resilience, and durability
Collaborate with engineers to understand their build pipelines and tooling needs.
Setup and maintain build and Continuous Integration/Continuous Deploy systems.
Provision infrastructure as code.
Develop and maintain automated release systems.
Setup and integrate new monitoring and alerting systems.
Review code to ensure it is high quality, efficient, well-tested, and documented.
Improve the reliability, flexibility, and scalability of our existing tools.
You Have:
3+ years of experience in software development, DevOps engineering, or site reliability engineering.
2+ years experience in Go or Python.
Experience using an Infrastructure as Code Tool (preferably CDK)
Experience working with Amazon Web Services or other cloud provider
Bonus Points
You've made petabytes of data usable.
You contribute to open source.
You've served in a team lead capacity in the data or data infrastructure space.
You have one or more AWS Certifications.
Perks
Medical, Dental, Vision & Disability Insurance
401(k), Maternity & Parental Leave
Flexible PTO
Commuter Benefits
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.),
Breakfast, Lunch & Dinner Served Daily
Free Snacks & Beverages

Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",https://www.indeed.com/rc/clk?jk=6aa650150b2e7079&fccid=e21f6affbb4bb0b2&vjs=3,"San Francisco, CA",DevOps Software Engineer - Data,Data Engineer
49,ASCENDING LLC,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.
Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.
Requirements:
Minimum 4 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.
Thanks for applying!
4jkNVq1nAC",https://www.indeed.com/rc/clk?jk=b6345c92a0c782ac&fccid=8012a5f0637a3b44&vjs=3,"Remote in Rockville, MD 20850 20850",Data Engineer,Data Engineer
50,Tekcogno,"Role: Data EngineerLocation: Remote, BUT, must go onsite for about a week for training.Visa: Only USC/GCMust-Haves/ Top 3 skills1. 5 years experience with Data Warehouse (Azure)2. 5 years Data Modeling3. 5 year utilizing SQLOpen to Remote- will have to come onsite for training.Must have: · SQL server· SSIS· Power BI on the front end· Azure (the tool set: Data Warehouses / Data Factory)Nice to have:· Cloud Analytics· Python· SAP S4/HANA – Analytics Cloud· Agile environmentSUMMARYThe Senior Data Engineer will play a pivotal role on the Vari Technology Team as a Thought Leader in our Data Practice. The Senior Data Engineer will be responsible for integrating, modeling, and designing analytic applications to create actionable insights for the organization. The Senior Data Engineer will play a broad role interfacing with multiple subdepartmental teams and technologies, including SAP and Salesforce. In addition to these enterprise applications, Vari is primarily a Microsoft (Azure Data Factory and Data Lake, SQL, and PowerBI, etc.) and SAP (SAP Analytics Cloud) ecosystem. The position will analyze, design, build, and support the data warehouse (Microsoft SQL Server/Azure), analytical models in SSAS, and support reporting in PowerBI. This position will report to the Director of Platform Operations and Data but work daily with the other Technology Team members, the Business Insights Team, and peer departments at Vari. We are looking for someone who is passionate about delivering excellence both tactically and through collaboration, strong partnerships, and exemplary customer service to internal stakeholdersWHO YOU ARE You: Raise the Bar by taking the initiative to achieve results and make excellence your goal.Create Lifelong Fans by making customer experience your top priority.Are Authentic and say what you mean, share how you feel, and are your true self.Are a Team Player and foster a sense of community by recognizing and valuing the unique contributions of our team?  * Believe it’s Possible and are the optimist who looks at challenges as opportunities and brings solutions to the table.Embrace Change and welcome change that comes your way learning from it. resultsWHAT YOU’LL DOCreate and maintain Vari’s data architecture using Azure Data Factory, Azure Data Lake, Microsoft SQL Server, SQL Data Warehouse, etc.Develop ETL processes using SQL, SSIS, Azure Data Factory with consideration to fault tolerance, error logging, auditing, and data quality.Build/automate schemas and cubes that will support reports/dashboard (in Power BI and Microsoft Analysis Services) consumption to key business performance metrics.Leverage strong data modeling skills to include data quality, source systems analysis, business rules validation, source to target, mapping design, performance tuning, and high volume data loads.Collaborate with Technology Product Owners, Business Analysts, the Business Insights Team, and Peer Departments to fully understand the business requirements for each data analytics and reporting initiative and develop solutions that deliver actionable data to the stakeholders to position them to make effective decisions about the business.Balance needs of multiple stakeholders. Gain buy-in from groups who may be resistant to change.Champion continuous process improvement, performance management, and metrics-driven results.HOW YOU’LL GET THEREBachelor’s degree in Business, IT, or related field required.Minimum 5 Years of Data Warehouse or Azure Synapse Analytics experienceMinimum 5 Years data modeling experience.Minimum 5 Years Microsoft SQL experience.Minimum 3 Years Azure Data Factory CI/CD with DevOps pipelines and Azure Data Lake experience.Minimum 3 Years Master Data Management (MDM) experience.Minimum 3 Years Power BI (or similar) Reporting Tool experience.Minimum 2 Years Python experience.  * Salesforce and SAP (S4 HANA) experience highly desired.SAP Analytics Cloud experienceAbility to manage projects with high complexity.Ability to foster long-term partnerships and work well in team environments.Well-organized, with attention to detail, and self-motivated.Accepts change positively and may help others adapt to change.Outstanding written and verbal communication skills.Comfortable multi-tasking on many projects at one time and work effectively to complete them.Proficiency in Atlassian (Jira, Service Desk, Confluence) preferred.Working in Agile environment preferred.Proficiency in MS Office (Word, Excel, Project, PowerPoint, SharePoint) preferredJob Types: Full-time, Contract, InternshipPay: $83,160.00 - $135,000.00 per yearSchedule:8 hour shiftApplication Question(s):Will you be comfortable going onsite for the 1st week for training.Therefore, it will be remoteWhat's your visa status and current locationExperience:Data Warehous/Azure Synapse Analytics: 5 years (Required)SQL: 5 years (Required)Azure Data Factory & lake: 3 years (Required)MDM: 1 year (Required)Power BI/Similar tool: 1 year (Required)Python: 1 year (Required)SAP S/4HANA: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Tekcogno/jobs/Data-Engineer-6d8cfd538df0694c?fccid=360f48ee217c77a6&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
51,Toyota,"Who we’re looking for
Toyota’s Digital Solutions Department is looking for a passionate and highly motivated Data Engineer.
The primary responsibility of this role is to solve complex data problems which will be used to deliver insights to help solve organizational goals for R&D. The Data Engineer will create data products used to increase productivity and deliver data pipelines for analytically driven use cases.
Reporting to the Senior Manager, the person in this role will support the Digital Solutions department's objective of delivering analytic insights through a defined model development lifecycle which utilizes vehicle data (CAN300 communication bus and other vehicle systems) along with other appropriate sources of data as defined by the business requirements.
What you’ll be doing
Partner with key business stakeholders to understand priorities, identify strategic initiatives, and develop effective and timely analytic solutions
Statistical analysis as needed to support these of business initiatives, such as customer segmentation and behavioral models
Presentation and communication of analytic results utilizing appropriate tool(s) (Power BI, Tableau, R, Python, other)
Review existing analytic processes to identify best practices and opportunities for improvement
Providing valuable input into business intelligence roadmaps and development plans
Mentor team members regarding analytic best practices
Stay abreast of data governance and data management best practices and continually recommend and implement process improvements
Support Data Governance steward with initiatives related to data governance and the maturation of the data governance organization
Project Owner on company-wide data analytics initiatives
Travel 5%
Overtime 20%
What you bring
Bachelor’s degree (or higher)
Professional relevant work experience
Demonstrated ability to communicate effectively with all levels of an organization, both technical and non-technical team members
Knowledge of predictive analytics techniques and statistical diagnostics of models.
Expert resource for tool development
Demonstrated ability to exchange ideas and convey complex information clearly and concisely.
Has a value-driven perspective with regard to understanding of work context and impact.
Proficient in an object-oriented programming language (Python, R, SAS or other).
 Willingness to learn new languages and technologies is mandatory
What you may bring
Degree in Statistics, Math, Economics/Econometrics or other related fields of study
Skill in creating mathematical models, experience in AI, machine learning, engineering design
Proven expertise in delivering data driven applications in the form of APIs, dashboards, or software packages
Healthy curiosity about the industry and trends in data analytics
Toyota ECU Design experience including Deep knowledge of the CAN bus
Progressive years of experience in a mathematically driven field of study or career
Toyota work experience
What we’ll bring
During your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include:
A work environment built on teamwork, flexibility and respect
Professional growth and development programs to help advance your career, as well as tuition reimbursement
Vehicle purchase & lease programs
Comprehensive health care and wellness plans for your entire family
Flexible work options based on business needs
Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute
Paid holidays and paid time off
Referral services related to prenatal services, adoption, child care, schools and more
Flexible spending accounts
Relocation assistance (if applicable)
To save time applying, Toyota does not offer sponsorship of job applicants for employment-based visas or any other work authorization for this position at this time.",https://www.indeed.com/rc/clk?jk=3c371946a87eeef5&fccid=90f0cbc4a30f8dba&vjs=3,"Saline, MI 48176 48176+1 location",Data Engineer,Data Engineer
52,Warren County Public Schools,"Position Type:
 Technology/Information Systems Technician

Date Posted:
 2/21/2022

Location:
 TECHNOLOGY OFFICE

Closing Date:

 Open until filled


Salary Schedule - 170
Hours per day 7.5 hrs/day",https://www.indeed.com/rc/clk?jk=ad6a2b217d124be2&fccid=827f964ffcdffa63&vjs=3,"Bowling Green, KY 42101 42101",Data Engineer,Data Engineer
53,Akshaya,"Position : Data Engineer Client : AMAT/TechMLocation: Santa Clara, CADuration: Long Term ContractRequirements:a. Data Engineer with Backend engineering roleb. should possess strong Python skillsc. should have experience in creating restful API’s ( having experience with FastAPI framework is major advantage)d. should have experience in dealing with Databases (experience with PostGre/ Clickhouse)Job Types: Full-time, ContractSalary: $70.00 - $80.00 per hourBenefits:Health insuranceSchedule:8 hour shiftExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One locationSpeak with the employer+91 +15109399872",https://www.indeed.com/company/AKSHAYA/jobs/Senior-Data-Engineer-69d86d855e088ba0?fccid=384b383b35930c68&vjs=3,"Santa Clara, CA",Senior Data Engineer,Data Engineer
54,Nespon IT Services,"We are looking for an SME on Cloud Data Warehouse.Must have strong experience on Azure Synapse and SnowflakeC2C and 1099 is allowed12 months contract, 100% remoteJob Types: Full-time, ContractPay: $65.00 - $75.00 per hourSchedule:8 hour shiftMonday to FridayExperience:Azure Synapse: 5 years (Preferred)Snowflake: 5 years (Preferred)Cloud Data warehouse: 5 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Nespon-IT-Services/jobs/Azure-Data-Engineer-dac79b99d21c5075?fccid=a913cc7a16012b8a&vjs=3,Remote,Azure Data Engineer,Data Engineer
55,Star Technology Solutions Inc,"__**ONLY W2Looking for a Sr. Data Engineer (focused within AWS Cloud Environments, with tech skill sets of Java, Lambda Web Services, DynamoDB, MySQL with RDS, and AWS cloud services) Top requirements would be having experience with Java Development and supporting Lambda / DynamoDB requirementsJob Types: Full-time, Part-time, ContractPay: $45.00 - $55.00 per hourSchedule:8 hour shiftWork Location: Remote",https://www.indeed.com/company/Star-Technology-solutions-Inc/jobs/Senior-Data-Engineer-ab8cab8ed3c9e905?fccid=ea1bfba0046c4925&vjs=3,Remote,Sr. Data Engineer,Data Engineer
56,TTS Solutions INC,"Client: ------Role: Data EngineerDuration: 12+Months (Contract W2)Location: Remote to startJob Description: Bachelor’s Degree in Computer Science, Engineering, Math or related field is required Master's degree is preferred3+ years of experience, specializing in BI solution development3+ years of SQL experience (No-SQL experience is a plus)2+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience designing, building, and maintaining data processing systemsExperience in interpretation of business needs from requests, and rapidly implement effective technical solutions3+ years of work experience in data management disciplines including data integration, data modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasksStrong experience with popular database programming languages including SQL, PL/SQL and others for relational databasesExperience sourcing data via a variety of sources like REST web services, MS SQL ServerExperience using software version control tools (Git, Azure DevOps, Subversion, etc.)Advanced level of SQL and query performance tuning techniques for Data Integration and ConsumptionSolid understanding of BI and analytics landscape, preferable in large-scale development environmentsExperience with or knowledge of Agile Software Development methodologiesBackground in Cloud Data Warehousing principles and Data Modeling preferredStrong aptitude for learning new technologies and analytics techniquesCommunication skills and ability to develop and present solutions to all levels of managementMust be able to interact effectively and patiently with customers especially while under pressureThe ability to work on multiple projects/tasks simultaneously to meet project deadlines for self and others as requiredAbility to establish and maintain positive working relationships with other employees.Strong analytical and problem-solving skills, with strict attention to accuracy and detail, and the ability to evolve data into knowledgePassion for delivering highly available, robust BI solutionsKnowledge of best practices and IT operations in an always-up, always-available serviceJob Types: Full-time, ContractPay: $70,000.00 - $85,000.00 per yearSchedule:8 hour shiftExperience:Informatica: 1 year (Required)SQL: 1 year (Required)Data warehouse: 1 year (Required)Work Location: Remote",https://www.indeed.com/company/TTS-Solutions-Inc/jobs/Data-Engineer-42b41911807f2b1f?fccid=012a0d5734a3318c&vjs=3,Remote,Data Engineer,Data Engineer
57,League Inc.,"Help Us Shape the Future of Healthcare

At League, we're big on building connections - both through our product and with each other. Our platform is consumer centric, personalized and always on. It's a front door to healthcare that empowers people to live healthier, happier lives. Every day.

When you feel connected to our mission, values and the work you do, you're driven to perform your best every day. When you feel connected to people you work with, you're able to build meaningful relationships that last. Together, we share the ultimate goal of delivering better health outcomes for everyone.

The world has changed, and so has the way we work. We believe you can work where you work best (whether it's in the office all the time, a few days a week, or none of the time), and still feel connected to our mission, values, purpose, and each other. League is headquartered in Toronto with a second office in Chicago and we believe in empowering Leaguers to work their way—anywhere that works for them in Canada and the US.

All Leaguers have League life moments – moments in our careers that connect us to League's mission and values, to each other and to our amazing work – every day.

Will your first League life moment be meeting our team? We can't wait to meet you.

As a Data Engineer, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross product and feature teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software engineers, data analysts, BI engineers and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

In this role you will:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using cloud-native and big data principles.
Build the framework to enable analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Marketing teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and regions.
Create data processes and tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Nice to Have:

Advanced working knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.
Experience building and optimizing big data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with message queuing, stream processing, and highly scalable big data stores.
Experience with end-to-end API design, implementation and sustainment.
Strong analytic skills related to working with unstructured and structured datasets.
Project management and organizational skills to self-scope and direct on a day-to-day basis.
Experience supporting and working with cross-product/feature teams in a dynamic environment.
The ideal candidate should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, Beam, etc.
Experience with both relational SQL and NoSQL databases, including Postgres, MongoDB, etc.
Experience with data pipeline and workflow management tools such as Apache Airflow, etc.
Strong Experience with Code Management + DevOps: Github, Github Actions, Concourse CI, Terraform, Atlantis, etc.
Experience with Google Cloud Platform (GCP) cloud services: BigQuery, DataFlow, PubSub, CloudSQL, Cloud Storage, Cloud Composer, VertexAI
Knowledge of the FHIR standard and/or healthcare data
Experience with object-oriented/object function scripting languages: Python, Go, Scala, Java, C++, etc.
At League, everyone is welcome. Diversity makes us better, and we believe in building diversity across cultural identity, gender, sexual orientation, age and education, thought and experience.

We are an equal opportunity employer, and we are committed to working with applicants requesting accommodation at any stage of the hiring process.

Here are some additional resources to learn more about League:
Learn more about us in this short video!
League, Cleveland Clinic collaborate to make employees healthier across North America
League and Loblaw bring next-generation digital health platform to customers
League Completes Workday Approved Integration

Given the current public health guidance on COVID-19 and to help support office health and safety, at this time, it is our company policy to require all Leaguers to be fully vaccinated to attend our offices in person.",https://www.indeed.com/rc/clk?jk=71dd5da3c66dbc70&fccid=533b130aca0b9cfa&vjs=3,"Remote in Chicago, IL+6 locations",Data Engineer (Remote),Data Engineer
58,Tekcogno,"Role: Data EngineerLocation: Remote, BUT, must go onsite for about a week for training.Visa: Only USC/GCMust-Haves/ Top 3 skills1. 5 years experience with Data Warehouse (Azure)2. 5 years Data Modeling3. 5 year utilizing SQLOpen to Remote- will have to come onsite for training.Must have: · SQL server· SSIS· Power BI on the front end· Azure (the tool set: Data Warehouses / Data Factory)Nice to have:· Cloud Analytics· Python· SAP S4/HANA – Analytics Cloud· Agile environmentSUMMARYThe Senior Data Engineer will play a pivotal role on the Vari Technology Team as a Thought Leader in our Data Practice. The Senior Data Engineer will be responsible for integrating, modeling, and designing analytic applications to create actionable insights for the organization. The Senior Data Engineer will play a broad role interfacing with multiple subdepartmental teams and technologies, including SAP and Salesforce. In addition to these enterprise applications, Vari is primarily a Microsoft (Azure Data Factory and Data Lake, SQL, and PowerBI, etc.) and SAP (SAP Analytics Cloud) ecosystem. The position will analyze, design, build, and support the data warehouse (Microsoft SQL Server/Azure), analytical models in SSAS, and support reporting in PowerBI. This position will report to the Director of Platform Operations and Data but work daily with the other Technology Team members, the Business Insights Team, and peer departments at Vari. We are looking for someone who is passionate about delivering excellence both tactically and through collaboration, strong partnerships, and exemplary customer service to internal stakeholdersWHO YOU ARE You: Raise the Bar by taking the initiative to achieve results and make excellence your goal.Create Lifelong Fans by making customer experience your top priority.Are Authentic and say what you mean, share how you feel, and are your true self.Are a Team Player and foster a sense of community by recognizing and valuing the unique contributions of our team?  * Believe it’s Possible and are the optimist who looks at challenges as opportunities and brings solutions to the table.Embrace Change and welcome change that comes your way learning from it. resultsWHAT YOU’LL DOCreate and maintain Vari’s data architecture using Azure Data Factory, Azure Data Lake, Microsoft SQL Server, SQL Data Warehouse, etc.Develop ETL processes using SQL, SSIS, Azure Data Factory with consideration to fault tolerance, error logging, auditing, and data quality.Build/automate schemas and cubes that will support reports/dashboard (in Power BI and Microsoft Analysis Services) consumption to key business performance metrics.Leverage strong data modeling skills to include data quality, source systems analysis, business rules validation, source to target, mapping design, performance tuning, and high volume data loads.Collaborate with Technology Product Owners, Business Analysts, the Business Insights Team, and Peer Departments to fully understand the business requirements for each data analytics and reporting initiative and develop solutions that deliver actionable data to the stakeholders to position them to make effective decisions about the business.Balance needs of multiple stakeholders. Gain buy-in from groups who may be resistant to change.Champion continuous process improvement, performance management, and metrics-driven results.HOW YOU’LL GET THEREBachelor’s degree in Business, IT, or related field required.Minimum 5 Years of Data Warehouse or Azure Synapse Analytics experienceMinimum 5 Years data modeling experience.Minimum 5 Years Microsoft SQL experience.Minimum 3 Years Azure Data Factory CI/CD with DevOps pipelines and Azure Data Lake experience.Minimum 3 Years Master Data Management (MDM) experience.Minimum 3 Years Power BI (or similar) Reporting Tool experience.Minimum 2 Years Python experience.  * Salesforce and SAP (S4 HANA) experience highly desired.SAP Analytics Cloud experienceAbility to manage projects with high complexity.Ability to foster long-term partnerships and work well in team environments.Well-organized, with attention to detail, and self-motivated.Accepts change positively and may help others adapt to change.Outstanding written and verbal communication skills.Comfortable multi-tasking on many projects at one time and work effectively to complete them.Proficiency in Atlassian (Jira, Service Desk, Confluence) preferred.Working in Agile environment preferred.Proficiency in MS Office (Word, Excel, Project, PowerPoint, SharePoint) preferredJob Types: Full-time, Contract, InternshipPay: $83,160.00 - $135,000.00 per yearSchedule:8 hour shiftApplication Question(s):Will you be comfortable going onsite for the 1st week for training.Therefore, it will be remoteWhat's your visa status and current locationExperience:Data Warehous/Azure Synapse Analytics: 5 years (Required)SQL: 5 years (Required)Azure Data Factory & lake: 3 years (Required)MDM: 1 year (Required)Power BI/Similar tool: 1 year (Required)Python: 1 year (Required)SAP S/4HANA: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Tekcogno/jobs/Data-Engineer-6d8cfd538df0694c?fccid=360f48ee217c77a6&vjs=3,Remote,Data Engineer,Data Engineer
59,ASCENDING LLC,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.
Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.
Requirements:
Minimum 4 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.
Thanks for applying!
4jkNVq1nAC",https://www.indeed.com/rc/clk?jk=b6345c92a0c782ac&fccid=8012a5f0637a3b44&vjs=3,"Remote in Rockville, MD 20850 20850",Data Engineer,Data Engineer
60,Akshaya,"Position : Data Engineer Client : AMAT/TechMLocation: Santa Clara, CADuration: Long Term ContractRequirements:a. Data Engineer with Backend engineering roleb. should possess strong Python skillsc. should have experience in creating restful API’s ( having experience with FastAPI framework is major advantage)d. should have experience in dealing with Databases (experience with PostGre/ Clickhouse)Job Types: Full-time, ContractSalary: $70.00 - $80.00 per hourBenefits:Health insuranceSchedule:8 hour shiftExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One locationSpeak with the employer+91 +15109399872",https://www.indeed.com/company/AKSHAYA/jobs/Senior-Data-Engineer-69d86d855e088ba0?fccid=384b383b35930c68&vjs=3,"Santa Clara, CA",Senior Data Engineer,Data Engineer
61,"JADE BUSINESS SERVICES, LLC","Manager/Lead – Date EngineeringPermanent / Must move to Houston. Remote until Covid.QualificationsDescription: Lead the Data & Analytics team and make an impact at the highest levelLead team and solutions developmentRequired: 7+ years experience with relevant technology experience to include, enterprise-scale technical experience with cloud and hybrid infrastructures for data and analytics, architecture designs, migrations, and technology managementHands-on experience with SQL, Spark and Python is a must4+ years of relevant technology architecture or industry experience with ability to contribute to an end-to-end architecture and solution overview using native Amazon Web Services (AWS) tools2+ years experience as a Manager/Team lead - leading, managing and delivering complex technology projects with resources in multiple locationsBachelor's Degree or equivalent professional experienceImmigration sponsorship may be availablePreferred: Experience with Databricks, cloud data warehouse highly preferredExperience with Amazon Web Services (AWS) to include:Amazon Kinesis - Data Analytics, Data Firehose, Video StreamsAmazon S3Amazon EMRAmazon RedshiftAmazon DynamoDBAmazon GlueHands-on experience in SQL with RedshiftProgramming experience with Unix/Shell scriptingExperience with Amazon Lambda, Amazon AthenaCloud infrastructure management experience with Data & Analytics tools and servicesAWS CertificationsExperience with other Azure Cloud platformsExperience with unstructured dataHands-on experience with Orchestration tools like Airflow or Step functionAbility to orchestrate, lead, and influence virtual teams, ensuring successful implementation of customer projectsPresentation skills with a high degree of comfort with both large and small audiencesAn advanced degree in the area of specialization is preferredEnergy / Utility industry experienceJob Types: Full-time, ContractPay: $55.00 - $85.00 per hourSchedule:8 hour shiftExperience:Lead a Data & Analytics: 3 years (Preferred)SQL, Spark and Python: 4 years (Preferred)Amazon Web Services (AWS): 3 years (Required)data and analytics, architecture designs, migrations,: 5 years (Required)Willingness to travel:25% (Preferred)Work Location: Remote","https://www.indeed.com/company/jade-business-services,-inc/jobs/Data-Engineer-9c6798a1089948a1?fccid=11aad70f2eb663a7&vjs=3",Remote,Data Engineer,Data Engineer
62,Integration Developer Network LLC,"Data / Python Engineer24+ Months@ Houston, TXSkills:experience with APIs and has a good understanding of microservices architectureexperience with CI/CD and DevOpSexperience with containerization (Docker and Kubernetes)experience with Python using data processing packages like Pandas, NumPyexperience with databases, both relational and non-relational Specific needs for the project:worked on streaming architectures using Kafka/RabbitMQ and moving large amounts of data.worked directly with business teams to understand needs and engineer solutions.Job Types: Full-time, ContractPay: $60.00 - $65.00 per hourSchedule:8 hour shiftCOVID-19 considerations:yesExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/Integration-Developer-Network-LLC/jobs/Data-a7933fa9ea7c6ff8?fccid=eba674cbf8de5489&vjs=3,Remote,Data / Python Engineer,Data Engineer
63,Star Technology Solutions Inc,"__**ONLY W2Looking for a Sr. Data Engineer (focused within AWS Cloud Environments, with tech skill sets of Java, Lambda Web Services, DynamoDB, MySQL with RDS, and AWS cloud services) Top requirements would be having experience with Java Development and supporting Lambda / DynamoDB requirementsJob Types: Full-time, Part-time, ContractPay: $45.00 - $55.00 per hourSchedule:8 hour shiftWork Location: Remote",https://www.indeed.com/company/Star-Technology-solutions-Inc/jobs/Senior-Data-Engineer-ab8cab8ed3c9e905?fccid=ea1bfba0046c4925&vjs=3,Remote,Sr. Data Engineer,Data Engineer
64,TTS Solutions INC,"Client: ------Role: Data EngineerDuration: 12+Months (Contract W2)Location: Remote to startJob Description: Bachelor’s Degree in Computer Science, Engineering, Math or related field is required Master's degree is preferred3+ years of experience, specializing in BI solution development3+ years of SQL experience (No-SQL experience is a plus)2+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience designing, building, and maintaining data processing systemsExperience in interpretation of business needs from requests, and rapidly implement effective technical solutions3+ years of work experience in data management disciplines including data integration, data modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasksStrong experience with popular database programming languages including SQL, PL/SQL and others for relational databasesExperience sourcing data via a variety of sources like REST web services, MS SQL ServerExperience using software version control tools (Git, Azure DevOps, Subversion, etc.)Advanced level of SQL and query performance tuning techniques for Data Integration and ConsumptionSolid understanding of BI and analytics landscape, preferable in large-scale development environmentsExperience with or knowledge of Agile Software Development methodologiesBackground in Cloud Data Warehousing principles and Data Modeling preferredStrong aptitude for learning new technologies and analytics techniquesCommunication skills and ability to develop and present solutions to all levels of managementMust be able to interact effectively and patiently with customers especially while under pressureThe ability to work on multiple projects/tasks simultaneously to meet project deadlines for self and others as requiredAbility to establish and maintain positive working relationships with other employees.Strong analytical and problem-solving skills, with strict attention to accuracy and detail, and the ability to evolve data into knowledgePassion for delivering highly available, robust BI solutionsKnowledge of best practices and IT operations in an always-up, always-available serviceJob Types: Full-time, ContractPay: $70,000.00 - $85,000.00 per yearSchedule:8 hour shiftExperience:Informatica: 1 year (Required)SQL: 1 year (Required)Data warehouse: 1 year (Required)Work Location: Remote",https://www.indeed.com/company/TTS-Solutions-Inc/jobs/Data-Engineer-42b41911807f2b1f?fccid=012a0d5734a3318c&vjs=3,Remote,Data Engineer,Data Engineer
65,League Inc.,"Help Us Shape the Future of Healthcare

At League, we're big on building connections - both through our product and with each other. Our platform is consumer centric, personalized and always on. It's a front door to healthcare that empowers people to live healthier, happier lives. Every day.

When you feel connected to our mission, values and the work you do, you're driven to perform your best every day. When you feel connected to people you work with, you're able to build meaningful relationships that last. Together, we share the ultimate goal of delivering better health outcomes for everyone.

The world has changed, and so has the way we work. We believe you can work where you work best (whether it's in the office all the time, a few days a week, or none of the time), and still feel connected to our mission, values, purpose, and each other. League is headquartered in Toronto with a second office in Chicago and we believe in empowering Leaguers to work their way—anywhere that works for them in Canada and the US.

All Leaguers have League life moments – moments in our careers that connect us to League's mission and values, to each other and to our amazing work – every day.

Will your first League life moment be meeting our team? We can't wait to meet you.

As a Data Engineer, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross product and feature teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software engineers, data analysts, BI engineers and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

In this role you will:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using cloud-native and big data principles.
Build the framework to enable analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Marketing teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and regions.
Create data processes and tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Nice to Have:

Advanced working knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.
Experience building and optimizing big data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with message queuing, stream processing, and highly scalable big data stores.
Experience with end-to-end API design, implementation and sustainment.
Strong analytic skills related to working with unstructured and structured datasets.
Project management and organizational skills to self-scope and direct on a day-to-day basis.
Experience supporting and working with cross-product/feature teams in a dynamic environment.
The ideal candidate should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, Beam, etc.
Experience with both relational SQL and NoSQL databases, including Postgres, MongoDB, etc.
Experience with data pipeline and workflow management tools such as Apache Airflow, etc.
Strong Experience with Code Management + DevOps: Github, Github Actions, Concourse CI, Terraform, Atlantis, etc.
Experience with Google Cloud Platform (GCP) cloud services: BigQuery, DataFlow, PubSub, CloudSQL, Cloud Storage, Cloud Composer, VertexAI
Knowledge of the FHIR standard and/or healthcare data
Experience with object-oriented/object function scripting languages: Python, Go, Scala, Java, C++, etc.
At League, everyone is welcome. Diversity makes us better, and we believe in building diversity across cultural identity, gender, sexual orientation, age and education, thought and experience.

We are an equal opportunity employer, and we are committed to working with applicants requesting accommodation at any stage of the hiring process.

Here are some additional resources to learn more about League:
Learn more about us in this short video!
League, Cleveland Clinic collaborate to make employees healthier across North America
League and Loblaw bring next-generation digital health platform to customers
League Completes Workday Approved Integration

Given the current public health guidance on COVID-19 and to help support office health and safety, at this time, it is our company policy to require all Leaguers to be fully vaccinated to attend our offices in person.",https://www.indeed.com/rc/clk?jk=71dd5da3c66dbc70&fccid=533b130aca0b9cfa&vjs=3,"Remote in Chicago, IL+6 locations",Data Engineer (Remote),Data Engineer
66,Samiti Technology Inc.,"Location:  LATAM Preferred, open to other regions. Must work CST hours.Soft Skills: *Fluent in Spanish and EnglishGood time management skillsExperience in a customer facing role and interacting with clientsTechnical Skills: *Strong Big Query and related GCP experience, certifications are a plusCloud Data EngineeringJava, Hadoop and Spark experience are a plusStrong SQL Optimization skills for Big QuerySoftware Engineering (Python)Modern Data Warehousing DesignData Analytics & Visualization (Tableau, PowerBI, Looker, etc)Job Types: Full-time, ContractPay: $100.00 - $120.00 per hourSchedule:8 hour shiftMonday to FridayEducation:Master's (Required)Experience:Tableau, PowerBI, Looker BigQuery: 4 years (Required)Java, Hadoop and Spark: 8 years (Required)Fluent in Spanish: 4 years (Required)BigQuery: 6 years (Required)Google Cloud Platform: 6 years (Required)GCP, Big Query, Big Data: 6 years (Required)Work Location: Remote",https://www.indeed.com/company/Samiti-Technology-Inc./jobs/Data-Engineer-6c18d50624ac5b8d?fccid=8b4524db87e91c5f&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
67,nThrive,"Summary: The Software Engineer- Data will be responsible for the design and development of new features and tools to support our data insights process. This role also supports and maintains existing products for the growth and development of our Data Insights technology suite currently hosted in Azure.Essential Duties & Responsibilities: Responsible for developing new features and the maintenance and enhancements of existing functionalityParticipate in design, development, and support of highly scalable Data PipelinesResponsible for working within and maintaining an extensible and performance system architectureMaintain a broad knowledge of emergent trends in data warehouse platforms, tools, methodologies, and their underlying principlesCode review, unit test coverage and continuous improvementBuild tools to support automation and productivityCommunicate effectively with team members and project leadership to identify needs and evaluate alternative business solutionsEnsure unit tests written for all new codeSeek opportunities to incorporate new technologies into the product’s technology stack when they can add valueUnderstand and work with multiple data sources to meet business rules and support analytical needsCreate stored procedures and using other methods within SQL Data Warehouse and Azure Data Factory to import/translate/manipulate dataAnalyze potential data quality issues to determine the root cause and creating effective solutionsOptimize processes involving large data sets to improve performanceTroubleshoot and resolve functional and performance related issuesRequirements: High School diploma or GED3+ years of experience in Software Development; Developing Software in an Agile Environment3+ years of experience working directly with data warehouses3+ years’ experience on Microsoft BI Stack (SSIS and SSRS)3+ years’ experience designing, developing, testing, and supporting of ETLDemonstrated ability to extrapolate database schema to meet the business needs of the applicationTest Driven Development (TDD)Excellent oral and written communications skills are requiredSQL Server database design, development & optimization with knowledge of SQL server best practicesExcellent problem solving and analytical skillsExcellent verbal and written communication skillsTravel and work setting: This is a fully remote role with no travel and no requirement of working from an office or client sitePreferred Skills: Healthcare experienceWorking experience in Microsoft Azure Data Warehouse technologies such as Azure Data Factory, SQL Data warehouse, and DatabricksC#, ASP.Net and .Net Framework 4 and upContinuous IntegrationJob Type: Full-timePay: $110,000.00 - $120,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programEmployee discountFlexible scheduleHealth insuranceLife insurancePaid time offParental leaveProfessional development assistanceReferral programRelocation assistanceRetirement planTuition reimbursementVision insuranceSchedule:Monday to FridayExperience:C#: 1 year (Preferred)Software deployment: 3 years (Preferred)Data warehouse: 3 years (Preferred)Work Location: Remote",https://www.indeed.com/company/nThrive/jobs/Data-Engineer-25db9eeb0d34a2d0?fccid=6adfe00d346f10b0&vjs=3,+1 locationRemote,Data engineer (SWE skills),Data Engineer
68,White Cap,"A position at White Cap isn’t your ordinary job. You’ll work in an exciting and diverse environment, meet interesting people, and have a variety of career opportunities.
The White Cap family is committed to Building Trust on Every Job. We do this by being deeply knowledgeable, fully capable, and always dependable, and our associates are the driving force behind this commitment.
Job Summary
Responsible for collaborating with technology and infrastructure teams to identify data relationships and functional requirements in support of strategic goals for data management. Analyzes and resolves issues related to information flow and content. Provides expertise around business intelligence, data modeling, infrastructure enhancement, and architecture for data generation.
Major Tasks, Responsibilities, and Key Accountabilities
Designs and implements data solutions and models to store and retrieve company data. Assesses implementation procedures to ensure compliance with internal and external regulations. Defines, designs, and oversees the systems and applications architecture for both current and future business applications.
Partners with information technology to ensure that applications meet business requirements, are documented, and have processes in place for ongoing support and maintenance.
Examines and identifies structural data necessities by evaluating operations, applications, and programming of data systems. Installs and organizes information systems.
Evaluates new and emerging technologies and development methodologies to find faster, cost effective, and reliable solutions for existing and proposed systems. Recommends solutions to improve new and existing data systems. Oversees the migration of data from legacy systems to new solutions.
Monitors system performance by testing, troubleshooting, and integrating new features. Maintains data security and creates standards to ensure system safety.
Offers data support by responding to system problems. Educates staff members through training and individual support.
Prepares data design and architecture reports.
Nature and Scope
Identifies key barriers/core problems and applies problem solving skills in order to deal creatively with complex situations. Troubleshoots and resolves complex problems. Makes decisions under conditions of uncertainty, sometimes with incomplete information, that produce effective end results.
Independently performs assignments with instruction limited to the expected results. Determines and develops an approach to solutions. Receives technical guidance only on unusual or complex problems or issues.
May oversee the completion of projects and assignments, including planning, assigning, monitoring and reviewing progress and accuracy of work, evaluating results, etc. Contributes to employees' professional development but does not have hiring or firing authority.
Work Environment
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.
Typically requires overnight travel less than 10% of the time.
Education and Experience
Typically requires BS/BA in a related discipline. Generally 5-8 years of experience in a related field OR MS/MA and generally 3-5 years of experience in a related field. Certification is required in some areas.
Preferred Qualifications
If you’re looking to play a role in building America, consider one of our open opportunities. We can’t wait to meet you.
Functional Area Marketing and Communications
Recruiter Haynes, Steven
Req ID WCJR-003990",https://www.indeed.com/rc/clk?jk=7c3813db807afd51&fccid=cdf9e51085e7cd02&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
69,Ergoss Logiciels,"Founded in 2009, Ergoss Software develops and provides a SAAS in the field of analysis and monitoring of aircraft flight data. Our customers are mainly airlines.Following the creation of a subsidiary in the USA (Ergoss USA Corp.), we need motivated people to join our dynamic and friendly teams based in Miami.We are looking for a Flight Data Analyst Engineer.Missions :Decode flight data recorders logical frames layoutDevelop encoded logics to detect deviations from standard and airlines proceduresRespond to specific requests of airlines Flight Safety OfficersInsure support of level 1 & 2Produce training documentations & supportsConduct Training sessions for customers (at customers premises) and internal needs.Innovate on flight safety indicators & follow up methodologiesRessources / skills :Engineering degree in aeronauticsDeep knowledge on aircraft operations and performances.Familiar with programming methodes and Ability to use different type of programming languages.Job Types: Full-time, ContractAbility to commute/relocate:Miami, FL 33132: Reliably commute or planning to relocate before starting work (Required)Experience:SQL: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Ergoss-Logiciels/jobs/Flight-Data-Analyst-Engineer-24942d027b48228d?fccid=56a9635a3f1f2727&vjs=3,"Miami, FL 33132 33132 (Downtown area)",Flight Data Analyst Engineer,Data Engineer
70,Equinix,"Equinix is the world’s digital infrastructure company, operating 200+ data centers across the globe and providing interconnections to all the key clouds and networks. Businesses need one place to simplify and bring together fragmented, complex infrastructure that spans private and public cloud environments. Our global platform allows customers to place infrastructure wherever they need it and connect it to everything they need to succeed.
We are a fast-growing global company with 70+ consecutive quarters of growth. Through our innovative portfolio of high-performance products and services, we have created the largest, most active global ecosystem of nearly 10,000 companies, including 1,800+ networks and 2,900+ cloud and IT service providers in over 26 countries spanning five continents.
Joining our operations team means that you will be at the forefront of all we do, maintaining critical facilities infrastructure as part of a close-knit team delivering best in class service to our data center customers. We embrace diversity in thought and contribution and are committed to providing an equitable work environment that is foundational to our core values as a company and is vital to our success.
Job Summary: In this role, you will complete repairs, corrective maintenance, and routine installations.
Do you have a background in Electrical/Electrician, HVAC and skilled Mechanical trades? This could be your next career move.
Responsibilities

You will perform site inspections and monitor the building and IBX alarms

Performs preventative maintenance on site infrastructure (e.g. maintenance of primary infrastructures), or leads vendors

You undertake repairs and corrective maintenance

Comprehensive knowledge of critical infrastructure i.e. UPS, generator, BMS, chillers, life safety systems

Completion of site logs and data gathering issuing for basic permits, such as MOPs and scripts

You respond to all on-site incidents and acts as required

Completes routine work requests and circuit installations

You provide assistance during critical maintenance activities

You are able to effectively collaborate within the department and provide recommendations to peers for general maintenance activities

You carry out basic infrastructure projects

You are capable of lifting up to 50 lbs. and are agile in manual dexterity (climb, stoop, et.) to rack and stack equipment, as an example, with or without an accommodation.

24/7 Operation - Your flexibility to work any assigned shift, off-schedule, fill in for work mate, respond to emergencies, etc.

Equinix is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, or status as a qualified individual with disability.
At Equinix, we’re tasked with leading in the communities we serve — and doing our part to help keep our communities and our teams safe. Our #IamSafeIBelongIMatter culture transcends and informs all we do. In alignment with our culture, as well as our status as a U.S. federal contractor, as of December 8, 2021, Equinix will be mandating all US-based employees to be fully vaccinated for COVID-19. If you are unable to get a vaccine due to a medical condition, or sincerely held religious belief, Equinix will consider exemption requests as an accommodation.
Visit our Career page here to understand about Equinix COVID-19 Vaccine Policy.",https://www.indeed.com/rc/clk?jk=a229bdcdb4da539c&fccid=e15d9e27876d9dc3&vjs=3,"Atlanta, GA+3 locations",Data Center Facility Engineer,Data Engineer
71,Enodo Networks,"Join a team of highly motivated and talented Engineers focusing on the design and deployment of Cisco Data Center product lines. In this role, you will be focused on consulting with clients on their Cisco ACI goals while working with them to deliver the solution. Knowledge of other Cisco solutions is a plus as our bench supports every Cisco technology.Please do not apply if you do not have ACI implementation experience.Our team provides pre and post sales resources to partners, customers throughout the U.S. with the majority of work being remote.This is contractor position with competitive pay and flexibility of compensation choices based upon your desire for upside.''Contract Length:More than 1 yearContract Renewal:LikelyFull Time Opportunity:YesCOVID-19 Precaution(s):Remote interview processVirtual meetingsSanitizing, disinfecting, or cleaning procedures in placeThis Job Is:A job for which military experienced candidates are encouraged to applyOpen to applicants who do not have a high school diploma/GEDA job for which all ages, including older job seekers, are encouraged to applyOpen to applicants who do not have a college diplomaVisa Sponsorship Potentially Available:Yes: H-1B work authorizationYes: Immigrant visa sponsorship (e.g., green card sponsorship)Work RemotelyTemporarily due to COVID-19Job Types: Full-time, ContractPay: $150,000.00 - $250,000.00 per yearSchedule:Monday to FridayNight shiftWeekend availabilitySupplemental Pay:Commission payEducation:High school or equivalent (Preferred)Experience:Computer Networking: 5 years (Preferred)Data center experience: 5 years (Preferred)ACI: 2 years (Required)License/Certification:CCNP (Preferred)CCNA (Preferred)Work Location: One location",https://www.indeed.com/company/Enodo-Networks/jobs/Data-Center-Engineer-95f44dfebafe5523?fccid=a3648a6616e9c4ca&vjs=3,"Temporarily Remote in Lanham, MD 20706 20706",Data Center Engineer - Cisco ACI,Data Engineer
72,Infoscape Software Solutions,"Data EngineerReports to Manager, Data EngineeringVisa type: GC,CitizenSUMMARY:We are looking for an experienced Data Engineer to join our team. You will use various methods to transform raw data into useful data systems. You’ll strive for efficiency by aligning data systems with business goals. To succeed in this position, the Data Engineer must have strong analytical skills and the ability to combine data from different sources. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you!Specific duties include but are not limited to the following essential job functions:ESSENTIAL JOB FUNCTIONS:To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. This position will work closely with all Bright MLS business groups and customers. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.· Identify and resolve data quality issues in data sets· Assemble large, complex data sets that meet functional / non-functional business requirements· Identify, design and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability· Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies· Create new ETLs in AWS Glue with Python or Node.Js as the scripting language· Create AWS Lambdas using Python or Node.Js as the scripting language· Modify existing ETLs to fix issues where approach is appropriate· Use Glue for ETLs inside of AWS to and from all AWS types of data sources· Support the migration of data into S3, Redshift, DynamoDB, AWS RDS· Advanced knowledge for Microsoft SQL Server for future migration to an AWS Database Platform· Use Spark via AWS EMR for Big Data scale data preprocessing and support Data Scientists with Feature Engineering· Use Terraform for their AWS componentsEQUIRED SKILLS/EDUCATION/EXPERIENCE:· 5+ years of experience in a Data Engineer role· 5+ years’ ETL experience· AWS Glue ETL experience· Experience using: Python, Spark, AWS S3, AWS Lambda, Microsoft SQL Server· Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases· Experience building and optimizing ‘big data’ pipelines, architectures and data sets· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement· A successful history of manipulating, processing and extracting value from large disconnected datasets· Preferred Experience:· Previous experience with data quality projects and public records· Previous experience with: AWS DynamoDB, AWS Elastic Map Reduce, AWS Lambda, AWS Step Functions, AWS Redshift, AWS RDS, Terraform or CloudFormation· AWS Certification is a plus· Document Storage in S3 buckets is a plusJob Types: Full-time, Part-time, ContractSalary: $90,000.00 - $100,000.00 per yearSchedule:8 hour shiftAbility to commute/relocate:Tignall, GA 30668: Reliably commute or planning to relocate before starting work (Required)Experience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Infoscape-Software-Solutions/jobs/Data-Engineer-e701e7740521706c?fccid=efbb8662cd2b7f24&vjs=3,"Tignall, GA 30668 30668",Data Engineer,Data Engineer
73,Sublime Wireless Inc,"Your Responsibilities Will Include: · Building Data Lake using AWS and hands-on experience in S3, EKS, ECS, AWS Glue, AWS KMS, AWS Firehose, EMR.· Developing sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies.· Operationalizing data pipelines to support advanced analytics and decision making.· Building data APIs and data delivery services to support critical operational and analytical applications.· Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake.Qualification, Knowledge, Skills & Experience: BS degree in Computer Science, Data Engineering or similar Intermediate experience in an Apps Development role.Job Types: Full-time, ContractSalary: $80.00 - $90.00 per hourExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data Engineering: 7 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Sublime-Wireless-Inc/jobs/Senior-Data-Engineer-9cb90629b57d830d?fccid=5c7a0a3cee7e1d04&vjs=3,Remote,Sr. Data Engineer,Data Engineer
74,High5,"Job Title: Data Engineer
Location: Bentonville, AR
Duration: Long term
Pro-active identification of business problems, constraints, dependencies that leads to tangible benefits to the stakeholders
Stakeholder Engagement
Champion written and verbal communication in and out of the client, specific to the engagement
Negotiation of various options Vs value delivered with stakeholders
Requirements EngineeringDevelop and drive adoption of tools, templates, and/or processes for aligning requirements and design.mastery of several business analysis tools to elicit, document and manage requirements
Solution Evaluation
Use a combination of metrics plus and strategic analysis to pick and recommend appropriate solution to the business problem
Process
Establish processes as required by the engagement and ensure adherence by the team
BA artefacts maintenance
Documentation of the process plus project artefacts to the best standards
Management and communication about project artefacts to the various stakeholder groups
Develop and proactively drive adoption of tools, templates, and processes to help others produce effective decision making and approval processes
Team ManagementConduct necessary trainings and workshops within the project team and in the organization to grow and develop the BA communityMonitor Junior Bas and suggest/assist with identification of areas of improvement with ways of working, new learning, market needs
New Business
Assist the organization/Unit with identification of new business opportunities and solution recommendation proactively
Skill Examples:
1. Analytical, organizational and problem-solving skills - Proficiently use Analytical thinking and problem solving skills to analyse problems and opportunities effectively, identify which changes may deliver the most value, and work with stakeholders to understand the impact of those changes. Rapidly assimilate various types of information. Quickly choose effective and adaptable methods to learn and analyse the media, audiences, problem types, and environments. Apply right competencies like Creative Thinking, Decision Making, Learning, Problem Solving, Systems Thinking, Conceptual Thinking and Visual Thinking
2. Behavioural Characteristics – Exhibit behavioural characteristics to gain the trust and respect of stakeholders. Continuously and consistently exhibit, practice and preach competencies like Ethics, Personal Accountability, Trustworthiness, Organization and Time Management, and Adaptability.
3. Business Knowledge – Have a very good knowledge to perform effectively within the Business, Industry, and Organization, solution or methodology that the business analyst operates in. Demonstrate a good ability to recognize potential limitations and opportunities. Understand the risks involved in the area of work and make suitable decisions to manage risks. Demonstrate very good understanding of Current trends, market forces, market drivers, Key processes, Services, Products, definitions, customer segments, suppliers, practices and regulations
4. Communication Skills – adaption of communication styles and techniques to the knowledge level and communication styles of recipients. Well versed ability to speak the language of the stakeholders Proficiency in the use of variety of communication methods – verbal, non-verbal, physical and written along with exceptional listening skills. Assist conversations to reach productive conclusions
5. Interaction skills - ability to relate, cooperate, and communicate with different kinds of people including executives, sponsors, colleagues, team members, developers, vendors, learning and development professionals, end users, customers, and subject matter experts(SMEs). Facilitate stakeholder communication, provide leadership, encourage comprehension of solution value, and promote stakeholder support of the proposed changes. Exceptional negotiation and conflict resolution
6. Tools and Technology – Good working knowledge of various software application and tools to support communication and collaboration. Create and maintain requirements artifacts, model concepts, track issues, and increase overall productivity. Excellent knowledge of prototyping and simulation tools, as well as specialized tools for modelling and diagramming. Requirements management technologies required to support requirements workflow, approval, baselining, traceability, change control and management. Well versed in the use of required tools and technology including presentation software for communication and collaboration among the team and stakeholders.
7. Business Analysis techniques – Advanced knowledge of various BA techniques and the expertise to pick and use the right technique for carrying out the BA tasks as appropriate to the area of work. Listing down all of the techniques commonly used by Business Analysts to execute their tasks.",https://www.indeed.com/rc/clk?jk=af13ff4dfa4fcbca&fccid=95d5df8c72881502&vjs=3,"Bentonville, AR+2 locations",Data Engineer,Data Engineer
75,Mackin Talent,"Mackin Talent is seeking a Data Engineer to support our client in Washington, DC. The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within the organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.Job Responsibilities:Design, construct, install, test and maintain highly scalable data management systems.Ensure systems meet business requirements and industry practices.Design, implement, automate and maintain large scale enterprise data ETL processes.Build high-performance algorithms, prototypes, predictive models and proof of concepts.Skills:Ability to work as part of a team, as well as work independently or with minimal direction.Excellent written, presentation, and verbal communication skills.Collaborate with data architects, modelers and IT team members on project goals.Strong PC skills including knowledge of Microsoft SharePoint.Education/Experience:Bachelor's degree in a technical field such as computer science, computer engineering or related field required.Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.Mackin Talent offers an attractive benefits package which includes choice of major medical carriers like Aetna, Kaiser and BCBS, 15 days of PTO plus Holiday and Sick pay, paid volunteer hours, paternity/maternity leave and many more. We pride ourselves on our company values, the top one being that Relationships Matter. Come experience the Mackin Difference and our welcoming company culture with a focus on teamwork and family. Learn more about Mackin and apply online at MackinTalent.com.Job Types: Full-time, ContractPay: $75.00 per hourBenefits:401(k)Dental insuranceHealth insurancePaid time offReferral programVision insuranceSchedule:8 hour shiftMonday to FridayEducation:Bachelor's (Preferred)Experience:Lean Six Sigma: 2 years (Preferred)CBPP: 2 years (Preferred)ETL Process: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Mackin-Talent/jobs/Data-Engineer-8086a521a2133280?fccid=cf80b7640bd8266c&vjs=3,"Washington, DC",Data Engineer $75/hr,Data Engineer
76,Neal Analytics,"Databricks – Delta architecture and delta lakes experienceAzure data factory – hands-on exp with complex ADFCompare data factory with ETL toolsHow to load data from external tables from Databricks to SynapseHow to handle incremental loadsStar schema design experienceType 2 dimension - If reporting manager is changed for a salesperson, how you will attribute to the right manager’s salesIf the Power BI model is too big, how you will make it more efficientIf I have a table with 1000 strings and 1000 integers, which one will take more space, and whyHow to figure out the duplicates in my dimensionHow do you deploy from dev to test to prod?Process – how to approach if business comes to you with a problem. what will be the process, how you will be communicated, what will you share – everything from initial conversation to the endpoint when a business says yes that’s itYour estimate is 6 months and business says to bring it down to 3 months – how will you handle itJob Types: Full-time, ContractPay: $60.00 - $70.00 per hourBenefits:401(k)Dental insuranceHealth insurancePaid time offVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Data Engineering: 5 years (Required)Azure: 5 years (Required)Data Migration: 3 years (Required)Power BI: 4 years (Required)Work Location: Remote",https://www.indeed.com/company/Neal-Analytics/jobs/Azure-Data-Engineer-f12b0eac4d052539?fccid=a3bcc3a125bee804&vjs=3,+1 locationRemote,Azure Data Engineer,Data Engineer
77,Alldus,"This is a full-time, permanent role. Candidates seeking contract-to-contract employment will not be considered.
I'm currently partnering with a major news media company that has earned the trust of readers throughout the United states as one of the country's most reliable media sources due to its data-driven reporting. We are currently searching for an experienced Data Engineer. This is a fully-remote opportunity and is open to candidates currently residing and eligible to work in the United States.
Responsibilities:
Work with large data sets coming from a variety of internal and external sources
Clean, transform, and merge data for optimized analysis
Create reports and visualizations out of transformed data
Architect and engineer robust data pipelines to obtain, manipulate, organize, and load large data sets
Architect and deploy robust cloud data warehouses

Required Qualifications:
Experience working extensively with cloud environments, i.e.: BigQuery, GCP, AWS Redshift, Azure, or Snowflake
Experience with both ETL/ELT processes
Strong experience with either SQL and/or Python
A minimum of 2-4 years of professional experience with the above processes

Benefits:
Medical, vision, and dental insurance options
401k plan
Tuition reimbursement
Pretax savings plans including FSA, HAS, and Commuter Assistance
Employer-paid life insurance and long-term disability
Referral bonus
Gym and exercise class discounts
Adoption assistance
Competitive vacation and paid sick leave
Parent leave for both primary and secondary caregivers
Scholarships for children of employees",https://www.indeed.com/rc/clk?jk=64747af299c63054&fccid=b4df24bc350094d0&vjs=3,"Remote in Washington, DC+6 locations",Data Engineer (Fully Remote),Data Engineer
78,CrowdStrike,"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. Because of that we’ve earned numerous honors and top rankings for our technology, organization and talent. Our culture was purpose-built to be remote first, and we offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. If you’re ready to work on unrivaled technology with a team that makes a difference every day, let’s talk.
About the Role:
Develop software to integrate a variety of vendor security tools into Falcon XDR
Automate processes using Python and assist in infrastructure projects
Unify separate datasets to provide customers greater visibility of their security environments
Drive development of parsers and scalable data ingestion strategies
Develop UI enhancements for large-scale applications in CrowdStrike’s Falcon platform using Splunk and JavaScript
Configure a wide variety of security applications to assist development efforts
Constantly re-evaluate our product to improve architecture, user experience, performance, and stability.
Brainstorm and design collaboratively with members across multiple teams.
Be an energetic ""self-starter"" with the ability to take ownership and accountability for deliverables.
Respond to a changing threat landscape, rapidly prototyping tools which provide customers greater visibility into their environments
You'll use…
Humio Query Language and backend knowledge
Splunk/SPL dashboarding and backend knowledge
HTML/CSS/JavaScript Vue
Basic Windows and Linux administration
Python scripting
Regex parsing knowledge
AWS Services (S3, SQS, IAM, EC2, etc.)
Git
Atlassian SDLC tools
You will also have exposure to ….
Golang
Chef
VMWare
Kafka
Grafana
SaaS security products across various domains
Experience with SIEM solutions
What You'll Need:
Ability to work across various backend technology stacks to create secure data ingestion pipelines
Excellent communication skills, especially working with cross functional teams
Experience administering Humio and/or Splunk
Previous roles as a Security Analyst or SIEM Engineer
Strong knowledge of Splunk search processing language (SPL), reporting, dashboards, and search acceleration techniques
Experience scripting with the Splunk REST API and regular expressions.
Knowledge of data ingestion, field extraction, and post-ingestion processing
Python scripting for automation
Windows and Linux systems administration
Highly Desirable Skills:
Working knowledge of Humio
Experience with developing Splunk apps for clustered deployment
Golang
Previous roles using core AWS Services (S3, Lambda, EC2, IAM, etc.)
Background administering security products such as firewalls, email appliances, NDR tools, etc.
#LI-NT1
#LI-Remote
Benefits of Working at CrowdStrike:
Remote-friendly culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive health benefits + 401k plan
Paid Parental Leave, including adoption
Wellness programs
A variety of professional development and mentorship opportunities
Open offices have stocked kitchens, coffee, soda and treats
We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives that encourage new ways of solving problems, so we strive to attract and develop talent from all backgrounds and create workplaces where everyone feels seen, heard and empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. For applicants in Colorado the salary range is $118,906 - $178,360 + bonus + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, and certifications. The salary range may differ in other states.
CrowdStrike participates in the E-Verify program.
 Notice of E-Verify Participation
 Right to Work",https://www.indeed.com/rc/clk?jk=0177a6392ebaf748&fccid=64e4cdd7435d8c42&vjs=3,Remote in United States+4 locations,Data Processing Engineer (Remote),Data Engineer
79,Set Solutions,"Set Solutions is working exclusively with a client in need of a Data Privacy Lead in the Mansfield, TX area,We are looking for a Data Governance & Privacy Lead, who will work with the Security Director to build a Data Governance and Privacy program that will create, develop, implement and maintain all ongoing activities of data governance, policies, regulations, and standards/controls. This role will work cross-functionally in developing a privacy strategy that will provide compliance and keep up to date with Privacy, related to local, state and Federal laws that impact the organization.Position Duties: Create and deliver overall strategy of the data governance & privacy programPerform data discovery and data privacy business impact analysisProvide training to all levels of staff on data privacy, security, and compliance expectationsAct as SME on GDPR, CCPA, and other data privacy regulationsDevelop and maintain data security and data privacy policies/standardsWork with stakeholders across the organization to understand and address associated data privacy requirementsUnderstand and be able to implement data security, data protection, data usability, performance and integrity programsBuild a risk assessment that takes into account, personal data and advise on data protection impact of assessments and monitoring.Job Type: Full-timePay: $83,106.00 - $125,000.00 per yearBenefits:401(k)Dental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payWork Location: One location",https://www.indeed.com/company/Set-Solutions/jobs/Data-Privacy-Engineer-e6d1ee5da3290bdc?fccid=6a20a96bee65300c&vjs=3,"Mansfield, TX 76063 76063",Data Privacy Engineer,Data Engineer
80,Total Expert,"We are looking for a Data Engineer to join our growing Software Engineering organization. As a member of our growing Data Engineering team, you’ll use cutting edge AWS technologies and leverage your experience with data and data technologies to build out our fast-growing data and analytics platform that serves both our internal team members and our customers.

The team is comprised of Data and UI engineers working to bring our big data platform into the future. Working with both Data Bricks as a backend and Sisence for our embedded reporting and visualization tool. We are looking for people to help us provide more data to our internal and external customers. If you like ETL / Terraform / glue / AWS that’s our jam. If you have experience with either Data Bricks or Sisence, that would be great, but those are not critical.
We’re looking for someone who is passionate about data, databases, and all things in between and can bring their experience around best practices and effective technology approaches for moving, assembling, and creating value with data within a cloud-based SaaS platform.
You’ll contribute to helping architect our new back end in Data Bricks. You also help create models and analytics schemas for several different types of use – between custom reporting, data transfers to customers, and ML/AI work.
What You'll Do:
Develop batch and streaming data pipelines using modern data architecture and technologies such as Spark, AWS Glue, Amazon Kinesis, Lambda, and Databricks
Leverage CI/CD practices to deploy and maintain data pipelines
Collaborate with data analysts and product designers to gather and refine analytics requirements and translate requirements into technical designs
Create data models to support complex internal and customer facing analytics and reporting use cases and advise/educate data analysts and business users on the benefits of various models
Implement process improvements for better quality, performance, scalability, and automation
Create automated tests to validate and improve data quality and pipeline reliability
Support and maintain production data pipelines including troubleshooting and performance tuning of Spark, SQL, and Python modules
Assist data analysts with query writing and tuning
Participate in team-based knowledge sharing opportunities and contribute to the overall growth and collective knowledge of Total Expert’s Data Engineering team
What We Look For:
2+ years of experience developing large-scale data pipelines using cloud-based technology and distributed storage/processing platforms such as S3, HDFS, AWS Glue, Spark and Hadoop in a production environment
1+ years of experience with Amazon Web Services environments and technologies such as Glue, Athena, S3, Lambda, SNS, SQS, and Kinesis
Enthusiasm to work within a startup culture where every day can be a new adventure
Action-oriented, collaborative, and iterative mindset
Working knowledge of data warehouse dimensional modeling techniques/patterns and experience with data warehouse SaaS platforms such as Redshift and Snowflake
Intermediate level of Python experience
Intermediate to advanced level SQL knowledge and ability to write and decipher complex, multi-part queries
Team based Agile/Scrum development experience using tooling such as Jira, Git, etc.
Preferred Qualifications:
1+ years of experience building data pipelines using Databricks and Spark (PySpark preferred)
Experience with Sisence
Total Wellness:
We believe that living a balanced life leads to more creativity and productivity. Total Expert offers the following benefits to support employees in their pursuit of Total Wellness.
Physical Wellness:Medical, Dental & Vision CoveragePrescription Drug CoverageGymPassHealth Advocate ProgramFlexible Time Off Program
Financial Wellness:Health Savings Account, Flexible Spending Accounts & Disability ProtectionLife & Voluntary Life CoveragePaid Parental LeavePet Insurance401(k) Retirement Savings Plan & Matching ProgramEmployee Referral Bonus
About Us
Total Expert is the leading fintech software company that delivers purpose-built CRM and customer engagement for modern financial institutions. The Total Experience Platform unifies data, marketing, sales, and compliance solutions to provide a cohesive experience across the customer lifecycle.
At Total Expert, we strive for excellence, innovation, and customer success in everything we do. We are determined to reimagine the way people and technology work together so that we can allow our customers to build more meaningful, human connections with their customers.
Total Expert's Awards & Accolades:Named to the Inc. 500 list for the third year in a row (#288 in 2020), 3rd fastest-growing Minneapolis-based company, and in the Top 40 fastest-growing software companies in the nation on the 2020 Inc. 500 list.America's Best Startup Employers 2021 list by ForbesDeloitte Tech Fast 500Top Workplace by the Minneapolis-St. Paul Star Tribune (2018, 2019, 2020)HousingWire Tech100 (2020)Tech 20 - Twin Cities Business (2020)
Our most successful people have a “teamwork makes the dream work” mindset, operate with a strong sense of urgency, have an entrepreneurial spirit, and are solutions-oriented. Come join us!",https://www.indeed.com/rc/clk?jk=cddc310786ad951d&fccid=9406021a1a65a887&vjs=3,"Remote in Minneapolis, MN 55418 55418+1 location",Data Engineer (Remote),Data Engineer
81,Exponentlogic Solution,"Responsibilities for Data EngineerCreate and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Qualifications for Data EngineerAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with big data tools: Hadoop, PySpark, Kafka, HiveExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.Experience with AWS cloud services: EC2, EMR, RDS, RedshiftExperience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.Job Types: Full-time, Part-time, ContractPay: $55.00 - $60.00 per hourSchedule:Monday to FridayWork Location: One location",https://www.indeed.com/company/Exponentlogic-Solution/jobs/Data-Engineer-eebeb6534296d431?fccid=7574576124b47b82&vjs=3,"Charlotte, NC 28202 28202 (Downtown Charlotte area)",Data Engineer,Data Engineer
82,TTS Solutions INC,"Client: ------Role: Data EngineerDuration: 12+Months (Contract W2)Location: Remote to startJob Description: Bachelor’s Degree in Computer Science, Engineering, Math or related field is required Master's degree is preferred3+ years of experience, specializing in BI solution development3+ years of SQL experience (No-SQL experience is a plus)2+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience designing, building, and maintaining data processing systemsExperience in interpretation of business needs from requests, and rapidly implement effective technical solutions3+ years of work experience in data management disciplines including data integration, data modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasksStrong experience with popular database programming languages including SQL, PL/SQL and others for relational databasesExperience sourcing data via a variety of sources like REST web services, MS SQL ServerExperience using software version control tools (Git, Azure DevOps, Subversion, etc.)Advanced level of SQL and query performance tuning techniques for Data Integration and ConsumptionSolid understanding of BI and analytics landscape, preferable in large-scale development environmentsExperience with or knowledge of Agile Software Development methodologiesBackground in Cloud Data Warehousing principles and Data Modeling preferredStrong aptitude for learning new technologies and analytics techniquesCommunication skills and ability to develop and present solutions to all levels of managementMust be able to interact effectively and patiently with customers especially while under pressureThe ability to work on multiple projects/tasks simultaneously to meet project deadlines for self and others as requiredAbility to establish and maintain positive working relationships with other employees.Strong analytical and problem-solving skills, with strict attention to accuracy and detail, and the ability to evolve data into knowledgePassion for delivering highly available, robust BI solutionsKnowledge of best practices and IT operations in an always-up, always-available serviceJob Types: Full-time, ContractPay: $70,000.00 - $85,000.00 per yearSchedule:8 hour shiftExperience:Informatica: 1 year (Required)SQL: 1 year (Required)Data warehouse: 1 year (Required)Work Location: Remote",https://www.indeed.com/company/TTS-Solutions-Inc/jobs/Data-Engineer-42b41911807f2b1f?fccid=012a0d5734a3318c&vjs=3,Remote,Data Engineer,Data Engineer
83,"RE/MAX, LLC","RE/MAX, LLC is looking for a Data Engineer to join the Data Development team. This team member will work cross functionally with the Data Development team to build existing data products as well as define and create processes for new products and solutions. They will also work with clients, both internal and external, and stakeholders to support our products and solutions.

Responsibilities:
Gain an understanding of and participate in the building of current data products and solutions
Gain an understanding of and develop the ability to use our current visualization tools as well as define and create new ones
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc
Work with clients and stakeholders to assist with data-related technical issues and support their data needs
Skills & Experience:
5+ years experience building data products and solutions
Strong problem solving skills with an emphasis on data development
Experience in using SQL, Python and GIS software to deliver geographic datasets
Fluency with one or more common data management languages(e.g., Python, Alteryx, R, SSRS, etc.)
Traits:
Adhere to our group’s core values: be above reproach, deliver an exceptional customer experience, achieve more together, treat others with dignity, default to action, benefit from freedom and be responsible, continually improve
Strong generalist excited to work with a variety of tools
Strong problem-solving skills and resourcefulness
Ability to effectively communicate complex ideas in a clear and concise manner across functional and technical departments, both verbally and in writing
Ability to establish and maintain effective working relationships with team members and other invested stakeholders
Ability to analyze and report on third-party software systems and platforms for possible inclusion in the team's technology stack
Sincere passion for delivering exceptional customer experiences
Intrinsically self-motivated and self-directed
Demonstrate a consistent sense of urgency when resolving issues causing poor customer experiences, resolving those issues with empathy
Can prioritize work items, and effectively communicate those prioritization choices
Keen attention to detail
Maintains collaborative communication, poise, and professionalism, particularly in stressful situations
Thrives in a rapid-evolving, constantly evolving environment
Interested in expanding breadth of skills, as opposed to depth in a single skill
Drive innovation in our products and services while continually improving our processes

Hire Range/Rate:

$115,000 - $125,000

Actual compensation offered to candidate will be finalized at offer and may be above or below the posted range due to skill level, experience, industry specific knowledge, education/certifications, or geographic location. The offer rate represents one component of the RE/MAX Holdings total compensation package. Employees will also receive a number of benefits as listed below. Other compensation for this position may include bonus eligibility.

Benefits Offered

Competitive medical, dental and vision benefits
401(k) and Roth 401(k) retirement plans with optional company match
Health savings account with a company contribution
Flexible spending accounts (medical, dependent care and transportation)
Company-paid maternity, adoption, foster and parental leave
Educational assistance
Student Loan Support Services
Paid employee assistance program
At least 7 paid holidays, and potential for up to 15, including discretionary early closures before holidays and company events. (More than your average company!)
MORE Time Away Program gives employees flexibility around time off needs and lets employees take time off as they need it, rather than waiting for accruals
ClassPass discount and monthly subsidy
Free covered garage parking (car chargers and bike racks available)
And More!

Now is your chance to become part of a world-class, industry leading organization that touts the #1 real estate brand in the world! RE/MAX is a business that builds businesses. We, alongside booj, our award-winning technology company, specialize in providing the tools, training and tech to our real estate network, which includes RE/MAX and Motto Mortgage franchises, agents, brokers and consumers. Join us and build a career where your contribution is heard, your innovative ideas are valued, and hard work and collaboration truly makes a difference.

RE/MAX LLC, Motto Mortgage, booj, First.io, welmo and Gadberry Group are an equal opportunity employer committed to diversity and inclusion, as well as non-discrimination in employment. All qualified applicants receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, veteran status, disability unrelated to performing the essential task of the job or other legally protected categories. All persons shall be afforded equal employment opportunity.",https://www.indeed.com/rc/clk?jk=368e9130c9b12e31&fccid=040bdbd1708b7eae&vjs=3,Remote,Data Engineer,Data Engineer
84,ACV Auctions,"ACV’s mission is to build and enable the most trusted and efficient digital marketplaces for buying and selling used vehicles with transparency and comprehensive data that was previously unimaginable. We are powered by a combination of the world’s best people and the industry’s best technology. At ACV, we are driven by an entrepreneurial spirit and rewarded with a work environment that enables each teammate to impact the company from day one. ACV’s network of brands includes ACV Auctions, ACV Transportation, MAX Digital and ACV Capital within its Marketplace Products as well as True360 and Data Services.

What you will do:
ACV Auctions is looking for a Lead SecOps Engineer, Data Security and Privacy. The Data Security and Privacy SecOps Engineer is someone who is passionate about building and managing Security Infrastructure and Business Practices and enhancement that drive effective data risk management and reduction. In this role you will be responsible for creating a model of Security for the cloud resources that supports the ACV Platform. This includes the AWS and GCP along with nodes that host K8 clusters and other third party partners.

We are building a layered Security approach which means the SecOps Engineer will need to work hand in hand with teams such as Infrastructure, AppSec, Detection and Response, Development Teams and compliance to ensure the flow from Applications to APIs to Cloud Resources are secured. In lieu of layering Security controls the person in this role will be working to enhance and strengthen the Security Controls within our environment as a whole, such as: anti-phishing gateways, EDR, AV, firewalls, IDS/IPS systems, AWS Security Hub. Further this position is not only about growing ACV's capabilities but our associates as well, it will be important to be able to work with various teams such as Dev, HelpDesk, HR, Legal etc guiding Security recommendations for the program.

Responsibilities:

Formalize the Data Security and Privacy Program including: data mapping, data identification, data security standards and data security practices and processes.
Drive the technical practices and implementation of securing data across technical systems and infrastructure.
Develop, implement and manage security standards, plans/roadmaps and operational processes to secure the AWS platform and resources such as RDS, EC2, S3, etc.
Manage Security Alerts and provide Incident Response support services, it's not expected someone knows everything but this person should be able to identify and perform triage to resolve a Security Incident.
Able to deploy and manage infrastructure and applications via code, CICD pipeline and K8.
Contribute to the development, improvement and operational management of Security Operations, Monitoring and Incident Response practices, processes and solutions.
Able to work with vendors and manage PoC's.
Overall understanding of Security Domains, Compliance Requirements, and Risk Management Practices.

What you will need:

Excellent communication, interpersonal and leadership skills, with the ability to interact with staff at all levels.
Knowledge of CASB, DLP and SASE technologies
Proven ability to be agile and work effectively in a dynamic environment.
Demonstrated ability to perform under pressure and respond rapidly to emerging incidents and situations.
Excellent coordination, project management, and organization skills and comfortable with multi-tasking in a high-energy environment.
Should be a creative and analytical problem solver with a passion to provide excellent customer service.
Practical hands-on experience engineering and implementing data security controls in cloud environments including databases, datastores and SaaS platforms.
Linux and Kubernetes/Container management and security
DevOps code based implementation and management
Knowledge of AWS including but not limited to S3, Lambda, RDS, EC2 and AWS Security Center
Understanding of TCP/IP Networking including knowledge of Protocols and Services
Understanding of what Information or Assets are of value to Threat Actors and how Organizations are Breached and Customer Accounts Compromised.
Overall understanding of the Security domain, compliance, business, risk, ops etc ALONG with its application to the business.

What we offer:

Comprehensive benefits offerings for benefits eligible Teammates.
Unique culture that truly values each and every Teammate.
Career development and Future Growth Opportunities.

At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.",https://www.indeed.com/rc/clk?jk=f0b0246d9d85ba51&fccid=6867c53ddcaa8459&vjs=3,"Caribou, ME 04736 04736+21 locations","Senior Data Security Engineer, SecOps",Data Engineer
85,Orion Inc.,"Identifying installation solutions for new databasesDetermining the requirements for a new databasePublishing and/or presenting design reportsIdentifying areas for improvement in current systemsCoordinating with other team members to reach project milestones and deadlinesAuditing database regularly to maintain qualityCreating systems to keep data secureExperience with NoSQL databases such as CosmosDB, Postgres NoSQL, and Cassandra a big plusWorking knowledge of Python and Kafka a plus.Develop set processes for data mining, data modeling, and data production.Create custom software components and analytics applications.Job Type: ContractSchedule:8 hour shiftDay shiftAbility to commute/relocate:Berkeley Heights, NJ: Reliably commute or planning to relocate before starting work (Preferred)Experience:Data Engineer: 5 years (Preferred)ETL/ELT: 5 years (Preferred)Azure/AWS: 3 years (Preferred)Python: 2 years (Preferred)Work Location: One location",https://www.indeed.com/company/Marvel-Infotech-Inc/jobs/Data-Engineer-54e317f3d16e0d2d?fccid=4d646c7bde0d7dab&vjs=3,"Berkeley Heights, NJ",Data Engineer,Data Engineer
86,Ergoss Logiciels,"Founded in 2009, Ergoss Software develops and provides a SAAS in the field of analysis and monitoring of aircraft flight data. Our customers are mainly airlines.Following the creation of a subsidiary in the USA (Ergoss USA Corp.), we need motivated people to join our dynamic and friendly teams based in Miami.We are looking for a Flight Data Analyst Engineer.Missions :Decode flight data recorders logical frames layoutDevelop encoded logics to detect deviations from standard and airlines proceduresRespond to specific requests of airlines Flight Safety OfficersInsure support of level 1 & 2Produce training documentations & supportsConduct Training sessions for customers (at customers premises) and internal needs.Innovate on flight safety indicators & follow up methodologiesRessources / skills :Engineering degree in aeronauticsDeep knowledge on aircraft operations and performances.Familiar with programming methodes and Ability to use different type of programming languages.Job Types: Full-time, ContractAbility to commute/relocate:Miami, FL 33132: Reliably commute or planning to relocate before starting work (Required)Experience:SQL: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Ergoss-Logiciels/jobs/Flight-Data-Analyst-Engineer-24942d027b48228d?fccid=56a9635a3f1f2727&vjs=3,"Miami, FL 33132 33132 (Downtown area)",Flight Data Analyst Engineer,Data Engineer
87,"eClinical Solutions, LLC","At eClinical Solutions, our goal is to leverage technology to accelerate clinical research and bring life-changing therapies to patients faster. Our industry-leading elluminate® clinical data platform is used by top life sciences companies worldwide to regain control of their data and provide meaningful analytical insights.The eClinical Solutions Engineering team has true greenfield opportunities for software engineers who want to develop innovative revenue-generating products that are revolutionizing the Life Sciences industry. You will work on a team that is developing enterprise-level clinical data software. At eClinical Solutions, you will be recognized and rewarded for your ideas and accomplishments, all while contributing to the latest disruptive technology incorporating modern software architectures, data science, and advanced analytics.Accelerate your skills and career within a fast-growing company while impacting the future of healthcare.OVERVIEWThe Data Engineer will work closely with clients and provide technical consulting services,configuration of the elluminate platform, development for specific projects that include trialconfiguration, quality control, process improvements, system validation, custom analytics development,clinical software implementations and integrations. platform configuration, ETL and custom analyticsdevelopment.The Data Engineer will engage in technical development and implementation of various softwareservice delivery related activities.This position reports to a management position of the Professional Services Team.KEY TASKS & RESPONSIBILITIESDesign, develop, test, and deploy highly efficient SQL code and data mapping code according tospecificationsDevelop ETL code in support of analytic software applications and related analysis projectsWork with Analytics developers, other team members and clients to review the businessrequirements and translate them into database objects and visualizationsBuild any analytics reports and visualizations using tools like QlikProvide diagnostic support and fix defects as neededEnsure compliance with eClinical Solutions/industry quality standards, regulations, guidelines,and proceduresOther duties as assignedWe are proud to be an equal opportunity employer that values diversity. Our management team is committed to the principle that employment decisions are based on qualifications, merit, culture fit and business need.Job Type: Full-time","https://www.indeed.com/company/eClinical-Solutions,-LLC/jobs/Data-Engineer-11e08acd508e2972?fccid=387b89c11ffe51f9&vjs=3","Mansfield, MA 02048 02048",Data Engineer,Data Engineer
88,DispatchHealth,"Emergency room visits and hospital stays can be uncomfortable and stressful times for families and their loved ones. DispatchHealth allows people to receive high quality and cost-effective medical care in the comfort of their homes. A rapidly scaling startup, DispatchHealth provides an integrated, convenient, high-touch care-delivery solution that extends the capabilities of the patient's care team and ensures that we provide personalized, quality care in the home or at the patient's location of need.

Technology and data drive all aspects of our business and allow for better care delivery from patient onboarding, patient care, and post care follow-up. We're looking to create a best-in-class technology platform that helps our patients stay in the comfort of their homes – and we're looking for people like you to help shape and build the technology that changes the way people receive healthcare.


Role Summary

DispatchHealth has started its next phase of technology development and growth and a cloud architect is a pivotal role in its success. Enable software development teams deliver changes with ease. Create the infrastructure that enables teams to be independent, secure and focused on solutions rather than bottlenecks in the process.

As a data engineer on the data team, you will develop robust, scalable data pipelines that drive decision making across the organization. Using a combination of pre-built and custom ETL tools, you will develop the robust data infrastructure that helps us support both internal and external customers with timely, accurate and complex data needs. If you love building high-quality data infrastructure that supports the future of healthcare and improves people's lives, we want to work with you!


Responsibilities:

Help to lead major efforts around building a data ecosystem that is robust, fast and scalable, and allows DispatchHealth to realize its goal of measuring all aspects of our business using data
Develop modern data pipelines that support data integrity, monitoring and lineage
Build well-designed, reusable systems to extract, transform and load data into centralized data lakes and reporting warehouses from a variety of sources
Build systems that collect and maintain customers' sensitive data, while holding DispatchHealth to the highest standards for security and HIPAA compliance
Design fault tolerant systems so the team can be brave and move fast, without ever negatively affecting our customers
Participate in ongoing training to maintain current knowledge of ETL, orchestration and data infrastructure and perform duties as required by job function


Qualifications

3+ years of experience using Python and SQL for data engineering
1+ years of data engineering experience in a cloud environment (AWS or GCP)
1+ years of experience working with row-based and columnar databases
Experience using orchestration tools such as Airflow or Prefect
Experience with ETL tools such as Stitch, Fivetran, Airbyte or Segment
A proven track record of working collaboratively on complex, medium-to-large ETL projects that span multiple months
Strong software development skills using an agile framework
Interest in working with product and engineering teams, as well as cross functional partners

Location: Remote, USA

The pay range for this position in Colorado is $110,000 - $140,000/yr; however, base pay offered may vary depending on job-related knowledge, skills, candidate location, and experience.

DispatchHealth is committed to creating and supporting a diverse and inclusive team and serving all communities. All qualified applicants will be considered for employment regardless of race, gender, gender identity or expression, sexual orientation, religion, national origin, disability, age or veteran status.

DispatchHealth offers a comprehensive benefit package, including medical, dental and vision insurance, 401k, paid time off, family and short-term disability leave

To see our Employee Privacy Notice, please see here: https://www.dispatchhealth.com/employee-privacy-notice/",https://www.indeed.com/rc/clk?jk=444a74db4e55d64d&fccid=e878d032323d129e&vjs=3,"Remote in Denver, CO",Data Engineer,Data Engineer
89,Catalyte,"Senior Data Engineer - CatalyteAre you a Data Engineering enthusiast with a wide variety of experience across the data lifecycle and with multiple technology stacks? Do you love to organize data and figure out all of the ways it can be used? Do you enjoy mentoring junior and future engineers?Catalyte is looking for an innovative Senior Data Engineer to join our team and help build out our Data and Analytics platform. If you want to put your skills to use to help find and create future developers, come talk to us!Responsibilities: - Working with stakeholders to ensure that business data needs are met- Building data ingestion pipelines to ensure timely and quality data into our Data Lake- Contributing to the data platform design and architecture decisions- Ensuring that data is stored, queried and displayed securely- Working with and evaluating existing data-centric applications and tools- Working on an Agile team to build high quality and modular data structures- Mentoring junior developers to allow them to grow their data careerQualifications: - At least 5 years working in a Data Developer, Engineer or Architect role with a broad focus on data movement, querying (SQL), analytics, storage and processing- At least 3 years experience with Python- At least 3 years experience with Spark (PySpark)- Experience with Databricks a plus- Excellent skills in both RDBMS (SQL Server, PostgreSQL) and NoSQL technologies- Exposure to data security regulation and enforcement - GDPR, HIPAA, etc- Cloud experience is desired - AWS preferred- Experience building pipelines in a modular, scalable way- Strong analytical skills- Curiosity about data and the ideal way to structure it to extract meaning from it- A “can do” attitude and a passion for innovation- Experience with Tableau Desktop and Server a plusCatalyte provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.Job Type: Full-timePay: $130,000.00 - $150,000.00 per yearBenefits:401(k)Dental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:8 hour shiftSupplemental Pay:Bonus payExperience:Data Engineering: 5 years (Required)SQL: 5 years (Required)Python: 3 years (Required)Work Location: Remote",https://www.indeed.com/company/Catalyte/jobs/Senior-Data-Engineer-a760a36fd91a30b5?fccid=6d26f2d6e7999c76&vjs=3,Remote,Senior Data Engineer - Catalyte,Data Engineer
90,Edmunds.com,"At Edmunds we’re driven to make car buying easier. Ever since we began publishing printed car guides in the 60’s, the company has been in the business of trust, innovating ways to empower and support car shoppers. When Edmunds launched the car industry’s first Internet site in 1994, we established a leadership position online and have never looked back. Now, as one of the most trusted review sites on the Internet, millions of visitors use our research, shopping and buying tools every month to make an easy and informed decision on their next car. For consumers, we bring peace of mind. For dealers, we make tools to help them solve their problems and sell more cars. How do we do it, you ask? The key ingredients are our enthusiastic employees, progressive company culture and cutting-edge technology. Want to join the team? Read on to find out how!
What You’re Applying For:
You will be using Java and Javascript to build fast, resilient, highly available and scalable microservices, Rest APIs & GraphQL to provide Data as a Service (DaaS). Adopt the best database solution (SQL or NoSQL) fitting the use case. Cloud-native software deployment applying AWS managed solutions and DevOps skills. These services are used to build customer experiences for millions of people do research and to find the best car deals. Come join a dynamic team who are thinking about building the most scalable web solutions every day.
What You’ll Do:

Develop and support Rest APIs with high availability and performance SLAs complying to microservice principles.
Develop and support GraphQL data services as we scale GraphQL adoption.
Be the subject matter expert on various domain applications.
Build and support serverless platform that resizes millions of images in a few hours.
Engineer and deploy multiple systems and processes across a spectrum of services following Continuous Delivery and Cloud-First Strategy.
Develop modern data curation solutions to allowing Data Content Editors to quickly onboard new data sources and enhance existing data integrations.
Challenges:

Wide spectrum of applications and domain knowledge required to be effective in job function. Quick learner.
Ability to handle multiple customer requests, which requires good due diligence and prioritization skills.
Problem solving skills for quick troubleshooting and optimal solution design.
What You Need:

Proficiency in Java programming is desired with experience using Spring framework along with an understanding of when to apply different Object-Oriented Programming principles.
Understanding of microservice APIs and exposure to NoSQL databases.
Understanding of GraphQL development ecosystem.
Experience with AWS especially AWS Gateway, S3, and Lambdas is a plus.
Proficiency in Javascript is a plus.
A passion to never stop learning - our stack is constantly evolving
Edmunds Tech Blog
At Edmunds we’re not just about making car buying easier, we're also passionate about technology! Click here to read through our Tech Blog!
Edmunds Perks:

Flexible time off

13 Paid Holidays

Comprehensive Health Benefits (medical, dental, vision, life and disability)

Flexible Spending Accounts (Employees) and Health Savings Accounts (Employee and Employer Contributions)

401K Plan with Company Matching at 50%, up to 6% of employee eligible contribution and vesting after 1 year

Up to 4 months Paid Parental Leave

HeartCash matches employee donations to the causes that are important to them

2 Days of Paid Time Off for time to dedicate to social impact causes

FitCash covers a portion of gym or fitness activity fees

Well being sessions and events such as yoga, meditation and walking challenges

On-going career development sessions and an annual learning event

Pet insurance

Sabbatical leave

Education Reimbursement

Plus a coffee bar, frozen yogurt and more!
Working @ Edmunds.com:
Employees think it’s a pretty great place to work and some pretty impressive publications think it is too: we have been recognized as one of the best places to work by the Fortune Magazine and Great Places to Work, LA Business Journal (for the last 6 years!), Computerworld, Built in LA and Inc. Magazine. We've also been identified as one of the best workplaces specifically in Technology and also for Diversity and Asian Americans. If you’re interested in learning more and joining our mission, we’d love to hear from you!
Edmunds will consider for employment qualified candidates with criminal histories in a manner consistent with the requirements of all applicable laws.
#LI-POST
#LI-REMOTE
This is a remote position.",https://www.indeed.com/rc/clk?jk=c4c0a5809869fa50&fccid=f69fe5673875c177&vjs=3,"Remote in Santa Monica, CA 90404 90404+1 location",Software Engineer (Data as a Service),Data Engineer
91,Kloutix Solutions LLC,"5-10 years of hands-on experience in building data integrations in analytical environmentsVery strong experience in building data pipelines using SQL and PythonExcellent SQL skills and exposure to cloud technologies like AWSExpertise in pythonHas in-depth knowledge of DW architectural principlesExcellent communication and client interfacing skillsJob Types: Full-time, ContractJob Types: Full-time, ContractSalary: $65.00 - $80.00 per hourExperience:SQL: 5 years (Preferred)Python: 5 years (Preferred)Data warehouse: 5 years (Preferred)ETL: 5 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Kloutix-Solutions-LLC/jobs/Data-Engineer-4853b66a0568d512?fccid=52ce8d5099728b71&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
92,Shield Healthcare,"Shield HealthCare is looking for an IT Data Integration Engineer for our Valencia, CA corporate office. The primary function of this position is the design, implementation, integration, monitoring and error handling of all company enterprise applications/systems (on-premises and cloud).JOB RESPONSIBILITIES: Responsible for engineering and maintaining the data integration interfaces for data life cycle managementManages IT data technology methods and Integrations for all company systems (on-premises and cloud)Work closely with members of the IT team to ensure that all data workflows are consistently compliant with all the services (cloud, third party, database, analytics) and integrate with other company servicesResponsible for maintaining company data workflows for best data management practices and governanceProduce and maintain complex data workflows to meet all the quality requirements of enterprise applicationsDesign and document dataflow architectureBuilds database schema, tables, procedures and permissions as necessary with DBA teamCreates, tests and executes data management for testing systems and applicationsDevelops reporting from various data systemsAnalyzes and sustains data integration capacity and performance requirementsMonitors systems and platforms for availabilityAccountable for uptime and fluidity on integrationsEvaluates and recommends new integration/database technologiesEnsures and maintains end-to-end security and encryption throughout the transport of dataPrepares written materials for the purpose of documenting activities, written reference and/or conveying informationSolve issues and participate in defect and incident root cause analysisParticipate in an Agile development environment by attending standups and sprint planning activitiesFamiliar with integrations to/from MS Azure environment; ability to diagnose, alert and monitor dataIn conjunction with the DBAs, provide data management, storage and retrievalIn tandem with the DBA team all data integration troubleshooting to isolate and diagnose issuesRecommends and oversees upgrades to integration systems and their effect to production environmentsUnderstands and supports key applications for bi-directional data integrations to systems such as MS SQL backend systems, RightFax, Alchemy (ECM), Salesforce.com (CRM), Epicor (ERP), Clippership manifest system and others as assignedParticipation in weekly on-call after hours support rotationQUALIFICATIONS: 5 years of systems and data pathway management experience in an enterprise environment5 years of experience creating, interacting and maintaining secure APIs (REST, SOAP) with a working knowledge of SFTP and SSH in an enterprise environmentExpert knowledge of data integration tools/applications (e.g., Jitterbit, EDI translaters, SSIS, SSRS, Mulesoft, DMT, etc)Strong experience with integrations (HL7, FHIR, EDI) to enterprise environments including EMR/EHR systemsExposure to integrating cloud-based enterprise applications via real-time, batch, sync/async, ETLProficient in at least one programming languageExcellent problem-solving & troubleshooting skillsStrong communication skills (written and verbal)Bachelor’s degree preferredSALARY & BENEFITS: Salary of $120-130K annuallyMedical, Dental, and Vision401(k) with Company MatchSick and Vacation DaysFlexible Spending AccountLife & Disability InsuranceEducation AssistanceCOMPANY OVERVIEW: Since 1957, Shield Healthcare has provided high-quality healthcare services while focusing on customer satisfaction and employee achievement. We are dedicated to fulfilling the medical supply needs of consumers and the caregiving community while maintaining a 99% overall customer satisfaction rating.Job Type: Full-timePay: $120,000.00 - $130,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programEmployee discountFlexible spending accountHealth insuranceLife insurancePaid time offReferral programTuition reimbursementVision insuranceSchedule:8 hour shiftMonday to FridayWork Location: One location",https://www.indeed.com/company/Shield-Healthcare/jobs/Data-Integration-Engineer-2590a639d3ebb5f9?fccid=7f9ad87b930b5891&vjs=3,"Valencia, CA 91355 91355",Data Integration Engineer,Data Engineer
93,Tekrek Solutions,"Title: Data EngineerDuration: 12+ months contract extendableLocation: Sunnyvale or San Jose, CA (Onsite role)Minimum qualificationsBachelor's degree in Data or Engineering, or related technical disciplines or equivalent practical experience.Experience with BI Analytics and Data ETL (Extraction, Transformation, and Loading) with large amounts of dataProficiency with Standard SQL and Dashboarding with DataStudioExperience with standard data metrics such as shape, distribution, standard deviation, and the corresponding visualizations such as histograms, trends, bars and chartsUnderstanding of standard data quality metricsPreferred qualificationsProficiency with PLX, F1, DremelFamiliarity with GCP BigQueryFamiliarity with with Jupyter Notebooks & PythonOther SkillsAnalytical and quantitative skills with the ability to use data (including SQL) and metrics to back up assumptions, evaluate outcomesSelf-starter with a firm sense of accountability and ownership on various deliverablesAbility to dive into a complex data set , find the hidden truth, and turn it into a plan of action.Job Types: Full-time, ContractSchedule:8 hour shiftExperience:Data Modelling: 6 years (Required)SQL: 6 years (Required)Python: 6 years (Required)Work Location: One locationSpeak with the employer+91 (925) 412-3732",https://www.indeed.com/company/Tekrek-Solutions-Inc/jobs/Data-Engineer-2133924944335ed8?fccid=feec4fef4f413549&vjs=3,"Sunnyvale, CA+1 location","Data Engineer - Sunnyvale, CA OR San Jose, CA - Onsite",Data Engineer
94,Starr Way,"Multiple roles available such as Airflow Developer, Cloud Architect, Data ArchitectA day in the life: You'll be able to speak to and deliver solutions around Apache Airflow. You'll interact with customers to drive successful Airflow implementation. You'll be a trusted advisor and help architect data pipelines across all types of environments and use cases. Last but not least, you'll have the opportunity to engage with the Apache Airflow project, a thriving open source community with 1000s of contributors. Work with customers and partners to ensure they are following best practices in Airflow data pipeline creation· Develop Airflow Pipelines with Python.· Perform Airflow code refactoring assessments for our customers.· Be the airflow technical resource for customers by educating them on Airflow best practices.· Collaborate with the customer to design, prototype, and implement proof-of-concept engineering solutions.· Be an advocate for all customers by providing them with Data Engineering as a Service (DEaaS).· Act as a liaison by collecting feedback and identifying improvements through client interactions.· Become an Airflow expert.Airflow Developer - Data Engineer A strong candidate has: · 2-4 Years Python Develop for Airflow DAGS· 2-4 years of industry experience including some experience working in Apache Airflow in a production level environment.· GCP experience is strongly preferred.· Excited to learn infrastructure tools like Docker and Kubernetes.· Empathetic and leans towards giving those around them the benefit of the doubt.· A strong verbal and written communicator.· Eager to help customers solve problems and succeed in making Airflow the de-facto standard in data orchestration.· Experienced with creating DAGs, writing Python, building custom operators and hooks.· Excited about implementing ETL/ELT processes.Job Type: Full-timePay: $83,183.00 - $300,000.00 per yearBenefits:Dental insuranceFlexible scheduleHealth insuranceLife insurancePaid time offVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus paySigning bonusWork Location: Remote",https://www.indeed.com/company/Starr-Way/jobs/Data-Engineer-Interview-Week-f23961775c47443d?fccid=488cf3f013cc9f3a&vjs=3,Remote,Data Engineer**up to 300k**interviews this week,Data Engineer
95,Orion Inc.,"Identifying installation solutions for new databasesDetermining the requirements for a new databasePublishing and/or presenting design reportsIdentifying areas for improvement in current systemsCoordinating with other team members to reach project milestones and deadlinesAuditing database regularly to maintain qualityCreating systems to keep data secureExperience with NoSQL databases such as CosmosDB, Postgres NoSQL, and Cassandra a big plusWorking knowledge of Python and Kafka a plus.Develop set processes for data mining, data modeling, and data production.Create custom software components and analytics applications.Job Type: ContractSchedule:8 hour shiftDay shiftAbility to commute/relocate:Berkeley Heights, NJ: Reliably commute or planning to relocate before starting work (Preferred)Experience:Data Engineer: 5 years (Preferred)ETL/ELT: 5 years (Preferred)Azure/AWS: 3 years (Preferred)Python: 2 years (Preferred)Work Location: One location",https://www.indeed.com/company/Marvel-Infotech-Inc/jobs/Data-Engineer-54e317f3d16e0d2d?fccid=4d646c7bde0d7dab&vjs=3,"Berkeley Heights, NJ",Data Engineer,Data Engineer
96,Stellar Health,"About Stellar Health:

Historically, US Healthcare has relied on a fee-for-service reimbursement system where providers are paid based on the quantity of patient visits and procedures instead of the quality of health outcomes. Stellar Health helps primary care providers continually engage with their patients by providing them real-time information and tangible action-based incentives for improving quality of care for their patients.

At Stellar Health, we're simplifying value-based care. The Stellar platform, a cloud-based, point-of-care tool, fits into existing provider workflows and creates a simple checklist of recommended actions that helps deliver the best quality care for patients. We're building our technology from the ground up, and the industry is starting to take notice.

Our product actively supports over 500K patient lives - and we have shown that we make a real difference for these practices and their patients. It's an exciting time to join the Stellar Team: We recently raised a $60M Series B at a $460M valuation from Top VCs in the health-tech space (General Atlantic, Point72, & Primary Venture Partners), have an established product & proven operating model, and we're scaling our team to meet the continued growth and impact on our healthcare system.

About the position:

Software Engineers working on our Data Platform at Stellar Health are responsible for building creative solutions that enable the exchange of healthcare information across various internal and external systems. In addition to supporting our client growth in the short term, you'll also have the opportunity to help build the next version of our data platform so that we can scale over the long term.

Healthcare providers work with many different systems and a software engineer at Stellar will need to be comfortable working within these varying environments. You'll have the chance to work with cross-functional teams to achieve the goal of translating institutional information into meaningful insights in order to improve patient care.

What you will do:

Develop solutions for processing data formatted in various industry standards (ANSI X12, HL7, FHIR, CDA, JSON, TXT, XML)
Develop, test and implement interfaces based on standards and customer requirements
Build and maintain networking and integration infrastructure
Monitor live customer interfaces and data pipelines, resolve errors or issues and ensuring data reliability and security
Take ownership of internal and customer-facing deliverables requiring technical input or development
Work cross functionally with different Stellar teams, as well as external customers and partners

Skills & Experience:

2+ years experience writing production code (required).
Bachelor's Degree in Computer Science, related technical field or equivalent
A self-motivated quick learner who works well with other members of the team
Well organized and detail-oriented
Capable of juggling multiple concurrent tasks successfully
Knowledge of programming languages and familiarity with data processing & storage. Python, Postgres & SQL experience is preferred
Experience with ETL tools & libraries, as well as familiarity with health data and standards (e.g., FHIR, HL7), are pluses

Pay, Perks & Such:

We offer a full slate of benefits, including competitive salaries, stock options, medical, dental, vision, life and disability coverages, a 401k program and flexible vacation.

Note on COVID response:

Stellar's Union Square NYC HQ is currently open for those that wish to use it and we are supporting both hybrid and fully-remote work across the US! If you would like to learn more about our COVID response measures and hybrid work policies, please mention this when you speak with a Stellar Recruiter.

Diversity is the key to our success. Stellar Health is an equal opportunity employer and we are open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, veteran status, or any other legally protected status.

We believe that diverse teams -and the different identities, cultures, and life experiences our team members bring to the table- enable us to create amazing products, find creative solutions to interesting problems, and build an inclusive working environment.

We encourage everyone to apply to our roles, even if they do not meet every single qualification on a job description, and our team is committed to reviewing every resume submitted.",https://www.indeed.com/rc/clk?jk=9405537a48c0abd5&fccid=9a68164dfb69290f&vjs=3,"Remote in Austin, TX+3 locations",Software Engineer ll - Data Platform,Data Engineer
97,Infinitek,"The Job: Our Global Engineering and Technology team is seeking a new Data Engineer! This person will analyze complex and unstructured data using state-of-the-art data science and engineering methods to help with data-driven decision-making.What You'll Do: This role uses knowledge of data science methods and applies analytics to solve real-world problems. The Data Engineer will work with customers and/or internal stakeholders to collect and understand data, perform wrangling, develop, and integrate applications and collaborate.Other Tasks: Utilize your knowledge of modern data-driven methods and domain understanding to support the creation of new products, services, and insights using predictive modelsYou have a demonstratable ability to write independent source code, validate and test for model quality improvementAre comfortable in machine learning, artificial intelligence, classification, regression, NLP, etc.Use your experience and are hands-on in Python, Database Systems (such as SQL), and Azure cloud tools to contribute towards solving data analytics problemsYou are ‘Database-Centric’ and perform data wrangling and processing from multiple databases (structured or unstructured data) with a keen eye for data quality all alongYou think in a ‘Pipeline-Centric’ manner and are comfortable with using DevOps Repo and can build, manage and optimize data pipelines and deploy these pipelines into productionYou have exposure to developing and using tools like Python libraries and visualizationBe able to think independently and work as part of a cross-functional team to achieve common goalsComfortable working independently on data modeling/engineering tasks and are eager to learnStay current in the field of advanced analytics and take initiative to apply new technologiesQualificationsWhat You'll Need: Minimum 3-4 years of experience in data analytics as a data engineer, data analyst, data scientist, or applied scientist roleBS or BA in Data Engineering, Data Science Computer Science, Engineering, or another relevant field. MS PreferredExperience in using Machine Learning, Ai, and other data science technologiesStrong and demonstrable experience with programming languages such as Python (used NumPy, pandas, etc.)Strong and demonstratable knowledge of GitHubStrong knowledge of data architectures and systems – SQL and others for ETL purposesStrong knowledge of DevOps, in particular, using the Repo and pushing and pulling source code with versioningAbility to work with MS Azure cloud toolsIntermediate C# and Power Shell scripting skillsGood skills in using Web APIs and XMLExperienced in using tools such as MS Power BI, Tableau or other visualization tools is a big plusKnowledge of object-oriented programming and building RestFul APIs for data science modeling is a plusDisplay an entrepreneurial mindset and think practical, ‘out of box’ solutionsAbility to work efficiently and deliver timely in a fast-paced environmentJob Type: Full-timeSalary: $100,000.00 - $135,000.00 per yearSchedule:Monday to FridayExperience:SQL: 1 year (Preferred)Python: 3 years (Preferred)Work Location: Multiple Locations",https://www.indeed.com/company/Infinitek/jobs/Data-Engineer-6274a275d76bd7be?fccid=cf2d4f7a404630c0&vjs=3,"Houston, TX",Data Engineer,Data Engineer
98,Sublime Wireless Inc,"Your Responsibilities Will Include: · Building Data Lake using AWS and hands-on experience in S3, EKS, ECS, AWS Glue, AWS KMS, AWS Firehose, EMR.· Developing sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies.· Operationalizing data pipelines to support advanced analytics and decision making.· Building data APIs and data delivery services to support critical operational and analytical applications.· Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake.Qualification, Knowledge, Skills & Experience: BS degree in Computer Science, Data Engineering or similar Intermediate experience in an Apps Development role.Job Types: Full-time, ContractSalary: $80.00 - $90.00 per hourExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data Engineering: 7 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Sublime-Wireless-Inc/jobs/Senior-Data-Engineer-9cb90629b57d830d?fccid=5c7a0a3cee7e1d04&vjs=3,Remote,Sr. Data Engineer,Data Engineer
99,Modak,"Position Description(Briefly state the primary duties and responsibilities of the position):Data Engineer5+ years of experienceDesign and develop distributed data processing pipelines and tools (StreamSets preferred)Scripting languages - Python, Groovy, KafkaExperience with GCP and/or Azure cloud servicesExperience with Relational and No-SQL document databases like PostgreSQL, MongoDB AtlasExperience with developing REST APIsPreferred to have -o StreamSetso Understands application security and cloud IAMo Serverless and cloud networkingNice to have -o Dataproc/Spark, Azure data factory, Azure Synapse, Azure datalake, CI/CDExperience in Agile Scrum FrameworkGood Communication skills, offshore collaborationRequisition ReasonPosition DurationNew Role (Yes/ No):yesPermanent (Yes/No):N/AReplacement (Yes/No):NoTemporary(Yes/No):N/AIf Replacement, name of the employeebeing replaced:If Temporary, please mention theend date:Skillset RequiredPrimary Skill:StreamSets, Kafka (ETL Tools), Python, Groovy, Kafka, GCP/Azure cloud services, SQL, NoSQL databases (PostgreSQL, MongoDB Atlas), REST API developmentSecondary Skill:Applied Security and cloud IAM, Serverless and cloud networkingTertiary Skill:Dataproc/Spark, Azure data factory, Azure Synapse, Azure datalake, CI/CD, Agile Scrum Framework, Good Communication skills, offshore collaborationJob Types: Full-time, Contract, TemporaryPay: $76,000.00 - $120,000.00 per yearBenefits:Flexible scheduleSchedule:8 hour shiftMonday to FridayCOVID-19 considerations:All customers n employees wear mask and follow the COVID GuidelinesEducation:Bachelor's (Preferred)Experience:Python , Kafka , Groovy: 3 years (Required)design and development: 5 years (Required)Azure cloud services: 2 years (Required)Work Location: One location",https://www.indeed.com/company/Modak/jobs/Data-Engineer-483bf79cb659e969?fccid=b52035682242fcd7&vjs=3,"Louisville, KY 40202 40202 (Central Business District area)",Data Engineer,Data Engineer
100,Direct Supply,"Position Summary: Direct Supply is building the future of senior living technology, helping connect the spectrum of healthcare in order to improve the lives of millions of seniors!Direct Supply is looking for a talented and passionate Data Engineer with the ability to optimize our data using ETLs, data pipelines, and analytics/BI tools for efficiency and quality. You’ll work on a team of engineers and architects on data initiatives, including advanced reporting and data analytics that will directly drive top-line revenue. You’ll be working to solve real problems our customers experience and make a meaningful difference. The team will collaborate to bring the design to life and provide you the opportunity to grow your craftsmanship skills with modern technologies.What You’ll Do and Impact: Develop solutions for reporting and data analytics that meet defined usability, maintainability, scalability and reliability requirements. Solutions must drive efficiency and meet quality standards.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Take ownership of assigned work, monitor and maintain the health of solutions generated. Increase operating efficiency and adapt to new requirements.Author small and well-defined programming tasks.Code, test and debug programs as outlined by a system’s technical and functional requirements.Create and maintain automated testing and document testing procedures.Provide task breakdowns, identify dependencies and provide effort estimates.Identify coding issues and communicate problems within a team.Assist in preparation of functional and technical documentation to provide guidance to end users and support trouble shooting.Take guidance from peers and senior members and offer constructive ideas to development solutions through sharing of discoveries and best practices.What You’ll Need: Bachelor’s degree in Computer Science, Computer Engineering or Software Engineering1-3 years of experience working with ETL frameworks and the SQL database environmentExperience developing with BI tools such as Cognos or Power BIDemonstrated understanding of proven design principles and code practicesKnowledge and understanding of RESTful APIsExperienced in automated testing frameworksAbility to work autonomously and collaboratively in a team environment using frameworks like SCRUM & Agile to deliver on customer commitmentsAdditional Items of Interest: AWS/cloud experienceKnowledgeable in Continuous Integration / Continuous Deployment (CI/CD) / Build automation practicesTerraform infrastructure as code experience#LI-PG1Job to be performed in the location listed. Generous benefit package available. Click here to learn more.Direct Supply, Inc. and its U.S. subsidiaries are equal opportunity and affirmative action employers committed to diverse workforces.© 2013 to 2020 Direct Supply, Inc. All rights reserved.Job Type: Full-time",https://www.indeed.com/company/Direct-Supply/jobs/Data-Engineer-a1910c95da8aeed1?fccid=7a9841454b2e9552&vjs=3,"Milwaukee, WI",Data Engineer,Data Engineer
101,PCS Global Tech,"ResponsibilitiesAnalyze and organize raw dataBuild data systems and pipelinesEvaluate business needs and objectivesInterpret trends and patternsConduct complex data analysis and report on resultsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesCombine raw information from different sourcesExplore ways to enhance data quality and reliabilityIdentify opportunities for data acquisitionDevelop analytical tools and programsCollaborate with data scientists and architects on several projectsRequirementsPrevious experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, and segmentation techniquesKnowledge of programming languages (e.g. Java and Python)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT, or similar field; a Master’s is a plusJob Types: Full-time, ContractSalary: $80,000.00 - $90,000.00 per yearSchedule:8 hour shiftMonday to FridayWork Location: Multiple Locations",https://www.indeed.com/company/PCS-GLOBAL-TECH/jobs/Data-Engineer-8654e440705e9d20?fccid=252beafbd99a6037&vjs=3,"Minneapolis, MN+1 location",Data Engineer,Data Engineer
102,WorkCog,"Description: · Required Bachelor's Degree or equivalent Preferred Master's Degree· 5 years of relevant experience 5 to 10 years’ experience required At least Five years of relevant experience developing and supporting applications, and or system development lifecycle including coding, testing, and implementation required.· Strong Experience in Programming with Oracle SQL and PLSQL.· Experience in Data Integration using Informatica Power Center.· Experience in Programming with Python.· Experience in Agile/Scrum Methodologies.· Well-versed with concepts and techniques of Business Intelligence and Data Warehousing.· Strong written and oral communication skills with ability to interpret technical requirements.· Excellent analytical skills to troubleshoot and resolve systems problems.· Effective and self-sufficient in working within a diverse technology portfolio.Job Types: Full-time, Part-time, ContractPay: $40.00 - $60.00 per hourSchedule:8 hour shiftWork Location: Multiple Locations",https://www.indeed.com/company/WorkCog/jobs/Data-Engineer-5f769ed5adef7f7b?fccid=04542d75b9ce9d61&vjs=3,"Jersey, GA+4 locations",Data Engineer,Data Engineer
103,WorkCog,"Description: · Required Bachelor's Degree or equivalent Preferred Master's Degree· 5 years of relevant experience 5 to 10 years’ experience required At least Five years of relevant experience developing and supporting applications, and or system development lifecycle including coding, testing, and implementation required.· Strong Experience in Programming with Oracle SQL and PLSQL.· Experience in Data Integration using Informatica Power Center.· Experience in Programming with Python.· Experience in Agile/Scrum Methodologies.· Well-versed with concepts and techniques of Business Intelligence and Data Warehousing.· Strong written and oral communication skills with ability to interpret technical requirements.· Excellent analytical skills to troubleshoot and resolve systems problems.· Effective and self-sufficient in working within a diverse technology portfolio.Job Types: Full-time, Part-time, ContractPay: $40.00 - $60.00 per hourSchedule:8 hour shiftWork Location: Multiple Locations",https://www.indeed.com/company/WorkCog/jobs/Data-Engineer-5f769ed5adef7f7b?fccid=04542d75b9ce9d61&vjs=3,"Jersey, GA+4 locations",Data Engineer,Data Engineer
104,PCS GLOBAL TECH,"Data Engineer Responsibilities:Liaising with coworkers and clients to elucidate the requirements for each task.Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.Reformulating existing frameworks to optimize their functioning.Testing such structures to ensure that they are fit for use.Preparing raw data for manipulation by data scientists.Detecting and correcting errors in your work.Ensuring that your work remains backed up and readily accessible to relevant coworkers.Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.Data Engineer Requirements:Bachelor's degree in data engineering, big data analytics, computer engineering, or related field.1 to 3 years of experienceExpert proficiency in Python, C++, Java, R, and SQL.Familiarity with Hadoop or suitable equivalent.Excellent analytical and problem-solving skills.A knack for independence and group work.Scrupulous approach to duties.Capacity to successfully manage a pipeline of duties with minimal supervision.Job Type: Full-timePay: $60,000.00 - $70,000.00 per monthWork Location: Multiple Locations",https://www.indeed.com/company/PCS-GLOBAL-TECH/jobs/Data-Engineer-Entry-Level-af419d93b41ee7a4?fccid=252beafbd99a6037&vjs=3,"Dallas, TX+1 location",Data Engineer Entry Level,Data Engineer
105,Central Intelligence Agency,"Data Engineers work with data consumers to create and populate optimal data architectures, structures, and systems to meet CIA’s business needs.
Full time
Starting salary: $66,077 - $164,102
Bachelor's or master's degree
Opportunities for domestic travel are possible

About the Job

As a Data Engineer for CIA, you will focus on the design, implementation, and operation of data management systems to meet the CIA’s business needs. This includes designing how the data will be stored, consumed, integrated, and managed by different data entities and digital systems. Data Engineers work together with data consumers to determine, create, and populate optimal data architectures, structures, and systems.

Data Engineers must also plan, design, and optimize for data throughput and query performance issues. This requires constantly updating expertise in areas such as platform, network and storage technologies, bandwidth management, data bus implications, and design.

Additionally, you will play a key role in the selection of backend database technologies (SQL, NoSQL, HPC, etc.), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.

Domestic and/or foreign travel may be required.

Who You’ll Work With

At the Central Intelligence Agency (CIA), we recognize our Nation’s strength comes from the diversity of its people. People from a broad range of backgrounds and viewpoints work at CIA, and our diverse teams are the reason we can keep our country safe.

Read more about diversity and inclusion

What You’ll Get

Our benefits support every aspect of a working professional’s life, including health and wellness, time off, family, finances, and continuing education. Our programs include highly sought-after government health benefits, flexible schedules, sick leave, and childcare. In some cases, we also offer sign-on incentives and cover moving expenses if you relocate.

As a CIA employee, you’ll also get the satisfaction of knowing your work is part of something bigger than yourself. Our work is driven by one mission: to keep our Nation safe. Every day is an opportunity to enhance U.S. national security.

Learn more about working at CIA

Minimum Qualifications
Bachelor’s or master’s degree in one of the following fields or related studies:
Archives/Digitization Management
Computer/Data Science
Information/Data/Knowledge Management
Information Technology
Management Information Systems
Mathematics
Engineering
At least a 3.0 GPA on a 4-point scale
Knowledge of the following:
Data manipulation
Databases
Data structures
Data management
Best engineering practices
Ability to meet the minimum requirements for joining CIA, including U.S. citizenship and a background investigation
Desired Qualifications
Work experience related to data engineering, information, and data management
What You’ll Need to Apply
Resume
Unofficial transcripts for all degrees
Cover letter in which you specify your qualifications for one or more positions. Please address why you want to work in this role and what differentiates you from other applicants.",https://www.indeed.com/rc/clk?jk=6fe8eebdc5bcf567&fccid=e9870e3159e9c6ac&vjs=3,"Washington, DC 20505 20505 (Foggy Bottom area)",Data Engineer,Data Engineer
106,Exponentlogic Solution,"Responsibilities for Data EngineerCreate and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Qualifications for Data EngineerAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with big data tools: Hadoop, PySpark, Kafka, HiveExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.Experience with AWS cloud services: EC2, EMR, RDS, RedshiftExperience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.Job Types: Full-time, Part-time, ContractPay: $55.00 - $60.00 per hourSchedule:Monday to FridayWork Location: One location",https://www.indeed.com/company/Exponentlogic-Solution/jobs/Data-Engineer-eebeb6534296d431?fccid=7574576124b47b82&vjs=3,"Charlotte, NC 28202 28202 (Downtown Charlotte area)",Data Engineer,Data Engineer
107,Capstone Logistics LLC,"JOB SUMMARY: The ideal candidate will possess strong background on Data Engineering development technologies & experience with cloud-based resources. The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.· This role will be responsible for both team collaboration as well as project initiative ownership and leadership.· This role is for contributing to both technical expertise as well as process leadershipSUPERVISORY RESPONSIBILITIES: · No direct reports but responsible for project ownership which may have reports.· Responsible for mentoring of Jr. resourcesESSENTIAL FUNCTIONS: Responsibilities: · Understand the business data needs, both current and future to help deliver and guide a data strategySet and ensure proper adherence to strategic and tactical patternsSet technical direction for obtaining and implementing technologyHelp evaluate current and future staffing needs· Drive the creation of new standards and best practices as technology evolves; communicate and drive adoption across the companyDrive creation of new data capabilities and offerings*Performs Additional Responsibilities As Assigned*QUALIFICATIONS: education and/or experience: · Bachelor’s Degree in a technical field (statistics, mathematics, science, accounting, finance) or related field· 4+ Heavy Data experience across the full data life cycle· Experience in architecting enterprise-wide data environments and platforms· Heavy experience in data movement, transformation, and modeling processes (ELT/ETL)· 4+ years experience working with large datasets or database systems· 2+ Years working with cloud data technologies· AWS/Azure Experience· MPP technology Experience (Snowflake/Synapse/Redshift)knowledge, skills and abilities: · Cloud data experience. Storage, compute, movement orchestration, reporting· Full data stack experience· Traditional and modern data warehousing expertise· Data modeling expertise· Experience in data lake patterns and process· Excellent interpersonal and communication skills (written and verbal)Preferred Skills: · AWS data experience· SnowflakeColumstore experienceMPP platform experience is a plus· Power Bi /Tableau· Micros Service ExperienceProfessional Skills: · Demonstrated ability to manage time and prioritize projects to meet deadlines· Excellent listening, written, and oral communication skills· Excellent critical thinking skills to help solve business problems and make decisions, paired with a desire to take initiative· Ability to maintain project plans, resourcing schedules, and forecasted activities· Ability to work well under continually changing deadlines and prioritiesJob Type: Full-timePay: $150,000.00 per yearSupplemental Pay:Bonus payWork Location: Multiple Locations","https://www.indeed.com/company/Capstone-Logistics,-LLC/jobs/Senior-Data-Engineer-4cbfa1a8320af7fc?fccid=ed5301e8ef9eec8f&vjs=3","Atlanta, GA+1 location",Sr. Data Engineer,Data Engineer
108,haramainsystems,"Role: Big Data Engineers (Multiple openings)Location _Remote (EST or CST time zones)Duration : Long Term Big Data Engineers, any who also have some DBA experience a bonus.They have built the logic but need help building the tuning and analytics for the 7000+ dealer locationsAll backend workQualifications: · BigData: Hadoop / Spark (preferably with Scala). Looking for Engineers with hands on experience.· Must have Strong Experience on Spark and Hive SQL.· Knowledge on Database architectures of RDBMS.· Experience in building Data-pipeline, complex SQL and ETL· Experience with Data-formats like Parquet, CSV, JSON etc.· Data-storage architectures like HDFS, S3 and Hive.· Data-transformations concepts including Partitioning, Shuffling,· Translate, load and present disparate datasets in multiple formats/sources including JSON, text, SQS.· Data-processing constructs like hive joins, MapReduce.Job Types: Full-time, Part-time, ContractPay: From $55.00 per hourSchedule:8 hour shiftExperience:Big data: 10 years (Preferred)Spark: 5 years (Preferred)Hive SQL: 3 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Haramainsystems/jobs/Big-Data-Engineer-a7242719137d1216?fccid=b4699182ab1c7c79&vjs=3,Remote,Big Data Engineer,Data Engineer
109,realtor.com,"Who We Are

Realtor.com is your one-stop shop for homebuyers, sellers, and dreamers, with comprehensive for-sale listings, insightful information, valuable tools, and professional expertise. We make buying, selling, renting, and living in homes easier and more rewarding for everyone. We're excited and hungry to make a difference to the millions of people in the US who buy a home each year. Our aspiration starts with treating our home buyer as our focal point - we believe that delivering on experiences that truly help people through this seemingly difficult life event will lead us to continued and greater success as a company.


About The Role

We have an exciting and unique opportunity for a motivated and energetic Senior Software Engineer to join the Realtor.com Data Ingestion team within the Data Platform organization. Senior Software Engineers on the Data Ingestion Team are responsible for the services and infrastructure capable of receiving, ingesting, and processing the extensive volume of data produced on the various Realtor.com platforms each day. Data Ingestion Engineers build the infrastructure and tools needed to enable engineers, analysts, and scientists to send real time data to products that operate against our largest data sets. In this position, you will work to build scalable ingestion points that guarantee quality data in real time to downstream infrastructure. We are building out a modern real-time data platform where you will get to drive decisions and be part of a team that has a lot of fun along the way!


What you'll do

Responsibilities

Contribute to a team culture that values collaboration, technical excellence and innovation
Implement new data infrastructure and features that enable our internal analytics teams to create real time feedback and make data driven decisions
Develop reusable components and frameworks for ingestion, cleansing, and data quality using services and tools offered by AWS
Develop and operationalize data pipelines, backend services, and distributed systems using advanced data architectures deployed on AWS
Utilize our developer toolchain to support instant provisioning of new services and infrastructure, fully automated deployment, and maximize efficiency.
Own the delivery, quality, and reliability of our data platform and act as support for production issues, profile performance, and root cause analysis.
Collaborate across departments or teams to deliver on projects
Maintain/support/update existing components and frameworks as required
Participate in hackathons and Proof Of Concept experiments to continuously improve our products
Perform presentations of our products/features to internal stakeholders
Work in a Agile/Scrum product development process

About you

Requirements

An experienced software engineer that is passionate about building great products and a quality codebase in a team-focused environment
Excited and motivated to learn new technologies
5+ years experience building backend infrastructure and APIs using API best practices and industry standards
Experience coding in JavaScript or TypeScript, Python and Node.js or other similar languages
Familiar with cloud-based architectures
Familiar with data collecting, streaming, and processing technologies
Exposure to build, test, and deployment automation technologies
Ability to clearly document and communicate technical requirements
Ability to perform technical presentations to internal stakeholders
Bachelor's degree in Computer Science/Engineering or related field or related experience

Nice to Have

Experience with Javascript libraries such as React for building UI for internal tools

About realtor.com

At realtor.com®, we believe that everyone deserves a home of their own. We're a community of nearly 2,000 employees that work hard to ensure that from the moment someone starts dreaming about a new home, to the moment they walk in the door and beyond, we're there to lend a helping hand. Every month, 70 million people trust us with their journey home by visiting our site and mobile apps, and we'd love to have you join our team to help.

We've got great offices across Canada and the US and lots of sweet jobs to choose from, so we're hoping you'll join us on our journey to make home buying and selling easier, and more rewarding for everyone.

Let's make a difference, together. For Real.",https://www.indeed.com/rc/clk?jk=f052a4d6c36c07da&fccid=1c8655b948495cbc&vjs=3,"Santa Clara, CA",Senior Software Engineer - Data Ingestion Team,Data Engineer
110,Replica,"Replica is an enterprise data platform that delivers critical insights about the built environment. With better data, human-context and an intuitive design, Replica helps public and private sectors make informed, effective, and responsive decisions. By showing how people live, move and work, we contextualize hard choices, allowing our clients to see around corners and understand the trade-offs surrounding their decisions. Whether for a city planner increasing public transit to underserved neighborhoods or for a grocery chain evaluating where to open a new location, Replica's insights lets clients make more informed, people-centered decisions.

We spun out of Alphabet in 2019 and recently secured series B funding from venture firms including Founders Fund, Innovation Endeavors, Sidewalk Labs, Firebrand Ventures, and Revolution's Rise of the Rest Seed Fund. Today, we are a team of 50 employees with offices in Oakland, New York, and Kansas City.

We value our customers, believe in being resourceful, and work in service of each other to scale our product. As we build our team, we are committed to pursuing and bringing together a diverse workforce and creating an environment of inclusion. We value our differences and we encourage all to apply. We are committed to equal opportunity regardless of race, color, ancestry, religion, gender, gender identity, parental or pregnancy status, national origin, sexual orientation, age, citizenship, marital status, disability, Veteran status, or any other status protected by the laws or regulations in the locations where we operate.

The Role

As our engineering team grows, we are looking for a data engineer working on geo data related data pipelines and data products. As an early member on the team, you will balance speed and quality as you build out new features for our customers.

You will collaborate with engineers, design, research and development, and customers to enable additional methods of querying and create valuable data visualizations for every customer.

Organizational Relationships

Supported by Data Engineering Manager who reports to the Director of Engineering. Primarily collaborates with Product, Data Management and Procurement and Customer Success teams as cross-functional partners.

Responsibilities
Work through all stages of a data solution life cycle: analyze the problem domain and derive data modeling solutions, create conceptual and logical data model/schema designs, design, improve and optimize data pipeline and workflows in terms of robustness and performance, derive reporting and analytics solutions.
Work closely with the customer success team to fully understand their needs and respond quickly to their feedback.
Own the process and automation for data intake, QA and data delivery.
Identify gaps and invent processes, automated scripts, and tools to efficiently carry out geo data processing tasks in a scalable fashion.
Work with machine learning algorithm researchers to incorporate cutting edge modeling techniques into production pipelines.
Establish testing strategy and best practices.
Focus on support for day to day operations for delivering customer data
Develop, execute, and maintain customer and mega region runbooks
Add logging, observability, and notification for data workflows.
Actively contribute to company culture by mentoring engineers, contributing to documentation, and actively collaborating with cross-functional groups.
Minimum Qualifications
5+ years of experience working with data engineering projects.
3+ years as a software engineer.
Efficiency with automation and tooling languages like python to build reusable components/tools.
Proven experience working with data pipelines at scale and knowledge of common big data frameworks, such as hadoop/bigquery.
Technologies: Python, Pandas, PostGIS, Big Query, Dask, Helm, Kubernetes, Prefect
Preferred Experience with common geo format (e.g., geojson, geodatabases like postgresql/postgis) is a plus
What We Value
We work in the service of others
We understand that talent + diversity + curiosity + relentlessness wins
We believe walking > talking
We operate with thoughtful urgency
We communicate openly and directly
We build products people use
Benefits
Our people! We work as a team and are excited to contribute to city planning
Competitive salary based on experience and potential for impact
Equity at an early stage startup
Health benefits including medical, dental, vision, FSA and HSA options
401k account + employer contribution
Offices in Oakland, New York, and Kansas City
Flexible PTO
Replica is an Equal Opportunity Employer
Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
To perform this job successfully, an individual must be able to perform each essential function satisfactorily.
The requirements listed are representative of the knowledge, skill and/or ability required.
Ability to operate at both a strategic/conceptual level and at a detailed, operational level metrics driven; highly disciplined
Must have strong interpersonal skills, maturity and good judgment and be capable of communicating with a diverse range of individuals
A hands-on, action-oriented approach that fits well with the entrepreneurial, fast-paced culture
Engaging leadership style that builds and sustains credibility with colleagues, clients and other stakeholders
Broad functional experience in areas of strategic planning and marketing, sales and market development and planning

If you don't think you meet all of the criteria above, but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team.",https://www.indeed.com/rc/clk?jk=8cf0436da62cca04&fccid=5741f95e8f7cd005&vjs=3,"San Francisco, CA",Data Engineer,Data Engineer
111,Restoration Hardware,"RH is seeking a Associate Data Analyst to analyze and sort large amounts of contact center data to identify performance and volume trends and anomalies. The Analyst plays a critical role in ensuring success of our Customer Delight teams.
YOUR RESPONSIBILITIES
Live Our Vision, Values and Beliefs every day
Build, monitor and troubleshoot Extract, Transform and Load nightly jobs
Curate, analyze and develop new attributes and metrics from disparate data sources and integrate with existing data warehouse
Create automated reporting using tools such as MicroStrategy, Microsoft Reporting Services, Tableau
Answer and resolve on demand business inquiries and analysis
Analyze and track data trends while gathering data via SQL or OLAP Cube
Discover and recommend business opportunities through data analysis
Provide timely status updates on projects and initiatives
Self-driven ability to gather data and documentation to support optimization and company vision
Completes post-mortem analysis to highlight areas of opportunities and planning adjustments
OUR REQUIREMENTS
Schedule flexibility to see a project/analysis to completion which commonly requires working nights and/or weekends
Experience with relational SQL and SQL Server databases, including Postgres, MySQL, or NoSQL
Advanced working SQL experience and expertise working with relational databases, query authoring (SQL), and working familiarity with various SQL dialects.
Understanding of basic supply chain metrics, processes and practices
Proven experience processing and extracting value from large ambiguous datasets.
Strong project management and organizational skills
Understanding of contact center metrics, processes, and practices
Ability to exercise judgement and make sound decisions under pressure
Excellent verbal and written communication skills
Must be able to work in a fast paced environment handling multiple activities simultaneously
Excellent prioritizing, planning and organizational skills
Prove ability to adapt openly and accept change
Takes accountability for decisions and actions
YOUR SKILLS
Strong SQL knowledge (verification required)
Proficiency in designing and implementing data warehouse concepts
Proficifiency in Extract, Transform and Load procedures
Microsoft Integration Services (SSIS) Experience a plus
Familiarity with Cloud providers (AWS, SnowFlake)",https://www.indeed.com/rc/clk?jk=8fc3d5b1d06142cb&fccid=e5ed0165d5ce58ea&vjs=3,"Corte Madera, CA",Data Warehouse Engineer,Data Engineer
112,Regal Voice,"Regal Voice

Regal Voice is the new way brands interact with their customers at the most important moments. We are a next-gen customer engagement platform that allows brands to build phone call and text message campaigns into their digital experiences to drive 25%+ more revenue. We work with leading brands including SoFi, Angi, Via, MakeSpace, and Farmer's Dog. Headquartered in New York City, Regal Voice is backed by fantastic venture capital investors.

Regal Voice is led by industry veterans who previously built Handy (acq. by IAC / Angi in 2018), and the rest of our team is made up of product managers and engineers who bring expertise in real-time data, digital marketing and e-commerce.

As the creators of online on-boarding and buying experiences for millions of users, we know how hard it is to engage customers once they leave your site. With Regal Voice, brands are empowered to reach customers before they buy elsewhere - using voice and SMS campaigns that are event-driven, no-code and customer friendly.

Most importantly, our team shares a common set of values:

We are customer-obsessed
We know data beats opinion
We believe fast-paced execution wins' markets
We espouse a growth mindset

Regal Voice Data

One of the most exciting parts of Regal Voice is the massive dataset we are building. Regal Voice ingests 10s of millions of real-time behavioral data points from customers daily and combines that with rich voice and sms conversation data to drive next best actions. We also track all agent actions in the product, and can use that data to derive insights about new product features or behaviors that can improve the efficiency of agents - so they can spend more time engaging with customers.

In this role, you will partner with the product engineering team building our online applications and our analysts who make the data mean something. You will help us organize, model, and present our data for internal use and in the Regal Voice application.

We are looking for Data Engineers of all skill levels or experienced engineers from other disciplines who may be looking to make the move to a big data environment. Compensation is $130-160K plus equity depending on experience.

Skills you have now or will quickly develop a deep understanding for:
Building ETL/ELT pipelines using Python
Modeling and querying data using SQL and DBT
Processing large workloads using distributed data systems such as Snowflake, Athena, etc.
Processing real-time streaming workloads with distributed event systems such as Kafka and Kinesis
Designing and maintaining production database systems leveraging your knowledge of different types of data stores, their use cases, trade-offs, performance, and limitations
Implementing data lake, warehouse, and lakehouse architectures leveraging your knowledge of data archiving and retrieval solutions and their relationship to access vs. cost

Things you will get exposure to:
Machine learning and data science
Business intelligence tools (e.g., Looker)
Continuous integration and delivery of production data products

Data engineering is a rapidly changing field - most of all, we're looking for someone who enjoys experimenting, keeping their finger on the pulse of current data engineering tools, and always thinking about how to do something better.

Perks & Benefits:

Paid parental leave
Competitive compensation packages, including cash and early stage equity
Medical, dental, vision insurance
Flexible PTO
Weekly happy hours
Monthly team events
Quarterly trips
Complete laptop workstation
Ground floor opportunity to work directly with the co-founders to shape the product

We are currently following a hybrid working model in our NYC office, with M/F work from home and T/W/TH in-office. Covid protocols in place.",https://www.indeed.com/rc/clk?jk=192f43f5962a012f&fccid=82a1af0798fedab3&vjs=3,"New York, NY",Data Engineer,Data Engineer
113,Tesla,"The Role:
Tesla is looking for a highly-motivated front-end data systems engineer to develop, implement, and optimize our data visualization, which will help drive the total manufacturing process of our cutting-edge Energy Products and Model 3 Powertrain at Gigafactory Nevada, outside Reno, Nevada. This is an exciting opportunity to contribute to Tesla’s world class manufacturing facility in Nevada. Tesla is a demanding and fast-paced environment where you will work on critical subsystems of an incredibly exciting product.
This role will contribute to, in whole or in part:Develop and execute data visualization tools that are highly scalable and dependable
Automate the generation of periodic reportingSet and enforce data visualization standardsGuide and upgrade our methods to align with emerging best practices? Develop and optimize ETL/pipelines/data streams from production equipment to centralized data warehousing, and other data warehousing work as neededOn-call support, where needed
Requirements:Track record of excellence in current and prior rolesMinimum 2 years of experience in comparable roleAble to work under pressure while managing competing demands and tight deadlinesBS in Computer Science or related fieldTechnologies and capabilities: Python, HTML, CSSKeywords and experiences: database security, database performance monitoring, database architecture, database administration (DBA), SQL, UI/UX
Desirable CriteriaInductive Automation - Ignition, High Performance HMI standards, Factory SCADA, JiraLinux Servers and Networking

Employee Benefits:As a full time Tesla employee, you will receive full benefits from day 1 for you and your dependents.Kaiser and UnitedHealthcare PPO and HSA plans (including infertility coverage)3 medical plan choices with $0 paycheck contributionVision & dental plans (including orthodontic coverage)Company paid Life, AD&D, short-term and long-term disability401(k), Employee Stock Purchase Plans, and other financial benefitsEmployee Assistance Program, Paid Time Off, and Paid HolidaysBack-up childcare and employee discounts",https://www.indeed.com/rc/clk?jk=7db2de18b2eaee40&fccid=86e9be6ce380173e&vjs=3,"Sparks, NV 89434 89434",Front-End Data Systems Engineer,Data Engineer
114,Judge Group,"Title: Data Warehousing Engineer Location: New York NY Duration: 12 Months contract Minimum Qualifications: Bachelor's degree in Technology, Engineering, Science, Economics, Business or related field or equivalent practical experience.5+ years of experience implementing data governance/data management capabilities that enable interoperability, decision making, and execution.Ability to effectively facilitate meetings using collaboration, dashboard, and other tools.Effective stakeholder management and verbal/written communication skills.Familiarity with using: Data Studio, SQL, Looker, Tableau, Google Sheets/Excel, and Google Slides/PowerpointEducation: Bachelor's degree in Technology, Engineering, Science, Economics, Business or related field or equivalent practical experienceYears Experience: 5+ years of experience implementing data governance/data management capabilities that enable interoperability, decision making, and execution.Ability to effectively facilitate meetings using collaboration, dashboard, and other tools.Job Types: Full-time, ContractSalary: $80.00 - $90.00 per hourSchedule:8 hour shiftExperience:Data modeling: 5 years (Preferred)data governance/data management: 5 years (Preferred)Data Studio: 5 years (Preferred)SQL: 5 years (Preferred)Looker: 5 years (Preferred)Tableau: 5 years (Preferred)Work Location: One location",https://www.indeed.com/company/Judge-Group/jobs/Data-Warehousing-Engineer-b078045acb2977c2?fccid=91bc0e1837aec5b1&vjs=3,"New York, NY 10002 10002 (Lower East Side area)",Data Warehousing Engineer,Data Engineer
115,Community Tech Alliance,"As a data engineer, you’ll leverage the products we build, along with our data warehouse and ETL pipelines, to empower our progressive organization partners to answer critical questions with data. You’ll work alongside our software engineers to develop pipelines in our ETL platform, build production data materializations in BigQuery, and design solutions to pressing analytics and targeting problems in the progressive space.
About us

Community Tech Alliance is a group of progressive technologists and strategists formed to provide data infrastructure building blocks to the progressive ecosystem at a low cost. CTA seeks to uplevel program impact by unlocking the potential of data, using software and data engineering, and removing the barriers to entry. We are a small team of engineers, data practitioners, product managers, and strategists looking to create infrastructure for progressive change.

Community Tech Alliance believes strongly that:

Inclusive teams are strongest, and supportive work environments take investment, intentionality, and openness
Empathy is the cornerstone of building smart technology solutions
All team members should take ownership of the project and team’s development
Iteration is key, and smart solutions require action not perfection
Nothing great has been built without making mistakes and learning from them
As a Data Engineer, you should:

Have experience working to build new pipelines and integrations using cloud platforms like Google Cloud Platform or Amazon Web Services
Have experience working day-to-day using a popular data warehouse, like Redshift, Vertica, BigQuery, or Snowflake
Bring a security-first mindset to how we build and support products
Act autonomously and thrive in a strong team environment, be excited to work in a fully remote role
Be able to work with, clarify and find creative solutions for ambiguous requirements
We use things like:

Python and SQL in Linux
Apache Airflow, DBT, Airbyte, and other open-source data tooling
Terraform and infrastructure management tools
Google BigQuery, Google Cloud Identity, Google Cloud Storage, and Google Compute Engine
Amazon Redshift
We don’t expect every applicant to know or have worked with every technology we’ve listed, so we urge you to apply if you’re interested and some of the above apply to you! We’re looking for engineers of all levels and hiring for the people over the position above all else.

Physical Demands

The physical demands here represent those that an employee must meet to perform the essential functions of this job successfully. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions:

Ability to sit at a computer monitor for extended periods of time
Salary and Benefits

Salary commensurate with experience in the range of $130,000-$155,000 annually.

We are a remote-first organization, so we provide equipment for your home workspace. We offer a competitive compensation salary and benefits package, including:

Generous healthcare, vision & dental insurance for employees
Matching 401K contributions
Generous paid time off (PTO)
Generous holiday schedule
Paid family leave
Community Tech Alliance provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.


This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.",https://www.indeed.com/rc/clk?jk=d64593b00a1e6f46&fccid=dd616958bd9ddc12&vjs=3,Remote,Data Engineer,Data Engineer
116,Hitachi Vantara,"The Team:At Hitachi Vantara’s Digital Insights practice, we help our clients by building technology solutions that addresses business challenges and improve business outcomes with data-driven insights. As we continue expand our big data team, we are looking for senior data engineers who are passionate about technology and want to build a career working on the latest technology platforms.A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.Position ResponsibilitiesPartner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storageAdvanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plansSkilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumptionExpert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handlingSupport continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review boardDevelop and ensure adherence to published system architectural decisions and development standardsRequirements8+ years of development experienceExpert level in data warehouse design/architecture, dimensional data modeling and ETL process developmentAdvanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)Extremely proficient in Python, PySpark, and JavaAWS Expertise – Kinesis, Glue (Spark), EMR, S3, Lambda, and AthenaStreaming Services – Confluent Kafka and Kinesis (or equivalent)Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)With Japanese Roots Going Back Over 100 Years, Our Culture Is Founded On The Values Of Our Parent Company Expressed As The Hitachi SpiritWe are proud to say we are an equal opportunity employer and welcome all applicants for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.Wa – Harmony, Trust, RespectMakoto – Sincerity, Fairness, Honesty, IntegrityKaitakusha-Seishin – Pioneering Spirit, ChallengeJob Type: Full-timePay: $100,104.00 - $189,378.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insuranceParental leaveProfessional development assistanceReferral programRetirement planTuition reimbursementVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payExperience:Python: 3 years (Required)AWS: 2 years (Required)Scala: 2 years (Required)Work Location: Remote",https://www.indeed.com/company/Hitachi-Vantara/jobs/Data-Engineer-53d198162982cd40?fccid=9536dde6bb34eec9&vjs=3,Remote,Data Engineer,Data Engineer
117,Penn Foster,"Position Summary:
Come be a part of the fast-growing Data Engineering team as we migrate our processes and systems to the cloud. You will play a critical role in this highly visible and strategic project to revolutionize Penn foster’s data capabilities. The Data Engineer is primarily responsible for the development of data engineering processes in an Azure Databricks environment. The ideal candidate will have experience writing Python/PySpark code, SQL, Jira, prioritizing tasks, agile development lifecycles (Kanban and SCRUM), and data analysis.
Essential Job Functions:
As a Data Engineer, you will work on projects that deliver key data and analytics to support Penn Foster’s business and growth. This person will develop dataflows on complex datasets. Your Data Engineer career will challenge you to apply your skills across the Penn Foster enterprise during this critical data transformation project.
Defining requirements
Experience working in a cloud environment (Databricks and Azure preferred)
Developing Python and PySpark based applications
Participating in code reviews
Estimating level of effort for assigned tasks and adhering to schedules
Worked in with Agile development processes
Complete team player
Comfortable working in a fluid constantly changing environment
Strong sense of ownership for all work
Knowledge, Skills, Abilities:
Bachelor's degree in one of the following fields or related studies:
Engineering
Computer Science
Information Systems
Business
Skills and Abilities
5+ years of Data Engineering experience
5+ years of Python and Spark
5+ years of SQL
Cloud work experience, Azure preferred
Databricks exposure is a huge plus
Tableau experience is a plus
Familiarity with Agile and Iterative Development (Kanban preferred)
Experience with large data centric projects and data migration projects
Excellent interpersonal and communication skills (written and verbal)
Ability to work independently and in a group
Self-starter attitude with initiative
Creativity
Ability to solve complex problems
Equal Employment Opportunity:
At Penn Foster we are proud to be an Equal Employment Opportunity employer. We are committed to creating a work environment that embraces and celebrates diversity. We do not discriminate based on race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other status protected under federal, state, or local law.
About Us:
At Penn Foster, we are dedicated to helping over 300,000 students each year achieve their goals through affordable, accessible, career-focused learning. Our mission has remained the same since 1890: to enhance the lives of our students and clients through the acquisition of skills and credentials that can help them work toward their career and life goals. Together with our extensive partner network of leading employers, community-based organizations, and academic institutions, we close skills gaps and are building a workforce that’s prepared for the future job market. We aim to help businesses thrive by mobilizing their individual workers and energizing communities with opportunities for growth and progress. We are proud to play a role in the success of over 80% percent of our graduates that see improvement within their careers, as they inspire us to keep finding new ways to further our reach and broaden horizons. Join the Penn Foster movement and start working toward a better future today.
What We Offer:
We offer a competitive base salary, plus a robust benefits package that includes medical, dental, vision, flexible spending, generous paid time off, sponsored volunteer opportunities, parking & commuter benefits, a 401K with a company match, plus free access to all of our online programs.
5LgPlFlHp7",https://www.indeed.com/rc/clk?jk=0a7f9c772ee16d5b&fccid=bedaf60f8d249974&vjs=3,Remote,Data Engineer,Data Engineer
118,ACV Auctions,"ACV’s mission is to build and enable the most trusted and efficient digital marketplaces for buying and selling used vehicles with transparency and comprehensive data that was previously unimaginable. We are powered by a combination of the world’s best people and the industry’s best technology. At ACV, we are driven by an entrepreneurial spirit and rewarded with a work environment that enables each teammate to impact the company from day one. ACV’s network of brands includes ACV Auctions, ACV Transportation, MAX Digital and ACV Capital within its Marketplace Products as well as True360 and Data Services.

What you will do:
ACV Auctions is looking for a Lead SecOps Engineer, Data Security and Privacy. The Data Security and Privacy SecOps Engineer is someone who is passionate about building and managing Security Infrastructure and Business Practices and enhancement that drive effective data risk management and reduction. In this role you will be responsible for creating a model of Security for the cloud resources that supports the ACV Platform. This includes the AWS and GCP along with nodes that host K8 clusters and other third party partners.

We are building a layered Security approach which means the SecOps Engineer will need to work hand in hand with teams such as Infrastructure, AppSec, Detection and Response, Development Teams and compliance to ensure the flow from Applications to APIs to Cloud Resources are secured. In lieu of layering Security controls the person in this role will be working to enhance and strengthen the Security Controls within our environment as a whole, such as: anti-phishing gateways, EDR, AV, firewalls, IDS/IPS systems, AWS Security Hub. Further this position is not only about growing ACV's capabilities but our associates as well, it will be important to be able to work with various teams such as Dev, HelpDesk, HR, Legal etc guiding Security recommendations for the program.

Responsibilities:

Formalize the Data Security and Privacy Program including: data mapping, data identification, data security standards and data security practices and processes.
Drive the technical practices and implementation of securing data across technical systems and infrastructure.
Develop, implement and manage security standards, plans/roadmaps and operational processes to secure the AWS platform and resources such as RDS, EC2, S3, etc.
Manage Security Alerts and provide Incident Response support services, it's not expected someone knows everything but this person should be able to identify and perform triage to resolve a Security Incident.
Able to deploy and manage infrastructure and applications via code, CICD pipeline and K8.
Contribute to the development, improvement and operational management of Security Operations, Monitoring and Incident Response practices, processes and solutions.
Able to work with vendors and manage PoC's.
Overall understanding of Security Domains, Compliance Requirements, and Risk Management Practices.

What you will need:

Excellent communication, interpersonal and leadership skills, with the ability to interact with staff at all levels.
Knowledge of CASB, DLP and SASE technologies
Proven ability to be agile and work effectively in a dynamic environment.
Demonstrated ability to perform under pressure and respond rapidly to emerging incidents and situations.
Excellent coordination, project management, and organization skills and comfortable with multi-tasking in a high-energy environment.
Should be a creative and analytical problem solver with a passion to provide excellent customer service.
Practical hands-on experience engineering and implementing data security controls in cloud environments including databases, datastores and SaaS platforms.
Linux and Kubernetes/Container management and security
DevOps code based implementation and management
Knowledge of AWS including but not limited to S3, Lambda, RDS, EC2 and AWS Security Center
Understanding of TCP/IP Networking including knowledge of Protocols and Services
Understanding of what Information or Assets are of value to Threat Actors and how Organizations are Breached and Customer Accounts Compromised.
Overall understanding of the Security domain, compliance, business, risk, ops etc ALONG with its application to the business.

What we offer:

Comprehensive benefits offerings for benefits eligible Teammates.
Unique culture that truly values each and every Teammate.
Career development and Future Growth Opportunities.

At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.",https://www.indeed.com/rc/clk?jk=f0b0246d9d85ba51&fccid=6867c53ddcaa8459&vjs=3,"Caribou, ME 04736 04736+21 locations","Senior Data Security Engineer, SecOps",Data Engineer
119,"Full Circle Financial Services, LLC","Please Note: This is an onsite position. Position will be required to work onsite M-F | 8-5 PM. This position is NOT remote.Full Circle Financial Services provides complete and compliant Debt Recovery Services custom tailored to each client. Our office is based in Tampa, FL.Description: We are currently searching for a * Data Engineer *whose role would be to work in a small team environment to develop internal data automation and business process solutions inside our Azure environment of VMs, Database Servers, SaaS tools and third-party Vendor and B2B (Business to Business) data. The position is responsible for delivering value products to enhance internal process improvement/automation and external custom solutions to meet our client’s needs.Duties and Responsibilities: Work as a member of a small development team reporting to IT Leadership and working closely with our Business Analytics, Operations, Accounting and Legal departments.Work with stakeholders including the Senior Leadership, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater effectiveness to solve problems and address customer requirementsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Microsoft Azure technologies.Develop, implement, document & optimize data integration, extraction and storage using Azure VM database servers and Azure micro services.Desired Skills & Experience: CRM Systems and MS Azure ServicesSQL, T-SQL Stored Procs, MS SQL Server, SFTP/FTP and REST APIs, SSHPowerBI, SQL Server Reporting Services, Tableau, ExcelSnowflake Cloud Data Platform and dbt (Data Build Tools)Data Modeling concepts such as Dimensional, Star Schema, Data VaultApplication design, functional design, technical characteristics of relational databases and client server systemsStrong oral communication and technical documentation writing.Ability to prioritize in a fast-paced environment (Task Management)Ability to work with Network and Security support to analyze and resolve problemsFinancial Services experience preferred (not required)Compensation & Benefits: Competitive salary and benefits are available. Some of our benefits include 401K, vacation, paid holidays, health, dental, and vision plans, as well as short term, long term, and life insurance plan options.Job Type: Full-timePay: $80,000.00 - $100,000.00 per yearJob Type: Full-timePay: $80,000.00 - $100,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insuranceLife insurancePaid time offVision insuranceSchedule:Monday to FridayAbility to commute/relocate:Tampa, FL 33626: Reliably commute or planning to relocate before starting work (Required)Work Location: One location",https://www.indeed.com/company/Full-Circle-Financial-Services/jobs/Data-Engineer-ebd123a6088d4995?fccid=942ba13ac5636635&vjs=3,"Tampa, FL 33626 33626",Data Engineer,Data Engineer
120,FlexIT Inc,"FlexIT client is looking for an immediate Data Engineer - Python, Spark for a 12-month remote contract.
The client is looking for great Engineers with talent and persistence who can leverage their existing skills and learn new ones. You should have some of the specific technical skills we’re looking for and be expert enough in one or two to help ramp others quickly.
Job Duties:
We are building petabyte-class solutions that consume fast-moving streams from eCommerce, retail, and partner channels and power the critical decisions that drive our business. We are building the Cloud Platform for Data and Analytics on AWS that fuels in digital transformation.
Focus areas include:
Data Streaming / Enrichment / Business Rules / MDM
Data Lake / Warehousing
Data Governance / GDPR / SOX
Data Strategy / Unified Access / IAM / RBAC
Be a great teammate on an agile/SCRUM team that sets and meets aggressive goals.
Mentor new and less experienced developers to advance their proficiency.
Leverage expert development skills and solid design skills to deliver reliable, scalable, performant solutions with modern tooling, data structures and algorithms.
Work with Product Owners, Engineering Managers and Principal Engineers to deliver solutions that enable digital transformation",https://www.indeed.com/rc/clk?jk=d03875672dba3744&fccid=2f3a12745010baf7&vjs=3,"Beaverton, OR 97005 97005 (Central Beaverton area)+10 locations","Data Engineer - Python, Spark?",Data Engineer
121,Catalytic Data Science,"Data Engineer

Engineering

 REMOTE OPPORTUNITY


About Catalytic Data Science (CDS):

Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.
The Role:

You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.


What You’ll Do:


Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform


What You Know:


Must Haves:


Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices


 Nice-to-Haves:


Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases


What do we love in team members?


Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:


Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team


 In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.",https://www.indeed.com/rc/clk?jk=74bcdd87692316bb&fccid=876854e9c7d69cba&vjs=3,"Austin, TX 78746 78746",Data Engineer,Data Engineer
122,Valley National Bank,"Data visualization is the art that delivers information in a variety of formats in the ways that make the most sense for each stakeholder. This role will work closely with your Product Manager, various Business teams and your own scrum teams to understand end user needs; designing data visualization solutions and forms that will create high value. You and the team will work with continual feedback cycles; analyzing product usage, end user behaviors and systems performance information as input into the continual evolution of your products; delivering continual and incremental value to the end user.
Responsibilities include, but are not limited to:
Data Visualization responsibilities:
Work alongside engineering, operations and product teams to help them gain deep understanding of current state processes and patterns.
Evangelize data visualization as high value content.
Deliver data visualization solution that teams can leverage to deliver outcomes from initial MVP as well as insights that can be used for subsequent iterative cycles of development.
Promote an evolutionary data visualization approach that allows designs to evolve as new information and forms of consumption becomes available.
Ensure teams have the practical and critical insights throughout the CI/CD development and product cycles.
Balance short term delivery needs with longer term aspirations to ensure that data is delivered with increasing value.
Work across teams to support the creation and delivery of non-functional requirements, including but not limited to performance thresholds and security insights.
Engage as a member ofthe engineering community of practice and supporting fellow technologists.
Work with teams to delivery quick insights through prototyping and spike development.
Work with teams to clearly define acceptance criteria and then deliver insights of business, technical and operational significance.
Present underlying data in a variety of forms from graphical to tabular and present raw data in graphical, tabular and three-dimensional forms using drill down navigation where appropriate.
Present data in forms that can be consumed by other tools according to the needs of each stakeholder.
Build the dashboards and delivering critical insights that that stakeholders from senior leadership, business, product, operations, security and engineering will use as guides in management and control.
Team membership responsibilities:
Evangelize the delivery of quickly digestible data.
Ensure your team members clearly understand the art of the possible, the practical balanced by delivery of immediately needs.
Champion knowledge sharing.
Requirements:
Required Skills:
A deep understanding of data reporting and visualization trends.
An ability to work closely with software engineers to deliver key DevOps insights.
An ability to work with non-technical people to deliver to their needs.
Ability to work within a team, to enjoy being challenged, and to enjoy challenging others.
Experience delivering critical insights to leadership and technical stakeholders Azure DevOps tools
Experience across a variety of data types and sources including graph database
Excellent understanding of PaaS (ideally Azure) data reporting tools and how they can be best utilized to increase velocity of delivery and consumption
Experience with Power BI, Azure Analysis Services, Azure Synapse Analytics and Azure Monitor visualization
Understanding of integration patterns and how best to give metrics and patterns
Experience with Python and Kusto (nice to have)
Experience with SQL, NoSQL, other data sources
Experience with DevOps practices and reporting needs
Experience with data delivery satisfying regulatory and high security demands
Required Experience:
HS Diploma/GED and a minimum of 5 years data visualization experience designing solutions for end users.
Bachelor's Degree in Computer Science, IT, Data or related technology discipline preferred.
Prior experience within the Banking sector and/or prior knowledge and experience in a Financial Service industry in any of the following business areas: Retail Bank, Commercial Bank, Payments or Insurance is preferred.",https://www.indeed.com/rc/clk?jk=3ab633b9658f5049&fccid=90e55ca4ad1bd093&vjs=3,"Wayne, NJ 07470 07470",Data Visualization Engineer,Data Engineer
123,"Ascella Technologies, Inc.","We are currently seeking an experienced Senior Data Engineer to design and build complex XBRL data processing logic using MS SQL Server and related software packages. We’re looking for a senior member to join the team who can architect, build, and debug complex scripts in MS SQL Server.What you will be doingEnhancing and maintaining a production MS SQL Server system that processes and stores large amounts of data extracted from XBRL financial reportsIntegrating data into an Apache Solr platform that provides search results on large amounts of financial data for different end users and applicationsDefining the technical vision, engineering rigor, and architectural best practices to enhance data processing of XBRL dataManaging current data processing job and serve as data lead for the development teamWhat you need to be considered for this role (required)Bachelor’s degree in Computer Science, Engineering, or related field from an accredited college/university5+ years of experience with MS SQL Server as a Database Developer/Engineer3+ years of experience with Apache SolrExemplary written and verbal communication skills to work with clients and partnerAbility to effectively and efficiently multi-task, prioritize, and carry out projects through to completion with minimal supervisionAbility to maintain a high level of accuracy and attention to detailU.S. Citizenship (and willingness to undergo a background investigation)What makes you stand out from the crowd (desired)Prior experience with and/or knowledge of Python strongly preferredPrior experience working with financial data or financial management systemsPrior experience with and/or knowledge of XBRLAbout UsAscella Technologies develops and implements innovative technology solutions to deliver business outcomes and help clients manage risk. Our senior consultants and software engineers bring extensive experience in our clients’ specialized business domains, applications, and technologies. With this experience, Ascella has helped Federal, State, and local government agencies achieve success in their most critical missions. Founded in 2002, Ascella is continuously growing and looking for team members that can help us continue to deliver value-added technology-based solutions.Ascella’s continued success is a reflection of our passion for people. We hire individuals with valuable experiences, and we strive to place people in roles that let them fully utilized their talents and flourish. We also maintain an entrepreneurial culture in which each person’s contribution makes a visible impact on our company and is recognized accordingly.What makes Ascella employees greatInnovative Thinkers: From small suggestions to large goals, employee voices are heard and acknowledged. We encourage employees to come up with creative ideas and give everyone the opportunity to help us define how we do things. You will have the opportunity to solve real business problems and be challenged to come up with innovative solutions.Lifelong learners:  We value lifelong learners who are excited about professional development and personal growth. At Ascella, employees are part of a company and a culture that encourages continuous improvement of both the organization and our employees. You will work in a collaborative environment with an experienced, highly capable team that learns from each other, and you will be both challenged and supported in your efforts to learn and grow.Customer-Centric:  Ascella is committed to complete customer satisfaction on every project. Our employees provide our customers with excellent service, proactively coming up with solutions to current and anticipated needs. The customer experience is an integral part of our mission and values, and you will work alongside people who are focused and dedicated to the commitment of customer satisfaction.Open Communicators:  Transparency and communication are important to us at Ascella. We have an “open door” policy, and our employees ask questions and share information on a regular basis. We believe in aligning employee work with overall company objectives and being open about our strategic plan and what we are trying to achieve. We encourage regular feedback and appreciate employee input on what’s working and what’s not so we can initiate discussions about how to improve our processes.Inclusive:  At Ascella, we encourage and embrace diversity in our workforce. We are committed to being an inclusive organization where all employees feel valued, respected, and engaged. Our employees have an array of talents, ideas, and experiences that help foster creativity and innovation and drive our success.Job Type: Full-timePay: $110,000.00 - $140,000.00 per yearBenefits:401(k)Dental insuranceFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offProfessional development assistanceTuition reimbursementVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Required)Experience:Database Developer/Engineer: 5 years (Required)SQL: 2 years (Preferred)Apache Solr: 3 years (Preferred)License/Certification:US Citizenship (Required)Work Location: Remote",https://www.indeed.com/company/Ascella-Technologies/jobs/Senior-Data-Engineer-558acb9e00fafcf3?fccid=dbac6bc501867d3c&vjs=3,+2 locationsRemote,Senior Data Engineer - Fully REMOTE position,Data Engineer
124,Connecticut RISE Network,"Reports to: Deputy Director of Data Systems
Salary: Competitive and commensurate with experience
Location: New Haven, CT
Overview:
The Connecticut RISE Network (RISE) is a Connecticut-based nonprofit organization whose mission is to ensure all RISE high school students graduate with a plan and the skills and confidence to achieve college and career success. Founded in 2016, RISE partners with Connecticut public high schools to lead statewide networks where school communities work together to use data to learn and improve. We partner with school communities across the state to understand complex challenges, unpack data, pilot new innovations, and scale effective practices.
Today, the RISE Network represents a collaboration between nine high schools and eight public school districts that form our core partnership network, reaching over 13,000 students; the majority of RISE students identify as Black, Latinx, and/or low-income. RISE high schools work together to ensure all students experience success as they transition to, through, and beyond high school.ng with a strong foundation in Grade 9, RISE high schools pursue school-wide improvements in Grades 9 through 12 that support college and career readiness, access, and success.
Position Summary and Essential Job Functions:
The RISE Junior Data Engineer will play an integral role supporting the Network’s research and analytic functions. RISE is seeking a talented and highly motivated individual to serve on a team collaborating with schools and districts to support data tools, systems, and practices to improve student outcomes. This is an extraordinary opportunity for an individual who thrives in an entrepreneurial environment and is passionate about closing achievement gaps and developing the potential of all students regardless of life circumstance.
Responsibilities include, but are not limited to:
 Database Management:
Facilitate the integration of new school variables into existing data warehouses and data tools
Support the ETL (extract, transform, load) data pipeline
Create data sets from raw files, bringing together longitudinal and immediate data for a variety of indicators
Uphold the highest data security standards
 Data Systems and Tools:
Monitor RISE district data needs and challenges, proactively looking for ways to enhance data-driven decision-making and workflow
Collaborate on the design, development, and ongoing refinement/maintenance of new data products and tools to address district data needs
Maintain existing data tools through rigorous, ongoing, quality assurance
Transform educator feedback into improvements and enhancements for existing online dashboards
 Data Practices:
Use data tools and dashboards to facilitate deeper understandings of student performance and serve as a support for school and district data team meetings
Provide support to RISE leaders around data systems and practices
Join cross-functional teams to effectively launch new data tools and complementary resources, promoting educator buy-in, understanding, fluency, enthusiasm, and high usage rates
Develop and maintain a “Help Desk” process for users of RISE data tools
Required Knowledge and Skills:
Demonstrated proficiency with spreadsheets and Google apps
Strong sense of accountability and responsibility for results
Outstanding interpersonal skills and teamwork
Ability to interact professionally and earn credibility with a diverse range of stakeholders
Commitment to systems-driven solutions for scalable and sustained impact
Strong organizational and project management skills
Strong belief that all students can learn and achieve at high levels
Ability to ramp up on new content and work in an independent and self-motivated manner
Desire to work in a dynamic, entrepreneurial, and results-driven environment
Ability to conduct analyses and present results in a variety of written and visual formats
Sense of humor, positive attitude, and willingness to work in a collaborative environment
Knowledge of SQL, R, and/or Python required
Preferred Knowledge and Skills;
Experience with version control systems (preferably Git)
Required Education and Experience:
Bachelor’s degree in a relevant field required
Minimum of one year of relevant experience
RISE is an Equal Opportunity Employer.",https://www.indeed.com/rc/clk?jk=8d1a615fc2d1a83e&fccid=8c3d7f18f62a6ce9&vjs=3,"New Haven, CT",Junior Data Engineer,Data Engineer
125,Emonics LLC,"Desired Candidate ProfileExperience - 0 to 14 YearsBig data, Hadoop, Hive, Spark, Kafka, MySQL, DatabricksKnowledge on AI/ML, online advertising system (e.g. ocpx, ctr, cvr)Roles and ResponsibilitiesStrong hands-on experience in Big data Analysis Techniques and Statistical models and various data analysis toolsStrong experience in applied statistics skills, such as distributions, statistical testing, regression, etc. Mathematical background in linear algebra.Excellent understanding of Hadoop.Experience with DatabricksExcellent scripting and programming skill preferably in Python 3Experience building data pipelines for batch and stream processing systems.Experience with Spark, various messaging systems, such as Kafka or RabbitMQExperience with SQL and NoSQL databases such as HBase, Cassandra or MongoDBExperience with classification techniquesExperience with BigData ML toolkits like Mahout, Spark MLGood knowledge of Bid data querying tools like Pig, Hive etc.Proficiency with Hive-QL & able to Analyze, Develop and Debug the Hive Scripts on his own.Candidate should have Data Processing ability (ETL techniques) using hive scripting experience.Candidate MUST NOT be limited to Data Migration capability from legacy DB to Hadoop ClusterProficient with Partitioning, Analytical aggregation and dealing with large tables.Proficiency Big data architecture, Spark & Hiva.Job Types: Full-time, ContractPay: $66,762.00 - $120,288.00 per yearBenefits:Health insuranceSchedule:8 hour shiftExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: Multiple Locations",https://www.indeed.com/company/Emonics-LLC/jobs/Data-Engineer-22a8acd0cbb5ff07?fccid=5275edd1f3f2b774&vjs=3,"Newark, NJ",Data Engineer,Data Engineer
126,DGN Technologies Inc,"Position : Business Intelligence Consultant - ISIJP00004985Client : Intuitive SurgicalLocation : Santa Clara, CADuration: Contract to Hire OverviewBusiness Intelligence Analyst : Analyzes complex business Supply Chain/ Planning problems and issues using data from internal and external sources to provide insight to decision-makers. Identifies and interprets trends and patterns in datasets to locate influences. Works with Business SMEs to understand and groom Business requirements , convert them to Functional / reporting specifications , write sql for defining metrics , definitions and business rules and communicate and work along with to the dev team to create data models . Creates specifications for reports and analysis based on business needs and required or available data elements. May provide consultation to users and lead cross-functional teams to complete , requirements gathering , UAT and address analytics issues . May directly produce datasets and reports for analysis/ prototyping using system reporting tools.· As a Sr. Analyst on the Supply Chain Planning Analytics team , building data products to augment decisions in Business process, you will :· Use data to inform data product roadmap.· Source to Target mappings / Functional Spec creation, documentation and communicate with Development teams· Test Data setup and UAT coordination with IT and Business· Expected results - document and review with business· Follow-up on IT application gaps / issues· Partner with business unit stakeholders to define metrics, visualizations· Own, coordinate, and solve complex, cross-functional data quality problems· Generate SQL queries to feed into the Functional spec for the dev teams· Develop visualization prototypes for complex dashboards, review with all stakeholders· Provide specs and lead data engineers and ETL developers to align data model with analytic needs· Perform Data analysis to provide insights into business process challenges and opportunities· Lead Business conversationsExperience· 5+ years of hands on experience with data, analytics projects· 5+ years of hands on SQL for data analysis and familiarity with Tableau / other reporting tools· 5+ years of BW/ SAP Functional other SAP tools experience. SAP environment familiarity (Functional or ABAP or BW) is a must for this role. Supply chain/ planning experience is highly valuable .· 5+ years of working directly with business users to develop metrics/KPI definitions as well as resolve data quality issues using SAP/ or other ERP data .Bonus points· Ability to use Tableau for data analysis , create dashboards for prototypes is highly desired.· Familiarity with SAP BW, ABAP, other SAP BI tools is a big plus· Ability to communicate with non technical user community as well as technical project team members· Should be able to create and own documents driving deliverables . Should be able to create and maintain test documents.· Independent, organized, flexible and proactive in responding to changes in business needs· Experience preparing data for data miningJob Types: Full-time, ContractPay: $80.00 - $85.00 per hourSchedule:8 hour shiftExperience:Informatica: 5 years (Preferred)SQL: 5 years (Preferred)Data warehouse: 5 years (Preferred)SAP BW 4HANA: 4 years (Preferred)Work Location: One location",https://www.indeed.com/company/DGN-Technologies-Inc/jobs/Senior-Data-Engineer-974e70bd355f8980?fccid=4bfff1ffdcbbf539&vjs=3,"Sunnyvale, CA (East Murphy area)",Senior Data Engineer,Data Engineer
127,OS National,"Are you intrigued by the thought of revolutionizing a trillion-dollar industry? Do you want to be part of a cutting-edge company that is customer-obsessed? A company growing at lightning speed that is full of career growth opportunities. If so, we are seeking smart, customer-focused, and energetic professionals to join our team to help us to the next level of excellence!Who is OS National LLC.?OS National LLC is a Title and Escrow company that focuses on providing boutique service on a national platform. Our delivery solutions include a full complement of origination and default services directed at leading national mortgage originators and servicers. We offer a centralized, single point of contact which creates efficiency throughout our process and reduces operating costs for everyone involved. We are part of the Opendoor family, revolutionizing the home buying experience.OS National is a place where you can begin your career AND expand your professional development. We appreciate different points of view and understand that appreciation leads to innovation. We want to partner with you to grow your career because we understand that our business benefits when your career expands and blossoms.WE BELIEVE IN:  Kindness. Appreciation. Growth. Respect.The Data Engineer position is a hands-on role that will be part of a team that is responsible for all of our databases, reporting, and integrations utilizing various API’s. This work includes installing and maintaining software, managing our SQL managed instances in Azure, and building out reporting using SSRS, PowerBI, and other various tools. You will be tasked with the overall health and performance for multiple environments including production, staging, testing, and training. You will be the subject matter expert for troubleshooting issues relating to OSN’s databases and provide assistance to the support staff as needed.What You Get to Do:Work with our Business Application (ResWare) team to continuously improve performance, stability, and user experience with the application.Create reports per business need using SSRS.Create live dashboards to meet the business need using PowerBI.Monitor, maintain, and secure all databases.Perform system upgrades.Display the characteristics of a high-quality service organization: reliable; responsive; competent; accessible; professional; credible.Deliver timely communication; understands the need of our users; and provides tangible results from our service activities.Maintains and demonstrates strong technical expertise in key technologies.Demonstrates initiative in recommending enhancements and improvements to our databases and integrations.Responsible for providing high-quality, consistent service levels to our employees and customers. Partner with IT and business personnel to discuss the impact of incidents on products and services.Encourages and builds positive relationships and communicates effectively with all co-workers and vendors.Requirements:Bachelor's degree in Computer Science or combination of related experience and certifications.Expert knowledge of SQLStrong knowledge of SSRS, PowerBI, or other reporting toolsStrong knowledge of API’s and system integrationsStrong teamwork and communication skillsFamiliar with Azure SQL Managed InstancesExperience implementing applicationsResWare experience a plusMust possess advanced level analytical, communication and technology skillsExperienced in documentation and change managementMust be a self-motivated individual with a willingness to work well with others on a regular basisWhy choose OS National?OS National offers its employees a comprehensive suite of benefits that include:- 100% company-paid employee-only health benefits- Competitive compensation- Benefits effective on 1st day of employment- Work-from-home stipend- Volunteer time off and much more!OS National is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender identity, age, pregnancy, genetic information, citizenship status, or any other characteristic protected by law. APPLY FOR THIS JOBJob Type: Full-timePay: From $1.00 per hourBenefits:401(k)Dental insuranceEmployee assistance programEmployee discountHealth insuranceLife insurancePaid time offReferral programVision insuranceSchedule:Monday to FridayApplication Question(s):We are only eligible to hire in the following states: AZ, CA, CO, FL, GA, ID, IN, NC, MI, MN, MO, NC, NV, OH, OR, PA, SC, TN, TX, VA, OR, WV. Do you reside in one of these states?Experience:SQL: 3 years (Preferred)Work Location: Remote",https://www.indeed.com/company/OS-National/jobs/Data-Engineer-f75d9d81cd8d4871?fccid=0fddf6c11e8f5bd5&vjs=3,Remote,Data Engineer,Data Engineer
128,"Corporate Systems Associates, Inc.","JOB ID: T4825 - Data Engineer Level II - Data Catalog/ Lineage/ Meta DataPLEASE NOTE: This is a a contract-to-hire opening and needs to meet Client full-time conversion policies. Those dependent on a work permit sponsor now or anytime in the future (ie H1B, OPT, CPT, etc) do not meet Client requirements for this opening.Must be Skilled in: Data Catalog/ Lineage/ Meta Data Data Engineering team contextualize and provide easy access to data for the entire enterprise. As a Data Engineer Data Catalog, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to design, build and deploy data solutions that capture explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights.Primary Job Duties & Responsibilities· Incorporate core data management competencies including data governance, data security and data quality.· Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.· Perform data and system analysis, assessment and resolution for complex defects and incidents and correct as appropriate.· Test data movement, transformation code, and data components.· Stay up to date on all features and functions of software tools.· Work, update, escalate, and resolve trouble tickets submitted by end users.· Work with end users to identify root cause of issues, resolve and train on proper procedure.· Work with development team to gain expert level understanding of the software and extended features.· Develop end user documentation on software functions, procedures, tasks, and troubleshooting.· Train new hires on the use and function of both the software and endpoint device.· Assist in data reconciliation efforts.· Perform other duties as assigned.Minimum Qualifications· Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.· Four years of data engineering or equivalent experience.Education, Work Experience, & Knowledge· Bachelor’s Degree in STEM related field or equivalent· Highly proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and a full understanding of modern software engineering practices.· Prior experience with data catalog tools including experience with data lineage, meta data management.Job Specific Technical Skills & Competencies· The ability to deliver work at a steady, predictable pace to achieve commitments, deliver complete solutions but release them in small batches, and identify and negotiate important tradeoffs.· Strong problem solver who ensures systems are built with longevity and creates innovate ways to resolve issues.· Strong written and verbal communication skills with the ability to work collaborate well with team members and business partners.Job Types: Full-time, ContractPay: $50.00 - $70.00 per hourBenefits:401(k)Health insuranceSchedule:8 hour shiftMonday to FridayApplication Question(s):Are you independent of a VISA Sponsor for employment ? (i.e. have your GC-EAD, Green Card, or U.S Citizenship) (*Deal Breaker)Please provide your base salary expectations and if relocation is an option for you. (*Deal Breaker)What is your hourly pay rate you are looking for? (*Deal Breaker)Do you work on W-2 or Corp. to Corp.? (*Deal Breaker)Experience:Data Catalog/ Lineage/ Meta Data: 4 years (Required)Work Location: Remote","https://www.indeed.com/company/Corporate-Systems-Associates,-Inc./jobs/Data-Engineer-Level-49f8d90926373b5a?fccid=ec708f8f577e0931&vjs=3",Remote,Data Engineer Level II - Data Catalog/ Lineage/ Meta Data,Data Engineer
129,"Dynamic Placement Services, LLC","Dynamic’s client is seeking a Data Engineer for a permanent position that’s 100% remote. The candidate will be part of a collaborative 5-person analytics team.NOTE: Candidates must reside in the U.S. and be authorized to work in the U.S. without visa sponsorship. No C2C.Qualifications/Duties:· 5+ years of data analytics and reporting· Data migration· Data warehouse· Build Tableau or Power BI dashboards· Build data pipelines using Azure Databricks· Streamsets· Spark· Pyspark· Azure Data FactoryJob Type: Full-timePay: Up to $120,000.00 per yearSchedule:Monday to FridayApplication Question(s):What is your desired salary?Experience:Azure Databricks: 1 year (Preferred)Work Location: Remote","https://www.indeed.com/company/Dynamic-Placement-Services,-LLC/jobs/Data-Engineer-073a4e2ed84cad43?fccid=b4b5ef3a7571bb88&vjs=3",Remote,Data Engineer (Azure Databricks),Data Engineer
130,"Insight Enterprises, Inc.","Assessment Engineer – Data Protection and StorageWe areInsight Enterprises (NASDAQ: NSIT)*NOT a Data Science Job*NOT looking for candidates right out of school. Must have min of 3 years working experience with Storage architecture and Data ProtectionAs a Fortune 500-ranked global provider of digital innovation, cloud/data center transformation, connected workforce, and supply chain optimization solutions and services, we help clients successfully manage their IT today while transforming for tomorrow.From IT strategy and design to implementation and management, our employees help clients innovate and optimize their operations to run smarter.Microsoft Worldwide AI Partner of the Year, 2018Microsoft Worldwide Modern Desktop Partner of the Year, 2018Microsoft US Partner Award for Intelligent Cloud - Application Innovation, 2019Microsoft US Azure Team Partner Choice Award - Data and Artificial Intelligence, 2019Our Insight Cloud+Data Center Transformation team is searching for an experienced, passionate and professional Assessment Engineer – Data Protection and Storage to join our team.At the heart of every great technology solution is great management, always making sure that the project delivers what it set out to do and matches expectations.Assessment Engineer – Data Protection and StorageAs an Assessment Engineer for Data Protection and Storage at Insight, you will work in a dynamic and leading-edge environment delivering client focused Data Protection (Commvault, Rubrik, Veeam, and Veritas) and Storage (NetApp, Pure, Dell, HPE) Assessments along with other technologies like converged infrastructure solutions. In this role, you will work collaboratively with other Insight teammates (sales, architects, engineers, and partner managers) to qualify assessment nominations including determining scope, level of effort, client outcomes and writing the Statement of Work for delivery of the assessment using Insight templates. The Assessment Engineer will work with the field sales, architect, and delivery engineer SMEs to deliver the assessments which involve a project kickoff, data collection, data analysis, documentation, and findings presentation to the client.What you’ll do at InsightProvide client assessments for:Data Protection solutions consisting of Commvault, Rubrik, Veeam, Veritas NetBackup, Veritas BackupExec, and other backup solutionsStorage and Converged Infrastructure: NetApp, Pure, Dell, Nutanix, HPE, and othersVirtualized Environments: VMware, Hyper-V, etc.Use of public, private or hybrid cloud architecturesApply systems analysis techniques and procedures, including consulting with clients, to determine requirements, operational challenges, outcomes, budget and timeframe.Maintains data collection repositories and recommends new solutions and services including technology upgrades, refreshes, re-platform, and expansion into public cloud (AWS, Azure, GCP)Executes data collection of client environment current state using vendor provided tools, Mitrend, and APTARE to name a few.Document current state, future state identifying risks, health, roadmaps, and recommendations based on quantitative and qualitative data analysis.Presents assessment deliverables to client in partnership with Insight teammates.What you’ll need to join InsightBachelor’s degree in Engineering, Computer Science, Information Technology, or a related field; or equivalent years of working experience.At least 5 years of experience working with information systems technology appropriate to the role, such as data protection, storage, converged infrastructure, virtualization, and/or servers and applications.Expert skills with Microsoft Excel & PowerPointExperience with Structured Query Language (SQL) query development for reporting, and data visualizationExperience with Veritas APTARE and SQL Template Designer is a plusStrong experience in managing and supporting multiple data protection and storage systems in a highly virtualized, and hybrid cloud environments.Growth in technical skills through relevant certifications or other similar achievements is desired.Knowledge of systems analysis techniques and proceduresMust have the ability to keep current with alternative solutions and technology and make recommendations to clients consistent with Insight’s business and system strategies.Ability to effectively communicate, present and articulate strategies across various audiences; clients, sales, marketing, partners, consultants, and executive leaders (internal and external)Eagerness to learn new tools and technologies, and passion to deliver quality deliverables both individually and as part of a team.Accuracy and attention to detail, must have organizational skillsHighlights of the 2021 Insight Corporate Citizenship Report include:In the last year, Insight has been named as a Forbes World’s Best Employer, a Forbes Best Employer for Veterans, a Fortune World’s Most Admired Company and a Fortune Best Workplace for Diversity. Insight also received a score of 95 out of 100 on the Human Rights Campaign Foundation’s 2021 Corporate Equality Index for LGBTQ workplace equality.Insight Digital Innovation launched a Detection and Prevention solution built on the company’s AI and IoT-powered Insight Connected Platform™, providing a scalable foundation to help organizations detect for common signs of COVID-19 and quickly curtail its spread in high-traffic areas.The Community Wireless Broadband solution, designed by Insight’s Cloud + Data Center Transformation team and provided through Insight Public Sector, brings much-needed broadband connectivity to teleworkers and remote students in underserved communities who lack reliable Internet connections.Celebrating diversity and inclusion, Insight held its first Global Harmony Day in recognition of Global Diversity Awareness Month in October 2020. The day united Insight teammates worldwide for a series of candid discussions on diversity, equality and inclusion.Insight’s teammate resource groups, promoting allyship for cultures and communities that have traditionally faced challenges in the workplace, expanded to five groups with the addition of the Afro-Professionals and Allies at Insight group. These teammate-led groups now have more than 1,200 active members.As part of the Insight Reach program, dedicated to empowering children’s lives through technology, the company’s 10 th annual Noble Cause campaign raised $250,000 for charities like the Ronald McDonald House, Make-A-Wish Foundation and the Boys & Girls Club of Arizona. More than $1 million has been donated in the last six years.In It Together Foundation, Insight’s 501(c)(3) charitable non-profit program helping teammates in crisis through teammate contributions and matching company funds, has raised more than $2.15 million since its inception in 2014. Since January 2020, 170 teammates have received support, and Insight’s executive team contributed $250,000 as part of its COVID-19 response to help families hit particularly hard by the pandemic.Environmental sustainability initiatives saved Insight more than $7 million in 2020.As an affiliate member of the Responsible Business Alliance, Insight’s asset disposition program – helping companies securely repurpose old devices and equipment – and diverse supplier program support a commitment to sustainability and equality.Job Type: Full-timePay: $120,000.00 - $160,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programEmployee discountFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveProfessional development assistanceReferral programRetirement planTuition reimbursementVision insuranceSchedule:8 hour shiftSupplemental Pay:Bonus payCOVID-19 considerations:Work is remote from comfort of your own homeExperience:Storage Technology: 3 years (Required)SQL Knowledge: 1 year (Required)Solutions Assessments: 3 years (Required)Work Location: Remote",https://www.indeed.com/company/Insight/jobs/Assessment-Engineer-d7765a37217bf199?fccid=69247daab4a84a92&vjs=3,Remote,Assessment Engineer (Storage and Data Protection),Data Engineer
131,ADT Security Services,"Data Engineer Position Summary: As a Data Engineer in Customer Experience Operations, you will be responsible for designing and building data pipelines for business intelligence and advanced analytic activities such as dashboard creation and machine learning. You will help transform how we support the customer experience operation organization by elevating the use of data to drive more value. You will be responsible for establishing pipeline design and data integration frameworks that bridge current state activities with target opportunities. This role involves coding, designing, and implementing data solutions. You will work directly with senior stakeholders and decision makers. The ideal candidate will have working knowledge of SQL Server services, Google Cloud Platform, Python, data literacy practices, API integration and agile methodologies. In this role you will also trouble shoot data quality issues and coordinate break-fix activities across technology departments.Responsibilities: Responsible for building end to end data pipelines that provide raw, cleansed and transformed layers for data consumption.Responsible for data quality and ensuring data is as clean as possible for consumption activities.Design best practices for data processing, data modeling and warehouse development.Develop data strategies and designs to provide proactive solutions and enable stakeholders to extract insights and value from data.Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.Create rapid prototypes to test concepts, stakeholder buy-in and conceptual direction.Responsible for data warehouse design, maintenance and understanding usage.Act as a change leader to introduce and implement increased capabilities to advance data usage.Partner with data and business users to develop a rich pipeline of value-added work.Oversee user acceptance testing for data releases to ensure high value output.Recommend technology and process improvements across the data environment.Reduce production support time.Partner across IT and business groups to identify integrated solution opportunities.Mentor and develop data users across CXO organization.Develop deep functional understanding of ADT Customer Experience Operations organization including Customer Care, Business Intelligence, Advanced Analytics, Retention, Sales, Marketing, Technology and Field Operations.Minimum Qualifications: BS in Computer Science, Information Systems, or related quantitative discipline.5+ years demonstrated experience with ETL technologies and building data pipelines.5+ years demonstrated experience with SQL Server (SSIS, SSRS).5+ years demonstrated experience building semantic layers to simplify reporting and analytics.2+ years demonstrated experience with cloud data warehouses (i.e. GCP BigQuery).Job Type: Full-timePay: $85,000.00 - $105,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insuranceLife insurancePaid time offTuition reimbursementVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payExperience:SQL: 5 years (Preferred)Work Location: Remote",https://www.indeed.com/company/ADT-Security-Services/jobs/Data-Engineer-851e16dc31815f3d?fccid=27a2140ddcca6697&vjs=3,+2 locationsRemote,Data Engineer,Data Engineer
132,Cisco Systems,"*** Must be a US Citizen and have the ability to obtain a U.S. Government Security Clearance***
What You’ll Do
As part of the CX Public Sector High Touch Technical Support (HTTS) team, you will work with an outstanding team of Tier 2 and 3 Customer Support Engineers. The CX PS HTTS team specialization in one of or more core technologies while understanding the foundations of our network products, protocols, and effective methodologies. You will be part of a collaborative team to provide support during critical network issues as well as leading a caseload of lower critical issues.

Who You’ll Work With
The CX Public Sector team provides second/third level technical support at security classification to the Federal and SLED customer base via phone, email, web, and remote access for R/S, Security, Collaboration, Datacenter, Wireless, Service Provider (both hardware and software) to Cisco customers, partners, account teams, and other engineers via phone/email/ consultation to independently solve & debug product problems. HTTS in Public Sector provides our Federal, SLED, and other customers operating within the regulatory space Cisco’s leading-edge technical support and services natively; thus redefining the way our customers meet their critical mission and business needs.

Who You Are
Minimum Qualifications: Sole US Citizenship and clearable for Top Secret clearance Technology expertise - CCIE or equivalent experience Resolution leader, problem-solving - troubleshooting methodology Communication & Facilitation, Listening & Affirmation, Influence & Persuasion, Public speaking & Presentation, Coaching Introspective - understand own social style, strength, flaws, and relation to others Results Focus - ego suppression, objectivity Phenomenal teammate passionate about customer success
Required skills Expertise with at least 1 of the following: Nexus ACI Routing/Switching
Nice-to-have skills Nexus Dashboard Tetration
Skills in server knowledge: Cisco UCS, VMware, Windows Server, Unix, Linux, Python,
REQUIRED EDUCATION Four-year college degree in Computer Science (or similar)
OTHER REQUIREMENTS US Citizen TOP SECRET/SCI with CI Poly Preferred Located in RTP, working in Cisco office daily Travel less than 25% Cisco Network Certifications (i.e., CCNA, CCIE, CISSP, CCNP, CCDA, etc.) a nice to have

Why Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!

But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)

Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward.

So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!


LI-VL1

dicesvs",https://www.indeed.com/rc/clk?jk=2263a363ce4f9842&fccid=904ac33af4a58e1e&vjs=3,"Research Triangle Park, NC",Customer Delivery High Touch Technical Engineer - Data Center,Data Engineer
133,Zayo Group,"Company Description
Zayo provides mission-critical bandwidth to the world’s most impactful companies, fueling the innovations that are transforming our society. Zayo’s 133,000-mile network in North America and Europe includes extensive metro connectivity to thousands of buildings and data centers. Zayo’s communications infrastructure solutions include dark fiber, private data networks, wavelengths, Ethernet, and dedicated Internet access. Zayo serves wireless and wireline carriers, media, tech, content, finance, healthcare and other large enterprises.
Position Description
 This role is responsible for the design, development, deployment of an MSSQL data warehouse to support analysis and reporting. The position is also responsible for maintaining, supporting, and enhancing existing integration systems as well as assisting in the ongoing development of technical best practices for data movement, data quality, data cleansing and other ETL related activities. This is an individual contributor position with no personnel responsibilities.
Essential Duties/Responsibilities:
• Member of the data engineering team working to protect, improve, and leverage the value of our data assets
• Build and automate new ETL processes to pull from many disparate data sources to centralize information in a new data warehouse design
• Maintain and develop existing data warehouse integrations and ETL processes to move data efficiently between various enterprise systems.
• Miscellaneous automation of data processes across a wide variety of customer platforms • Implements interfaces to meet changing integration requirements after fully understanding company specific configurations
• Production support for integration servers
• SQL & Reporting support across various business groups
• Follows standard test plan to test each client and ensure that configured functionality meets the needs of the company
• Negotiates and makes changes to existing interfaces as needed
• Ability to handle multiple priorities and meet aggressive deadlines
• Required to be part of an on-call rotation and will occasionally be responsible for after-hours maintenance work
Qualifications:
• Intermediate to Advanced SQL skills
• Proficiency in MS SQL Server, SSRS & MS BI stack preferred
• Bachelor's degree Business Intelligence, Computer Information Systems or similar • 5 or more years database design, data management, reporting and ETL. • The ability to collaborate across functional groups.
• The ability to communicate effectively at all levels of the organization.
• Prior experience with data migration/integration efforts
• Prior experience with Salesforce.com a plus
• Prior experience with Cast Iron integration application or similar tools a plus
Prior experience with DbAmp a plus
Base Salary Range- $65,600-$87,500
Benefits, Rewards & Wellness
Excellent Health, Dental & Vision Insurance
Retirement 401(k) Savings Plan
Fitness membership discounts
Generous paid time off policy including paid parental leave
Please note, in accordance with Zayo's commitment to providing and maintaining a workplace free of recognized hazards, all U.S. and Canadian employees and any employee, vendor, customer, or visitor who enters a Zayo office or facility in the U.S. and Canada must be fully vaccinated against COVID-19 and provide proof of such vaccination. If you are hired by Zayo, you will be required to provide proof of vaccination or have a valid religious or medical reason not to be vaccinated.",https://www.indeed.com/rc/clk?jk=38c35ae367b975c7&fccid=44666e7051739fbb&vjs=3,United States+5 locations,Data Engineer Specialist,Data Engineer
134,Sub-Zero Wolf,"Provide for, and supports, the development, analysis, implementation and security of all of the company’s analytical data solutions for corporate, affiliated distributors, and independent distributors. This position will manage, monitor and maintain company data warehouses, analytics solutions, and ensure the high availability of the data. This position is responsible for designing, building and implementing complex data warehouses, dimensional structures, data replication, and integrations in both on premise and cloud environments. This position will work in conjunction with Domain Architects, Data Analysts, Product Owners and other Data Engineers and work with Business Analysts, Product Owners and IT leadership to obtain business requirements, outline data solutions and implement.

Responsibilities

Act as a technical mentor and advisor within IT, including mentor junior staff as needed.
Analyze, plan and implement automated systems to solve complex business opportunities.
Write detailed design documents and assisting with gathering business requirements with business users and analysts.
Provide escalation support from other IT team members.
Ensure that data engineering documentation remains current, including documentation on data warehouse, ETL and structure metadata.
Design, develop, implement and maintain ETL and modeling processes on premise and in the cloud utilizing current ETL and modeling tools and methods
Design, implement and maintain semantic layers, datasets and data models.
Develop, implement, and maintain automated deployment pipelines and automated testing processes for modifications to the data warehouse environments.
Coordinate with appropriate IT and business stakeholders.
Participate in the execution and planning of team work utilizing Agile Scrum.
Assist with analysis for budgeting on future projects, tools, and technology.",https://www.indeed.com/rc/clk?jk=9cdd04fd0330122e&fccid=de79aef41fe8d12a&vjs=3,"Madison, WI 53711 53711",Data Engineer III,Data Engineer
135,Blutium,"Key Responsibilities: Build pipelines to manage audience data for personalised marketing campaigns, integrate audience data from various sources to build one view of the customerPerform experimentation at scale by tagging audiences based on complex hierarchy of customer, product, and regional factorsResponsible for development of the workflow orchestration & ETL pipelines within marketing's analytics and data science platforms. You will ensure that the data pipeline infrastructure meets the analysis, reporting, and data science needs of the marketing organization.Implement continued design, development and optimization of the marketing data pipelines & data prep infrastructure built on cutting-edge cloud technologies.Embrace an active team role to help design, implement, and launch efficient and reliable data pipelines moving data across several platforms including Data Warehouse, online caches, and real-time systems.Create data architecture that is flexible, scalable, and consistent for cross-functional use, and aligned to stakeholder business requirements.Deploy workflow orchestration and demonstrate expertise in data modeling, ETL development, and data warehousingValidate Data Engineering business data elements, organizational and business intelligence architecture designs for engineering functional areas from Dashboards, Data Lakes, Data Operations, ML - AI, and upstream/downstream intake and output processes,Qualification − BA/BS Degree in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.Certifications − NAExperience − 5+ years of industry experience in data engineering, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets − Work experience with developers and business areas to design, configure, deploy, and maintain custom ETL Infrastructure to support project initiatives − Experience in interacting with multiple teams, client stakeholders and able to fix the issues in a shorter span of time SkillsTechnical/Domain − Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS − Experience in Airflow is a must Page 3 of 4 Private & Confidential − Experience extracting/cleansing data and generating insights from large transactional data sets using Spark SQL, SQL, Python, and PySpark on cloud − Experience with optimizing Spark pipelines on Dataproc, Databricks or similar technologies. CompetenciesBehaviourl − Must have exceptional and effective communication, organization, and time management skills − Must be pro-active and determined (Initiates action, Analytical and problem solver)Job Types: Full-time, ContractPay: $100,000.00 - $110,000.00 per yearSchedule:8 hour shiftMonday to FridayExperience:airflow: 4 years (Required)Azure: 4 years (Required)AWS: 5 years (Required)Work Location: Remote",https://www.indeed.com/company/Blutium/jobs/Data-Engineer-067f5a64baa4bd44?fccid=07ba05388d24e41c&vjs=3,Remote,Data Engineer,Data Engineer
136,EPMA,"Our client is looking for an experienced Data Engineer for an initial contract of 6 months, with strong ability to extend based on performance.Start: ASAP Location: Remote (may be light travel in the future) Client HQ: Chicago, ILRole: Data Engineer: (4-5 years + experience)Experience working with Spark Structured Streaming, Pyspark, DatabricksExperience with creating and submitting Spark jobsExperience with debugging Spark jobs to optimize memory/cpuExperience working with one of the major cloud providers, viz, AWS, GCP, AzureExperience working with high frequency streaming dataExperience working on a client facing teamExperience working with version control systems such as GitExperience working on multi-skill scrum teamsGood communication skillsCandidate's with Independent visa may apply with no sponsorship required.Not accepting Corp to Corp- C2C applications.Job Type: Full-time",https://www.indeed.com/company/EPMA/jobs/Data-Engineer-ab78ec0a182cdb17?fccid=3d11d7e35a6c2115&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
137,KLDiscovery,"KLDiscovery, a leading global provider of electronic discovery, information governance and data recovery services, is currently seeking a Software Developer I.Join the Data Storage Technology Engineering Team. The position will assist in review and analysis of applications, product development, and enhancements including documentation, code development, and unit testing of releases while adhering to KLDiscovery development processes and workflows with supervision and direction from lead developers and superiors.Remote, work from home opportunity.ResponsibilitiesDevelop, maintain, and update a variety of data recovery desktop applications that need to work with the latest technologies in the marketAssist team members with researching existing storage technologies from a hardware and software perspectiveDevelop automated unit testsDevelop automated functional and regression testsSupport bug fixes and implement enhancements to applications in ProductionWork with the SQA team on defect resolutionCreate design and user documentation as neededUtilize, communicate and enforce coding standardsSeek and participate in personal development opportunities to maintain a detailed knowledge of industry standards, engineering best practices, and business needsProvide Technical Support - bug fixes and implement enhancements to applications in Production within defined SLAWork in a team or with multiple organizational departments in a dynamic and rapidly changing environmentAdhere to development processes and workflowsQualificationsBS in Computer Science, Engineering, or related scientific field preferred1-2+ years of related experience preferredCapable of problem identification and resolutionsAbility to learn from others and adapt to standardsAbility to change projects as neededAbility to express complex technical concepts effectively, both verbally and in writingUnderstands various programming languages including C/C++, C#, SQL, Python, etc.Ability to work in a team or with multiple organizational departments in a dynamic and rapidly changing environmentAbility to pass a background check upon offerThis position does not qualify for VISA sponsorship .This position operates under International Traffic in Arms Regulations (ITAR) and therefore, any person hired must demonstrate with verifiable documentation that he/she is either (i) a U.S. Citizen; (ii) ac active Green Card Holder; pr (iii) a ""Protected Person"" as defined by 8 U.S.C. 1324(b)(a)(3).What We OfferA friendly and welcoming team-oriented environmentOpportunities for career advancement and growthBusiness casual dressMedical/Dental/Vision benefits as well as company provided Life Insurance, Short Term and Long-Term DisabilityPaid Time Off & 401k retirement savings plan with company matchOur Cultural ValuesEntrepreneurs at heart, we are a customer first team sharing one goal and one vision. We seek team members who are:Humble - No one is above another; we all work together to meet our clients’ needs and we acknowledge our own weaknessesHungry - We all are driven internally to be successful and to continually expand our contribution and impactSmart - We use emotional intelligence when working with one another and with clientsOur culture shapes our actions, our products, and the relationships we forge with our customers.Who We AreKLDiscovery provides technology-enabled services and software to help law firms, corporations, government agencies and consumers solve complex data challenges. The company, with offices in 40+ locations across 19 countries, is a global leader in delivering best-in-class eDiscovery, information governance and data recovery solutions to support the litigation, regulatory compliance, internal investigation and data recovery and management needs of our clients.Serving clients for over 30 years, KLDiscovery offers data collection and forensic investigation, early case assessment, electronic discovery and data processing, application software and data hosting for web-based document reviews, and managed document review services. In addition, through its global Ontrack Data Recovery business, KLDiscovery delivers world-class data recovery, email extraction and restoration, data destruction and tape management.KLDiscovery has been recognized as one of the fastest growing companies in North America by both Inc. Magazine (Inc. 5000) and Deloitte (Deloitte's Technology Fast 500) and CEO Chris Weiler was recognized as a 2014 Ernst & Young Entrepreneur of the Year™. Additionally, KLDiscovery is a Relativity Certified Partner and maintains ISO/IEC 27001 Certified data centers around the world.KLDiscovery is an Equal Opportunity Employer.Job Type: Full-time",https://www.indeed.com/company/KLDiscovery/jobs/Software-Engineer-dc384b11df710b78?fccid=d455b737dea6e4d9&vjs=3,"Remote in McLean, VA",Software Engineer I - Data Storage Team (Remote Opportunity),Data Engineer
138,Veear Projects,"Essentials for the role · Python coding skills for data movement and data manipulation (not app development)· SQL skill areas: table partitioning, bucketing, Hive, HQL, windowing functions, aggregates, ranking, table joins, self-joins, subqueries, data modelling, User defined functions (UDF) and Common Table Expressions (CTE)· Skills in optimizing SQL query performance.The team needs Data Engineer having strong in SQL and have great experience in building data frameworks. · Understand and execute SQL queries efficiently (understands the trade-off between different types of query)· Ability to ask the right questions, very good communication skills.· Good problem-solving skills (ability to think through problems and walk through the with the stakeholders)Job description: · Create and maintain optimal data pipeline architecture, · Assemble large, complex data sets that meet functional / non-functional business requirements.· Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.· Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.· Experience building and optimizing data pipelines, architectures, and data sets.· A successful history of manipulating, processing, and extracting value from large, disconnected datasets.Job Types: Full-time, ContractPay: $80.00 - $125.00 per hourSchedule:Monday to FridayWork Location: Remote",https://www.indeed.com/company/Veear-Projects/jobs/Data-Engineer-9e924c053d70e33a?fccid=1b62f6efacf00111&vjs=3,Remote,Data Engineer,Data Engineer
139,APPIC Solutions LLC,"Location: Englewood, COAbout the Role: *This role will be responsible for expanding our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler, who enjoys optimizing data systems and building them from the ground up.This position will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.Day to Day Responsibilities:**Create and maintain optimal data pipeline architecture;Assemble large, complex data sets that meet both functional and non-functional business requirements;Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.;Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies;Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics;Collaborate with stakeholders including the executive, product, data and design teams to assist with data-related technical issues and support their data infrastructure needs;Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader;Work with data and analytics experts to strive for greater functionality in our data systems.**Must Have:**Five+ years of experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines;Strong Software Engineering experience with proficiency in at least one of the following programming languages: Java, Golang, Python, Scala or equivalent;Implement data ingestion pipelines both real time and batch using best practices;Experience with building stream-processing applications using Apache Flink, Kafka Streams or others;Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.;Experience supporting and working with cross-functional teams in a dynamic environment;Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.Experience with ELK stack.Ability to work in a Linux environment.**Nice to Have:**Experience in building distributed, high-volume data services;Experience with big data processing and analytics stack in AWS: EMR, S3, EC2, Athena, Kinesis, Lambda, Quicksight etc.;Knowledge of data science tools and their integration with data lakes;Experience in container technologies like Docker/Kubernetes*Job Types: Full-time, ContractPay: $45.00 - $65.00 per hourSchedule:8 hour shiftAbility to commute/relocate:Englewood, CO 80110: Reliably commute or planning to relocate before starting work (Required)Experience:Data Engineering: 4 years (Preferred)MapReduce: 3 years (Preferred)Hadoop: 3 years (Preferred)Java: 4 years (Preferred)SQL: 4 years (Preferred)Python: 5 years (Preferred)Work Location: One location",https://www.indeed.com/company/APPIC-Solutions-LLC/jobs/Data-Engineer-d90732e5037ddfa5?fccid=c43a102ed5c0c415&vjs=3,"Englewood, CO 80110 80110",Data Engineer,Data Engineer
140,"Baanyan Software Services, Inc","Duration:  12+ MLocation:  Dallas, TX (Remote Until Covid)Min 7 Years of Experience as Data EngineerHands on Experience on PythonHands on Experience on SparkJob Types: Full-time, ContractPay: From $50.00 per hourApplication Question(s):Current Work Autorization?Free time for a Quick Call?Education:Bachelor's (Preferred)Experience:Python: 6 years (Preferred)Spark: 4 years (Preferred)Work Location: Remote","https://www.indeed.com/company/Baanyan-Software-Services,-Inc/jobs/Data-Engineer-e9a08a19f1fd3909?fccid=a781b84555c30b72&vjs=3",Remote,Data Engineer,Data Engineer
141,Clarify Health Solutions.,"Our Mission
We exist to power better care.

We are on a mission to enable better care by delivering the technology and insights to improve every patient journey. To achieve this, we are deliberately bringing together a multi-disciplinary team with healthcare, Silicon Valley big data software engineering, and cutting-edge data science capabilities. Clarify Health's enterprise analytics platform empowers providers, payers, and life sciences companies to deliver better care to patients through actionable insights. With industry-leading statistical modeling and machine learning, Clarify Health harnesses the power of the most comprehensive longitudinal dataset in the US by linking clinical, claims, prescription, lab, and social determinants of health data across over 300 million lives.
We came together to empower physicians and care teams to deliver great care. We are a purpose-driven team bringing our vision of better care to life.

*Please note, while this job is listed as remote, we can only employ in the following states: AZ, CA, CO, CT, FL, GA, ID, IL, KY, MD, MA, MI, MN, MO, NH, NJ, NY, NC, ND, OH, OR, PA, SC, TN, TX, UT, VA, WA, WI. If your state is not listed, we cannot move forward with your application. *

Clarify is looking for Data Engineer to focus on building big data pipelines running on Apache Spark.

Does this sound like you?You are excited to help solve healthcare problems with big dataYou are eager to learn new technologies as well as our existing architectureYou have an enthusiastic, energetic personality; inquiring, investigative mindYou embrace change as an opportunity to learnYou take great care in the details
The Role
Design, develop and test pipelines for big data processing
Gain solid understanding of our platform architecture and help develop it further
Work with senior team members to optimize and scale new and existing big data pipelines
Follow best practices and coding standards
Be integral part of scrum team in fast-paced startup environment; be flexible and willing to switch tasks based on team's needs and priorities
What we are looking for
We are always looking for new team members who will add to our culture and have a strong passion for impact. In particular, the Associate Data Engineer will have:
Strong programming and algorithmic skills
Experience with Python and SQL
Familiarity with Git, Airflow, relational databases, Spark or Hadoop
Bachelor’s or foreign equivalent degree in Computer Science or a related field and 1+ years of experience in the position or related occupation
What we offer you
Competitive salary ($115-130k base commensurate with experience + bonus + equity options)
Quality health insurance
Traditional 401K plan
Vision, dental, disability and life insurance
Flexible Spending Accounts and Commuter Benefits
Generous PTO
Flexibility
Monthly wellness stipend
Remote friendly
A collaborative workplace, which will challenge you and celebrate your work
A chance to learn with and from interesting and enthusiastic colleagues
 We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.",https://www.indeed.com/rc/clk?jk=35aead1a28dc0b21&fccid=a43657d8af5c59dd&vjs=3,+1 locationRemote,Associate Data Engineer,Data Engineer
142,Pattern Energy Group LP,"Overview:

Company Overview
Pattern Energy is an independent, fully integrated energy company that develops, constructs, owns and operates renewable energy projects and transmission assets across North America, Canada and Latin America. The company focuses primarily on wind and transmission with limited investments in other generation technologies.
The Pattern Energy team has a history as one of the top North American renewable energy and transmission providers in the industry. The team is dedicated to delivering the highest value for its customers, partners, financial supporters and the communities in which it works, while exhibiting a strong commitment to promoting environmental stewardship and corporate responsibility.
Pattern Energy operates in the United States, Canada and Latin America with offices in San Francisco, Houston, San Diego, New York, Toronto, Puerto Rico and Chile. Pattern Energy Group's corporate headquarters are in San Francisco.
Responsibilities:

Job Purpose
Provide core data services to company partners and customers used to inform commercial and operational decisions, at direction or supervision of senior internal support. Manage critical data processes & infrastructure, systems and software integrations, and cloud-based subscriptions & applications that are necessary for short and medium term project decision making. Perform peer reviews of team member products before dissemination to other business units.
Qualifications:

Key Responosibilities
The key responsibilities will include the following:
Develop, implement and continuly support new and existing data pipelines and system/software integrations.
Organize, maintain, and monitor existing data processeses to ensure the Operations Data Warehouse has a high level of data quality and availability..
Architect and manage accesible data schemes for internal and external users.
Support data requirements for ad hoc studies as needed relating to fleet performance, equipment reliability, and root cause analysis.
Build technical documentation and data maps pertaining to the Operations Data Warehouse.
Develop analytics tools, reports, and dashboards using Power BI.
Manage and innovate within the Microsoft suite of tools such as cloud-based applications, services, and subscriptions.
Education/Experience/Qualifications Required
Educational/Work Experience
A bachelor’s degree in computer science, mathematics/statistics, physics, or engineering OR demonstration of equivalent work experience is required as a minimum.
Bachelor's degree in computer science, engineering, or related field. 2-5 years of experience in relevant discipline, preferably in the energy and utilities industry.
Highly authoritative working knowledge of specialized methods and processes gained from on the job experience or past professional qualification.
Ability to consider individual business aspects and increasingly, the implications of the work product beyond the immediate problem.d
Additional Qualifications
Strong technical capabilities and a creative, curious, self-driven work ethic
Ability to work well with customers and support.
Ability to travel (Up to 10% travel)
Technical Skills
Data Engineering knowledge in Microsoft SQL, Microsoft Azure (Data Factory, Databricks, Azure Synapse) or other cloud storage databases.
Demonstrated programming skills with software applications, databases, and historians that produce large time-series based data sets.
Proficient with Spark, Delta Engine/Delta Lake, Polybase, and/or BLOB storage.
Familiar with data visualization tools such as Power BI and Plotly.
Demonstrated excellence and ability to learn new programming skills and languages.
Familiarity with SCADA systems (particularly in electrical utility applications)
Ability to occasionally perform physical requirements including standing, sitting, walking, kneeling, twisting, reaching, and climbing at project locations in order to install or inspect equipment.
Ability to read and understand plans, specifications, drawings, and documents relating to computers and data systems.
Ability to quickly assess emergency situations and requirements and to develop and implement appropriate action plans
Pattern Energy Group is an equal opportunity employer
#LI-SJ1",https://www.indeed.com/rc/clk?jk=287d9a9595f11515&fccid=7b28733b7afedc61&vjs=3,"Houston, TX 77002 77002 (Downtown area)",Data Engineer,Data Engineer
143,Percept Health,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.****Please do not apply if you do not have healthcare data experiences****Responsibilities· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances. Develop complex reporting and extract-based stored procedures and SQL queries· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, PySpark, Scala on Spark clusters· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formatsRequirements· Bachelors or master’s degree in computer science or related field· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehousesPreferred ­­­­­· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics· Experience with FHIRJob Types: Full-time, ContractPay: $50.00 - $55.00 per hourSchedule:8 hour shiftExperience:SQL: 4 years (Required)Python or Scala or PySpark: 3 years (Required)AWS / Azure: 3 years (Required)Healthcare data: 4 years (Required)Work Location: Remote",https://www.indeed.com/company/Percept-Health/jobs/Healthcare-Data-Engineer-e7599188f5a999ca?fccid=d51006b33150350a&vjs=3,Remote,Healthcare Data Engineer,Data Engineer
144,Azurity Pharmaceuticals,"
Mission:
Implement the data models and data structures needed for each use case as defined by the Data Architect, in the most convenient format to be used by the Data Scientist
Own the structural elements of data, e.g., data storage, data piping, interfacing with analytics platforms
Participate in data requirements, modelling and testing
Tasks & responsibilities:
Provide technical support related to data structures, data models and meta data management to relevant stakeholders
Creates data models, providing the right format and structure for the use case solutions
Participate in early data modeling and testing for use case development, provide input on how to improve proposed solutions and implement necessary changes
Extract relevant data to solve analytical problems; ensure development teams have the required data
Interact with the business ([function]) to understand all data requirements to develop business insights and translates them into data structures and data models, in close collaboration with Data Architect
Work closely with IT/IM teams on internal data acquisition (e.g., CRM, ERP) and with Data Architect for external data acquisition
Knowledge & experience:
5+ years’ experience with advanced data management systems (e.g., PostgreSQL, etc.)
Deep expertise in data modeling and structuring
Experience in high volume data environments
Ability to quickly learn new technologies
Developing and maintaining formal documentation that describes data and data structures including data modelling
Strong attention to detail and an ability to think critically and conceptually
Team oriented and flexible with proven track record in collaborating with multiple stakeholders
Strong verbal and written

",https://www.indeed.com/rc/clk?jk=bd05e18c6cd7dc78&fccid=dfaf5097e7d970f4&vjs=3,"Atlanta, GA 30328 30328+1 location",Data Engineer,Data Engineer
145,"Insight Enterprises, Inc.","Assessment Engineer – Data Protection and StorageWe areInsight Enterprises (NASDAQ: NSIT)*NOT a Data Science Job*NOT looking for candidates right out of school. Must have min of 3 years working experience with Storage architecture and Data ProtectionAs a Fortune 500-ranked global provider of digital innovation, cloud/data center transformation, connected workforce, and supply chain optimization solutions and services, we help clients successfully manage their IT today while transforming for tomorrow.From IT strategy and design to implementation and management, our employees help clients innovate and optimize their operations to run smarter.Microsoft Worldwide AI Partner of the Year, 2018Microsoft Worldwide Modern Desktop Partner of the Year, 2018Microsoft US Partner Award for Intelligent Cloud - Application Innovation, 2019Microsoft US Azure Team Partner Choice Award - Data and Artificial Intelligence, 2019Our Insight Cloud+Data Center Transformation team is searching for an experienced, passionate and professional Assessment Engineer – Data Protection and Storage to join our team.At the heart of every great technology solution is great management, always making sure that the project delivers what it set out to do and matches expectations.Assessment Engineer – Data Protection and StorageAs an Assessment Engineer for Data Protection and Storage at Insight, you will work in a dynamic and leading-edge environment delivering client focused Data Protection (Commvault, Rubrik, Veeam, and Veritas) and Storage (NetApp, Pure, Dell, HPE) Assessments along with other technologies like converged infrastructure solutions. In this role, you will work collaboratively with other Insight teammates (sales, architects, engineers, and partner managers) to qualify assessment nominations including determining scope, level of effort, client outcomes and writing the Statement of Work for delivery of the assessment using Insight templates. The Assessment Engineer will work with the field sales, architect, and delivery engineer SMEs to deliver the assessments which involve a project kickoff, data collection, data analysis, documentation, and findings presentation to the client.What you’ll do at InsightProvide client assessments for:Data Protection solutions consisting of Commvault, Rubrik, Veeam, Veritas NetBackup, Veritas BackupExec, and other backup solutionsStorage and Converged Infrastructure: NetApp, Pure, Dell, Nutanix, HPE, and othersVirtualized Environments: VMware, Hyper-V, etc.Use of public, private or hybrid cloud architecturesApply systems analysis techniques and procedures, including consulting with clients, to determine requirements, operational challenges, outcomes, budget and timeframe.Maintains data collection repositories and recommends new solutions and services including technology upgrades, refreshes, re-platform, and expansion into public cloud (AWS, Azure, GCP)Executes data collection of client environment current state using vendor provided tools, Mitrend, and APTARE to name a few.Document current state, future state identifying risks, health, roadmaps, and recommendations based on quantitative and qualitative data analysis.Presents assessment deliverables to client in partnership with Insight teammates.What you’ll need to join InsightBachelor’s degree in Engineering, Computer Science, Information Technology, or a related field; or equivalent years of working experience.At least 5 years of experience working with information systems technology appropriate to the role, such as data protection, storage, converged infrastructure, virtualization, and/or servers and applications.Expert skills with Microsoft Excel & PowerPointExperience with Structured Query Language (SQL) query development for reporting, and data visualizationExperience with Veritas APTARE and SQL Template Designer is a plusStrong experience in managing and supporting multiple data protection and storage systems in a highly virtualized, and hybrid cloud environments.Growth in technical skills through relevant certifications or other similar achievements is desired.Knowledge of systems analysis techniques and proceduresMust have the ability to keep current with alternative solutions and technology and make recommendations to clients consistent with Insight’s business and system strategies.Ability to effectively communicate, present and articulate strategies across various audiences; clients, sales, marketing, partners, consultants, and executive leaders (internal and external)Eagerness to learn new tools and technologies, and passion to deliver quality deliverables both individually and as part of a team.Accuracy and attention to detail, must have organizational skillsHighlights of the 2021 Insight Corporate Citizenship Report include:In the last year, Insight has been named as a Forbes World’s Best Employer, a Forbes Best Employer for Veterans, a Fortune World’s Most Admired Company and a Fortune Best Workplace for Diversity. Insight also received a score of 95 out of 100 on the Human Rights Campaign Foundation’s 2021 Corporate Equality Index for LGBTQ workplace equality.Insight Digital Innovation launched a Detection and Prevention solution built on the company’s AI and IoT-powered Insight Connected Platform™, providing a scalable foundation to help organizations detect for common signs of COVID-19 and quickly curtail its spread in high-traffic areas.The Community Wireless Broadband solution, designed by Insight’s Cloud + Data Center Transformation team and provided through Insight Public Sector, brings much-needed broadband connectivity to teleworkers and remote students in underserved communities who lack reliable Internet connections.Celebrating diversity and inclusion, Insight held its first Global Harmony Day in recognition of Global Diversity Awareness Month in October 2020. The day united Insight teammates worldwide for a series of candid discussions on diversity, equality and inclusion.Insight’s teammate resource groups, promoting allyship for cultures and communities that have traditionally faced challenges in the workplace, expanded to five groups with the addition of the Afro-Professionals and Allies at Insight group. These teammate-led groups now have more than 1,200 active members.As part of the Insight Reach program, dedicated to empowering children’s lives through technology, the company’s 10 th annual Noble Cause campaign raised $250,000 for charities like the Ronald McDonald House, Make-A-Wish Foundation and the Boys & Girls Club of Arizona. More than $1 million has been donated in the last six years.In It Together Foundation, Insight’s 501(c)(3) charitable non-profit program helping teammates in crisis through teammate contributions and matching company funds, has raised more than $2.15 million since its inception in 2014. Since January 2020, 170 teammates have received support, and Insight’s executive team contributed $250,000 as part of its COVID-19 response to help families hit particularly hard by the pandemic.Environmental sustainability initiatives saved Insight more than $7 million in 2020.As an affiliate member of the Responsible Business Alliance, Insight’s asset disposition program – helping companies securely repurpose old devices and equipment – and diverse supplier program support a commitment to sustainability and equality.Job Type: Full-timePay: $120,000.00 - $160,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programEmployee discountFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveProfessional development assistanceReferral programRetirement planTuition reimbursementVision insuranceSchedule:8 hour shiftSupplemental Pay:Bonus payCOVID-19 considerations:Work is remote from comfort of your own homeExperience:Storage Technology: 3 years (Required)SQL Knowledge: 1 year (Required)Solutions Assessments: 3 years (Required)Work Location: Remote",https://www.indeed.com/company/Insight/jobs/Assessment-Engineer-d7765a37217bf199?fccid=69247daab4a84a92&vjs=3,Remote,Assessment Engineer (Storage and Data Protection),Data Engineer
146,Volto Consulting,"US Citizen /Green Card Holder/ EAD onlyRequired Qualifications5 + Years’ hands on experience in data engineeringExperience working with Databricks and Apache Spark/PySpark * Experience with cloud-based data services, including data pipeline orchestration tooling (i.e. Azure Data Factory).Proficiency with complex SQL developmentExperience in modern DevOps practices (including Git, CI/CD)Experience in Data Modeling.Strong business acumen and adaptability to partner with the AI/ML product teams on innovative solutions to constantly changing business requirementsAny ETL experience in design, mapping and configuration in a complex environment processing large volumes of dataJob Types: Full-time, ContractPay: $50.00 - $65.00 per hourSchedule:8 hour shiftApplication Question(s):Kindly mention your Visa statusExperience:SQL: 5 years (Preferred)Data modeling and Data bricks: 5 years (Preferred)CI/CD: 5 years (Preferred)Spark: 5 years (Preferred)ETL: 5 years (Preferred)Data engineering: 5 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Volto-Consulting/jobs/Data-Engineer-72364d5e5aca3cdc?fccid=242f54a0bd5d5b7f&vjs=3,Remote,Data Engineer,Data Engineer
147,"Dynamic Placement Services, LLC","Dynamic’s client is seeking a Data Engineer for a permanent position that’s 100% remote. The candidate will be part of a collaborative 5-person analytics team.NOTE: Candidates must reside in the U.S. and be authorized to work in the U.S. without visa sponsorship. No C2C.Qualifications/Duties:· 5+ years of data analytics and reporting· Data migration· Data warehouse· Build Tableau or Power BI dashboards· Build data pipelines using Azure Databricks· Streamsets· Spark· Pyspark· Azure Data FactoryJob Type: Full-timePay: Up to $120,000.00 per yearSchedule:Monday to FridayApplication Question(s):What is your desired salary?Experience:Azure Databricks: 1 year (Preferred)Work Location: Remote","https://www.indeed.com/company/Dynamic-Placement-Services,-LLC/jobs/Data-Engineer-073a4e2ed84cad43?fccid=b4b5ef3a7571bb88&vjs=3",Remote,Data Engineer (Azure Databricks),Data Engineer
148,Stratasan,"Position Summary:
In the Data Engineer role, you will be responsible for processing, transforming, and automating the ingestion of data into our data warehouse. This can also involve troubleshooting data issues and working with clients, and internal customers to resolve data issues.
In this role you will:
Aid the development and automation of processing incoming data into the data warehouses.
Automate manually generated reports.
Run existing data pipelines that have yet to be automated.
You will love this job if:
You understand that building data-intensive applications requires teamwork.
You believe that diagramming and writing documentation are key pieces to the long-term maintainability of any significant software product.
You have keen attention to detail and enjoy diving deep into problems.
You consider yourself proactive, spotting and addressing small issues before they grow.
You enjoy problem solving
Why We Should Hear From You:
You have some experience in SQL and Python
You have some experience in automating data processes and reports
You are familiar with relational databases
You are familiar with Linux OS
You have some experience in data formatting (CSV, TXT, JSON, Parquet, Fixed Width)
Aws/ redshift or other similar cloud experience is a plus
Compensation:
As always, the pay is generally based on the level of experience.
About Stratasan:
For today’s healthcare providers, decisions matter more than ever. Teams that spend too much time in the weeds of data processing can’t keep pace with those who’ve learned how to elevate their decision quality. Providers who are winning are spending their resources on planning and execution, resulting in better, more confident decisions. That’s where Stratasan can help.
Our bright, diverse team of analysts, engineers, product developers, and everyday dreamers keep our software and services running smoothly so our customers can make decisions with confidence. By enabling our clients to analyze and visualize what they need to, we set them on a course for sustainability and growth that can survive even the most disruptive of circumstances.
We’re stronger because of our customer-first culture and inclusive work environment. We're a team of entrepreneurially-minded, self-motivated healthcare aficionados and technology geeks. Together, we solve complex problems, rethink conventional processes, and celebrate big wins.
Remote work has been and always will be a part of our culture and in the spirit of 2020, we have transitioned to a fully distributed workforce for the time being. We engage regularly throughout the workday and keep things social with virtual happy hours, lunches, Slack chatter and group games.
We’re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status. We believe our differences truly make us stronger.
We look to our company core values as a standard for identifying candidates who will best fit our culture. Our core values are non-negotiable guiding principles that steer our team and trickle-down to how we interact with our clients.
Everybody Makes the Coffee
We help where help is needed. Whether we’re asked to pitch in to support a customer, run a meeting at the last minute, or make the morning coffee, everybody does what’s necessary to keep the engine running. We don’t believe in “pulling rank.""
Our Success is My Success
We support one another, strive for the success of our team, and recognize that nothing is done alone.
We'll Figure it Out
We may not immediately have the answer to a question, but we’re always committed to finding one. We believe most challenges are great opportunities to consciously show up, and with a resourceful attitude, brainstorm and consider the best course of action. Whatever comes our way, we won’t quit until we find a solution.
Honest Conversations Make Us Stronger
We approach conversations from a position of vulnerability. We assume others have positive intentions and strive to see their perspective before sharing our own. This combination allows us to create stronger, more collaborative relationships—which, ultimately, is how we grow.
Assume Positive Intent
We believe it’s important to start from the assumption that people are good and that the intent behind their action is positive. We believe this builds stronger and more trusting relationships.

ngrGjESwZr",https://www.indeed.com/rc/clk?jk=c9bc1557773d79d7&fccid=8004833dd36ccbd5&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
149,Canonical - Jobs,"This is an exciting opportunity for a data center professional passionate about open source software, Linux, and the latest server and network technologies. Come build a rewarding, meaningful career working with the best and brightest people in technology at Canonical, a growing international software company.

As a Data Center Engineer in Canonical, you will be responsible for the day-to-day management and operations of our data center and labs in the Boston area. This includes hardware management, working with data center automation tooling, interacting with vendors, asset tracking and handling deliveries. This is an office-based role, and requires that you're located in Boston area.

What you'll do
Be responsible for the full hardware lifecycle: ordering, delivery and installation, migration into production, maintenance and eventual decommissioning
Diagnose and repair system errors, infrastructure issues and network connectivity problems
Work to standardize processes, configurations, and procedures in cooperation with engineers in our other data centers
Use your Python development skills to actively contribute to Canonical's MAAS data center automation tooling
Minimize downtime with effective planning and notify relevant stakeholders of scheduled maintenance periods
Improve and extend hardware and network monitoring
Regularly update asset management tool to ensure accuracy and completeness
Update of equipment firmware when appropriate/needed
Who you are
Bachelor's degree, preferably in Computer Science or Software Engineering
Python programming experience
At least 1 year of Linux Administration experience
Mix of Rack and Virtual systems experience
Able to communicate clearly and effectively in English
Strong time management skills
Ability to manage competing priorities
Ability to work with a globally distributed team of passionate engineers
Ability to lift up to 50lbs
Experience working for an internet service provider or large hosting provider is a bonus
CCNA preferred

Canonical is proud to foster a workplace free from discrimination. We truly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better platform for our users and customers. This is something we value deeply and we encourage everyone to come be a part of the world of Ubuntu.

#stack",https://www.indeed.com/rc/clk?jk=80a8d91239adf192&fccid=d40b824c7b625849&vjs=3,"Remote in Manchester, NH+67 locations",Data Center Engineer (Greater Boston Area),Data Engineer
150,NN Tech,"JOB DUTIES AND RESPONSIBILITIES: Design, develop, test, and maintain ETL and integration solutions using the Informatica platformUtilize industry best practices for data warehousing with InformaticaDevelop scalable, reusable ETL jobs using the Informatica platformDevelop, document, and maintain procedures for programs and processesWork with other departments to understand requirements for the creation of data jobsHelp identify data quality issues and work with source systems to addressCoordinate and work with other technical staff to develop coding standards and quality assurance policies and proceduresMaintain change control and testing processes for modifications to ETL processesPerform on-call duties which may include weekends, holidays, and nights as neededProvide technical support as neededAssist end users with issues/questions relating to business intelligence, the data warehouse, and other processes supported by the teamEnsure systems conform to all internal security, compliance, and performance requirementsComplete all required BSA and OFAC training. Report all suspicious activity to SecurityOther duties may be required and assigned by the supervisorJob Types: Full-time, ContractPay: $100,000.00 - $110,000.00 per yearSchedule:8 hour shiftExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/NN-Tech/jobs/Senior-Data-Engineer-bd099626a0665502?fccid=907d8f41df56236a&vjs=3,"Midlothian, VA+1 location",Sr. Data Engineer,Data Engineer
151,"Baanyan Software Services, Inc","Duration:  12+ MLocation:  Dallas, TX (Remote Until Covid)Min 7 Years of Experience as Data EngineerHands on Experience on PythonHands on Experience on SparkJob Types: Full-time, ContractPay: From $50.00 per hourApplication Question(s):Current Work Autorization?Free time for a Quick Call?Education:Bachelor's (Preferred)Experience:Python: 6 years (Preferred)Spark: 4 years (Preferred)Work Location: Remote","https://www.indeed.com/company/Baanyan-Software-Services,-Inc/jobs/Data-Engineer-e9a08a19f1fd3909?fccid=a781b84555c30b72&vjs=3",Remote,Data Engineer,Data Engineer
152,"Koverse, Inc.","Koverse is looking for a Cleared Data Engineer who will be part of a world-class team working to help customers realize the promise of big data. Secret or Top Secret Clearance is required. This role is based in the Denver metro regions.As a Data Engineer you will: Work directly with customers in a technical role to help ensure smooth adoption of the Koverse platform and develop cutting-edge solutions using the Koverse platformSpec, build, test and deploy customized machine learning and/or other mission-focused applications based on customer requirementsParticipate in deployment and implementation planning with customersDirectly assist with installation and setup of the Koverse platformProduce customer documentation to support custom applicationsContribute technical content and tutorials to product documentationDeliver customer trainingWe are looking for the following technical experience: Familiarity with distributed systems like Hadoop, HBase/Accumulo, Spark and KafkaSystems-level understanding of Machine Learning concepts and technologyExperience writing scripts using Linux shell, Python, or similar scripting languageExperience working with Cloud technologies such as Azure, AWS, or GCPExperience working with Linux to install and maintain a distributed computing infrastructureOur BenefitsAside from our supportive culture, Koverse employees enjoy some of the best benefits in tech:Hybrid model - work-from-home + in-office hybrid optionsTeam lunch during on-site daysFlexible schedule to support work-life balanceOccasional travel for company-wide meetings and social eventsMedical, dental and vision insurance: Low employee premiums with multiple levels of coverage availableCompany-paid life insuranceParental leaveEmployee Assistance Program (EAP)Flexible Spending Account (FSA)401(k) retirement planUnlimited personal time offReferral programApply now to join the team!About KoverseKoverse enables organizations to create and use data intelligence. Our technology is trusted by global industry leaders in biopharma, finance, government, professional services, and high tech. Our founders are former NSA data architects who created Apache Accumulo, the fast and scalable data store with cell-level security. Accumulo is the foundation for the Koverse Data Platform (KDP). Koverse is a wholly owned subsidiary of SAIC with hubs in Seattle, Denver, and the Washington, D.C.-Baltimore metro areas.Koverse Commitment to Safety and WellbeingAs part of our commitment to health and safety, Koverse, An SAIC company, is taking a responsible approach to creating environments that allow us to do what we do best by implementing several new and enhanced safety measures. As part of our commitment to health and safety, COVID-19 vaccines are now required for all newly hired employees and applicants.Given the constantly changing global health environment, these practices may evolve as we consider the latest guidance. Koverse will share more information as we look towards the future and the success of our company and the safety and wellbeing of our employees.If an applicant believes there is a need for reasonable accommodation, in order to search for a job opening or apply for a position at Koverse, please email Koverse with the request. This email address is not for general employment inquiries or correspondence.2021 COVID-19 Precaution(s): Remote interview process; virtual meetings.An Equal Opportunity EmployerKoverse, Inc., An SAIC company, is an equal opportunity employer. Applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, or protected veteran status or any other basis prohibited by federal, state or local law.Koverse does not currently sponsor H1B visas and therefore candidates must be legally authorized to work for any employer in the United States on a full-time basis and not require current or future visa sponsorship for employment.Koverse fosters a business culture where ideas and decisions from all people help us grow, innovate, create and be relevant in a rapidly changing world.Job Type: Full-timePay: $125,000.00 - $140,000.00 per yearBenefits:401(k)Dental insuranceHealth insurancePaid time offVision insuranceSchedule:8 hour shiftMonday to FridaySupplemental Pay:Bonus payEducation:Bachelor's (Preferred)Security clearance:Confidential (Preferred)Work Location: One location","https://www.indeed.com/company/Koverse,-Inc./jobs/Data-Engineer-7da033fb2cf6666d?fccid=4b4606931268d6e9&vjs=3","Denver, CO 80202 80202 (Central Business District area)",Data Engineer,Data Engineer
153,Disney Media & Entertainment Distribution,"Movies Anywhere is the next generation of in-home entertainment, providing an unparalleled digital entertainment experience. Leveraging cutting edge technology, unique partnerships, and a talented team, Movies Anywhere is an exclusive, cross-platform, cloud-based movie service that enables consumers to seamlessly discover, grow, access, and enjoy their personal digital movie collection across a variety of studios, retailers, and platforms all in one convenient app and/or website.

Movies Anywhere is looking for a self-starting, data engineer who wants to help shape the way that a major, fortune 100 entertainment company engages with data engineering. You really enjoy building solutions, writing code, running code, and debugging large data systems. You are insulted when someone asks you if you’ve written the unit tests yet, because you wrote them first. You can crank out both simple and complex algorithms in code that is clean, efficient, readable and performant.




Responsibilities :

Leverage your data engineering skills to impact our business by taking ownership of key projects requiring coding and data pipelines
Collaborate with product managers, software engineers and data engineers to design, implement, and deliver successful data solutions
Define technical requirements and implementation details for data solutions
Design, build and optimize performant databases, data models, integrations and ETL pipelines
Maintain detailed documentation of your work and changes to support data quality and governance
Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers
Be an active participant and advocate of agile/scrum practices to ensure health and process improvements for your team

Basic Qualifications :

3+ years of experience designing and delivering large scale, 24/7, mission-critical data pipelines and features using modern big data architectures
2+ years of Scala
2+ years of Spark
2+ years of experience with AWS data ecosystem (S3, EMR, Glue, Athena, lambdas, EKS, etc)
1+ years of experience in ElasticSearch
Deep knowledge in various ETL/ELT tools and concepts, data modeling, SQL, query performance optimization
Experience with building stream processing applications using Kafka
Experience with workflow management tools (Airflow, Oozie, Azkaban, Luigi, etc.)
Comfortable working in K8s/EKS based environments
Comfortable working in Linux environment",https://www.indeed.com/rc/clk?jk=266ddce5f1ccc193&fccid=c3092a91bcb42ca9&vjs=3,"Burbank, CA+16 locations",Data Engineer II,Data Engineer
154,One Medical,"About Us

One Medical is a primary care platform challenging the industry status quo by making quality care more affordable, accessible and enjoyable. But this isn't your average doctor's office. We're on a mission to radically transform healthcare, which means tackling the frustrations of everyone involved — from patients and providers to employers and health networks.

Across the country, our members enjoy seamless access to comprehensive care at more than 180 locations across 28 cities (and counting!) as well as 24/7 access to virtual care powered by intelligent uses of technology. In addition to a direct-to-consumer membership model, we work with more than 7,000 companies to provide One Medical health benefits to their employees.

On January 31, 2020 we marked a milestone with our public listing on Nasdaq, but our work is far from over. As we continue to grow and broaden our impact, we're building a diverse, driven and empathetic team, while working hard to cultivate an environment where everyone can thrive.
The Opportunity

As we continue to expand nationally, the Business and Clinical Intelligence team is seeking a new Data Engineer. You will be responsible for designing and building reliable & performant data models and pipelines to provide actionable insights and accommodate needs from our cross-functional partners and distributed analysts across the organization. You will also utilize expertise in data modeling to facilitate front-end reporting, visualizations, and dashboards to drive insight.

If you like to work with data that is directly affecting business decisions, and you have a passion for doing brand new things with data, then this role is for you. As a key member of our data engineering team, you will work alongside our business analysts, business leaders, and the rest of our engineering team to help transform the way healthcare uses data.

What you'll work on:

Work closely with data scientists and analysts to build complex data pipelines from health and business operations data utilizing Matillion, Python, Airflow, Snowflake, DBT, and AWS
Develop frameworks for connecting to data sources and building ETL tasks
Work on the data pipeline platform to enable Analytics and Data Science team members to quickly and efficiently build reliable pipelines
Consistently evolve data model & data schema based on business and engineering requirements
Build proper data quality detection to identify data issues in the data transformation stages and fix the problems to meet pipelines / table health SLAs
Champion the practice of making data-informed decisions by ensuring internal stakeholders such as product management, operations, marketing, clinical effectiveness, finance and the executive team are making data driven decisions quickly and with confidence

You'll be set up for success if you have:

Minimum 2+ years of expertise in building out data pipelines, ETL design (both implementation and maintenance), and data warehousing
Proficiency in Python (Object oriented programming and Pandas/PySpark libraries experience a plus)
2+ years of experience with SQL
An understanding of business analytics tools such as Tableau and Looker is highly desirable
1+ years of experience with Python
B.S. / M.S. in Computer Science, Math, Statistics, Engineering, Bioinformatics, or other quantitative field

Not required, but would be great if you also had:

Familiarity with Airflow/DBT, Hadoop/EMR, Hive, Presto, PySpark, Pig, and the AWS ecosystem
Healthcare Industry experience

Benefits designed to aid your health and wellness:

Taking care of you today

Paid sabbatical after 5 and 10 years
Employee Assistance Program - Free confidential advice for team members who need help with stress, anxiety, financial planning, and legal issues
Competitive Medical, Dental and Vision plans
Free One Medical memberships for yourself, your friends and family
Pre-Tax commuter benefits
PTO cash outs - Option to cash out up to 40 accrued hours per year

Protecting your future for you and your family

401K match
Opportunity to participate in company equity programs
Credit towards emergency childcare
Company paid maternity and paternity leave
Paid Life Insurance - One Medical pays 100% of the cost of Basic Life Insurance
Disability insurance - One Medical pays 100% of the cost of Short Term and Long Term Disability Insurance


""This is a full-time role based anywhere in the US. You have the option of working remotely or from our San Francisco headquarters, Austin, TX or New York, but most of your coworkers will be remote for now.""

#LI-Remote

#LI-EL1
One Medical is an equal opportunity employer, and we encourage qualified applicants of every background, ability, and life experience to contact us about appropriate employment opportunities.

Subject to applicable law, proof of COVID 19 vaccination is required for employees and contractors who interact with patients, access a shared office space or engage with other team members, except where a medical or religious accommodation applies.",https://www.indeed.com/rc/clk?jk=33a12de712877017&fccid=404f7a47a9668469&vjs=3,"Remote in San Francisco, CA",Data Engineer,Data Engineer
155,"Dynamic Placement Services, LLC","Dynamic’s client is seeking a Data Engineer for a permanent position that’s 100% remote. The candidate will be part of a collaborative 5-person analytics team.NOTE: Candidates must reside in the U.S. and be authorized to work in the U.S. without visa sponsorship. No C2C.Qualifications/Duties:· 5+ years of data analytics and reporting· Data migration· Data warehouse· Build Tableau or Power BI dashboards· Build data pipelines using Azure Databricks· Streamsets· Spark· Pyspark· Azure Data FactoryJob Type: Full-timePay: Up to $120,000.00 per yearSchedule:Monday to FridayApplication Question(s):What is your desired salary?Experience:Azure Databricks: 1 year (Preferred)Work Location: Remote","https://www.indeed.com/company/Dynamic-Placement-Services,-LLC/jobs/Data-Engineer-073a4e2ed84cad43?fccid=b4b5ef3a7571bb88&vjs=3",Remote,Data Engineer (Azure Databricks),Data Engineer
156,Gointechs,"JOB SUMMARYThe team member’s Number One job responsibility is to deliver the most remarkable patient experience, in every dimension, every time, and understands how to contribute to the health system’s vision of achieving that commitment to patients and families. At Novant Health, people are our business. We treat each other with respect and compassion. We embrace the differences in our strengths while fostering an environment of inclusion, empowerment, inspiration and courage. Theteam member will use Novant Health’s First Do No Harm (NHFDNH) safety behaviors/error prevention tools and high reliability strategies as appropriate to ensure a safe, remarkable patient experience.Digital Products and Services team members are responsible for securely managing information systems throughout their lifecycle, including knowing what information systems are within their scope of responsibility, understanding what sensitive data is stored, transmitted, or processed on those information systems, enforcing the security principles of least privilege and least functionality, knowing what events may constitute a cybersecurity incident, and understanding their role in securityincident response activities.The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projectsQUALIFICATIONSEducationEducation Level Required/Preferred Description2 Year / Associate Degree Required Computer Science or related field4 Year / Bachelors Degree Preferred Computer Science or related fieldExperienceYears of Experience Required/Preferred Description5 Required experience in Information Technology1 Preferred experience with at least one contemporary data transformation toolRefer to the Life Support Training Policy NH-HR-3096.Licensure/CertificationType (ex NP, RN, CMA) Required/PreferredAdditional Skills/Requirements (required)Capable of handling multiple tasks with minimal supervision.Possess strong organizational, communication, problem solving/analytical skills.Experienced with standard query language (SQL), knowledge of and experience with Data Warehouse concepts and design.Must be able to build processes supporting data transformations, data structures, metadata, dependency and workload management.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores, and big data pipeline.Strong verbal and written communication skills.Ability to read, understand, and develop various data structures.Able to work independently and take ownership of tasks.Also must be able to work well within a team.Ability to drive/travel to multiple facilities/locations as needed.Ability to be on-call and respond to production issues in a timely manner.Additional Skills/Requirements (preferred)Understanding of Healthcare data is desired.Working knowledge of and experience with ETL tools such as SSIS, Informatica, or Attunity Compose.Working knowledge of and experience with data orchestration tools such as Control-M or ADF.Working knowledge of and experience with one or more of ADLS, ADW, SQL PaaS, Azure Data Bricks,and NoSQL databasesFUNCTIONSEssential FunctionsProcess: Extracts, tests, loads, and validates data. Analyze the data flow requirements and develops a scalable architecture for staging and loading data. Interprets the transformation rules for all target data objects and develops the software components to support the transformation. Assist with design/implementation of data staging methods. Prepare/implement data verification and testing methods. Create scripts/processes required to extract, transform, clean, and move data and metadata from a source application so they can be loaded into a data warehouse, data mart, or operational data store. Must be able to troubleshoot the existing code and develop new code, stored procedures, views, and functions.Documentation: Create and maintain ETL, data orchestration, and system documentation. Create source to target data lineage documentation. Maintain the development, test, user acceptance, and production ETL and data orchestration environments.Compliance: Adheres to the principles of the Novant Health Code of Ethics Compliance Plan in every encounter with customers, vendors, regulating agencies, the public and fellow employees. Adhere to corporate policies, procedures and standards. Follow SDLC standards.Communication: Effective in giving and receiving the information required to perform the job.Job Types: Full-time, ContractPay: $50.00 - $55.00 per hourSchedule:8 hour shiftApplication Question(s):Please share your email and let me know your rate expectation?Experience:Information Technology: 5 years (Preferred)contemporary data transformation tool: 1 year (Preferred)healthcare: 1 year (Preferred)Azure data platform stack: 1 year (Preferred)dimensional modeling and data warehousing: 1 year (Preferred)Spark, SQL, python, Pyspark, shell scripting: 1 year (Preferred)building metadata driven data processing frameworks: 1 year (Preferred)License/Certification:Type (ex NP, RN, CMA) (Preferred)Work Location: One location",https://www.indeed.com/company/Gointechs/jobs/Azure-Data-Engineer-495c477e5014eef4?fccid=f0e20ff6b0a0960e&vjs=3,"Atlanta, GA+2 locations",Azure Data Engineer,Data Engineer
157,Brainjolt,"We're looking for a remote Data Engineer to support our team. The ideal candidate will creatively solve problems in evaluating our current Brainjolt data requirements across all of our departments (including Editorial, Video, and Business Development). They'll be driving growth by optimizing and evolving current processes that will directly impact our company's scalability. They'll be constantly learning about new platform resources and the financial costs associated with them. They'll be radically candid in communicating roadmaps, priorities and processes, across multiple teams. They'll own the process of proactive error detection to uphold the integrity of our data.

Here at Brainjolt, our sole mission is to battle boredom. Across our 25+ brands, we curate and create viral video content (including Snapchat's Bad Parenting Moments) as well as click-worthy articles on our HUGE web properties (including 22Words, 22BestThings, The Indy), with a collective audience size greater than the entire population of Thailand (69.8 million+) across all of Brainjolt's platforms . Furthermore, our growing partnerships with both globally recognizable and up-and-coming Amazon affiliates constantly surprise and delight our audiences with the latest gadgets we know. they'll. love.

Intrinsically, you understand how the internet is rapidly growing and evolving. You'll spend many of your days working multiple languages including Python or SQL in our current environment that includes GCP, BigQuery and AWS. Other days, you'll be contributing your solution-driven attitude towards finding the path of least resistance. While establishing best practices in larger team-driven projects will not fall solely on your shoulders, you will be a critical part of it. If this doesn't sound terrifying, you've likely been here before. And we can't wait to support what you'll do here next…

You Will:

Evaluate and help evolve the data requirements in Brainjolt's processes as well as departments
Proactively detect errors across our data and optimize current processes in order to uphold the integrity of our data
Work in our current environment of SQL and Python that includes GCP, BigQuery and AWS with an eye towards evolving how we function and scale
Monitor usage of platform resources and financial costs associated with them
Have an eye towards the future and scalability as we move forward. You will have an impact in the direction your department and ultimately the company will go.
Take highly technical processes and break them down for others so they are easily digestible
Display an attitude leaning toward solutions with path of least-resistance in mind
Communicate roadmaps, priorities, and process across multiple teams leaning into Radical Candor and a service based approach

You Are Ready To:

Proactively be in-sync with our communication channels on delivery (Slack, Asana, GitHub)
See things from multiple perspectives, understanding the ask of non-technical teammates and providing solutions they didn't know was possible
Pivot between simple projects to larger team-driven solutions for our data needs
Be a thought leader on industry-leading possibilities, happy to teach and educate, both internally and externally
Lift the team by leveling up everyone's skill with visibility over the processes you build & conquer
Switch from player to coach and back on a moments notice, with the ability to work independently and with many stakeholders
Place a heavy emphasis on on best practices rather than half baked temporary solutions

Technical Skill and platforms:

Python
SQL
GCP or AWS
GitHub
Asana or similar project management software
Nice to have:
Javascript
Hadoop/Spark/Docker/Kubernetes",https://www.indeed.com/rc/clk?jk=7e8811aaebc06ff8&fccid=d476e731e4d69a57&vjs=3,"Remote in Los Angeles, CA",Data Engineer,Data Engineer
158,Infomerica inc,"Hi All,Please find the below role and let us know your interest.Data Engineer IIPhysical Location: Princeton, New JerseyDuration : 6+ Months contractJob Description: Job SummaryDesign, develop, test, maintain, and enhance the following systems:o SQL Server 2012 database objects (stored procedures, views, table-valued functions, scalar-valued functions)o SSIS 2012 Packageso SSRS 2012 PackagesMentor and assist other developers in best practices of SQL and software developmentMust possess excellent communication skills as you will interact directly with our stakeholders to determine requirements and present features and modifications.Must have a strong desire to deliver well designed, defect free, fully tested software.Migration to 20xxEssential Duties/ResponsibilitiesFollow the team’s agile development life-cycle best practices.Work independently for design, development, and testing.Collaborate with team members to identify and prioritize work when necessary.Facilitate requirements gathering efforts and clearly document system requirements.Develop strong working relationships with individuals at all levels of the company.Anticipate and adjust for problems and impediments while eliminating impediments where possible.Use appropriate change control procedures for implementing software enhancements.Remain abreast of current technologies and best practices as applicable to NRG.Conduct unit, functional, and integration testing to ensure application reliability.Working ConditionsTeam-based environment in a cubical, or open work area.Duties are performed during standard business hours and overtime based on project requirements.Must be willing to work longer hours as necessary, especially during critical-issue resolution.Will need to attend meetings and respond to application problems for short durations at other corporate sites when necessary.Minimum RequirementsBachelor’s degree in computer science, software engineering, or relevant business discipline, or equivalent work experience.A minimum of 5 years professional database development experience in a production setting working on enterprise applications.Initiative and communication skill to proactively ascertain business rules behind data relationships.Preferred QualificationsThe ideal candidate will have expert-level experience in SQL, particularly Microsoft T-SQL, with demonstrable ability to write complex and performant queries.Experience working with and analyzing very large datasets, in a SOX-controlled, enterprise environment.Agile methodology experience within SDLC frameworkSome experience developing web or desktop applications using high-level languages, such as C# and python.Additional Knowledge, Skills, and AbilitiesExperience writing and maintaining multi-tier applications.Thorough understanding of SQL Server development, including SSIS, SSRS, and query analysis and tuning.Some knowledge of .NET development, with an emphasis on C# .Net core using Entity Framework.Experienced in testing software all the way from unit testing to system testing to integration testing. A strong desire to build well-tested software.Strong knowledge of version control software, git preferred.Microsoft Power BI exposureExperience with cloud computing, preferably AWS (EC2, S3, Lambda, SNS, SQS) is a big plus.Knowledge of event-based, and batch-oriented processes is a plus.Physical RequirementsMinimal. Ability to carry the provided laptop to and from your workspace.If you are sharing the hotlist, please share it in text format in email body. it will help me to search with keywords and call you instantly.ThanksAshwa MohanDelivery Lead- US operations ( Recruitment )919 439 3682 extn 9014""Certified Minority Business Enterprise""""Certified Women Owned Enterprise""Job Types: Full-time, ContractPay: $65.00 per hourSchedule:8 hour shiftExperience:Informatica: 1 year (Preferred)SQL: 9 years (Required)Work Location: One locationSpeak with the employer+91 9194393682",https://www.indeed.com/company/Infomerica-inc/jobs/Senior-Data-Engineer-76b860bb583e0af3?fccid=25d9fab913f02a59&vjs=3,"Princeton, NJ",Sr. Data Engineer,Data Engineer
159,"Torch Technologies, Inc.","Job Description:The successful candidate will assist the 53d Electronic Warfare Group (53 EWG), 53d Wing, at Eglin AFB, Florida. The 53 EWG is the technical focal point for all electronic warfare (EW) support of warfighter systems for the Combat Air Forces (CAF). The mission of the 53 EWG is to develop and test mission data (MD) to defeat enemy radar and infrared guided missile systems, thus enhancing aircrew and aircraft survivability in combat. This mission includes operational EW testing, MD development/validation/verification, force development evaluation execution and facilitating foreign materiel exploitation.The successful candidate will conduct appropriate EW research, MD development and MD testing. The successful candidate will support EW system programming/reprogramming, work with EW system engineers to coordinate programming/reprogramming requirements, prepare validation and verification test plans, and organize and participate in MD configuration control boards. The successful candidate will assist with the collection, recording, and post-test analysis of data generated during MD testing. The successful candidate may be required to travel.Job RequirementsCandidates must have at least a Bachelors' degree in a technical/engineering. They must also have a demonstrated ability to recognize and analyze problems, conduct research, summarize results, and make appropriate recommendations. Applicants must have a working knowledge of computer systems and an understanding of Windows-based personal computers and Microsoft Office software and possess the ability to communicate effectively both orally and written. 'Highly desirable (but not required) attributes include a current active Secret security clearance and previous experience in RF integration, EM spectrum management, antenna design, and digital signal processing. Also desired (but not required) is knowledge of EW weapons systems development, test and evaluation, and systems engineering. Candidates without and active Secret security clearance must be a US Citizen able to obtain and maintain a Secret clearance.Preferred SkillsKnowledge, skills and attributes associated with this position(s) include: EW and the electromagnetic (EM) spectrum, computer software, threat warning, radio frequency (RF) jammers, electro-optical/infrared (EO/IR) jammers, expendables, threat analysis, foreign/US/radar/weapon systems, airborne EW computer software, avionics, systems integration, electronics engineering concepts, principles and practices applicable to a broad range of engineering assignments. Candidates must be analytical, methodical and detail-oriented.This position is located at Eglin Air Force Base, Florida.Job Type: Full-timePay: From $75,000.00 per yearBenefits:401(k)Dental insuranceHealth insurancePaid time offVision insuranceSchedule:Monday to FridayWork Location: One location",https://www.indeed.com/company/Torch-Technologies/jobs/Junior-Entry-Level-Mission-Data-Engineer-d43c26e75e72fe91?fccid=b0df153a019c5ca2&vjs=3,"Eglin AFB, FL 32542 32542",Jr/Entry Level Mission Data Engineer,Data Engineer
160,PepsiCo,"Auto req ID: 267389BR

Job Description
We are PepsiCo
PepsiCo is a global food and beverage leader operating in more than 200 countries and territories with a product portfolio that includes 22 world-famous, billion-dollar brands. From Gatorade to Quaker, LIFEWTR to Lay’s, we make hundreds of enjoyable foods and beverages that are loved throughout the world.

Guiding PepsiCo is our vision to be the global leader in convenient foods and beverages by Winning with Purpose. “Winning with Purpose” reflects our ambition to win sustainably in the marketplace and embed purpose into all aspects of the business.

Our employees drive our culture. No two days are the same. We’re dynamic and full of passionate teams embracing new ideas through our collaborative spirit. At PepsiCo, what makes you unique, makes us better!

Functional Description:
PepsiCo operates in an environment undergoing immense and rapid change, driven by eCommerce and emergent retail technologies. To ensure continued success in the food and beverage space, PepsiCo has assembled a dedicated eCommerce team – tasked with optimizing eCommerce operations and developing innovations that will give PepsiCo a sustainable competitive advantage. While tied closely to broader PepsiCo, the eCommerce group more closely resembles a start-up environment; embracing the core values of having bias for action, being results oriented, maintaining a community-focus, and prioritizing people.

What you can expect:
PepsiCo eCommerce is looking to hire a data engineer to join the Supply Chain Data Engineering Team. The team applies Data Warehouse and Cloud techniques to build data pipelines to provide supply chain data like orders, fulfillment, customer, product from different sources for our business, customer, and Data Science team.
Roles available in the following locations:

San Francisco, CA/ Palo Alto, CA

Note : Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter.
Qualifications/Requirements
What we’re looking for:
Graduate with a BS or MS degree in Computer Science or a related technical field.
Exposure to Python or any other programming languages.
5+ years of experience in Data Engineering.
Good understanding of RDBMS database and SQL language.
Knowledge of ETL/ELT processes.
Knowledge with AWS or any other cloud technology.
Strong verbal and written communication skills.
Good team player.
Good to have:
Knowledge of Web Scraping.
Knowledge of Airflow, Kubernetes, and Docker.
Relocation Eligible: Not Eligible for Relocation
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement",https://www.indeed.com/rc/clk?jk=10214c286592264b&fccid=2973259ddc967948&vjs=3,"San Francisco, CA (Financial District/South Beach area)+5 locations",Ecom Data Engineer,Data Engineer
161,Abine,"Data Engineer
at Abine / DeleteMe (View all jobs) (https://joindeleteme.com/about-us/#careers)
Work from anywhere
About Abine / DeleteMe
Abine is a leading online privacy company. Backed by General Catalyst VC and Atlas VC, it was founded in 2010 and is a growing + profitable SaaS privacy business with multiple subscription privacy tools for consumers and corporations. For more info on our products, please check out Blur (https://abine.com/)and DeleteMe (https://joindeleteme.com/) for yourself. Growth in online privacy is being driven by consumers tired of their data being used without their consent for political ads, doxxing, and ID theft. Advertising by major brands like Apple's privacy campaign help spread awareness. Finally, new global regulations like GDPR, LGPD, and CCPA are giving consumers new privacy protections and forcing businesses to think proactively about data privacy.
About The Role
Abine / DeleteMe is looking for a Data Engineer to help us manage and use data to make good decisions about what we build, how we help our customers, and how we grow our company. You will work closely with our engineering and product teams to improve the way we collect, structure, and store data, and with our management, marketing, sales, and service teams to understand what they need to know about the business and get them the data they need to help them make good decisions.
What You'll Do
Analyze the data we have and the data we create to better understand its value
Improve our data collection and storage practices to enhance the value of our internal data

Build pipelines and data flows to ensure our data is available where it is needed when it is needed

Talk to everyone in our business who needs to make good decisions with our data, and design systems that will get them the data that will help them do so

Design and implement a strategy and systems for data reporting, querying, and visualization to enable everyone in the business to explore our data, answer their questions, and make good decisions
Qualifications
You don't need a specific background for this role. We're looking for someone with strong technical and people skills, with some experience in data engineering, who wants to be a big part of helping a team achieve big things, who believes in privacy as a mission, and can both operate independently and help a team be more than the sum of its parts.
Experience with SQL
Experience with database design and implementation
Experience with appropriate programming languages and technologies
Experience with data reporting and visualization tools
Excellent written and verbal communication skills
A Bachelor's or more advanced degree is great, but not required
A Computer Science or other STEM degree is great, but not required
Benefits:
Medical, Dental, Vision, FSA, HSA, Commuter, Life and Disability Benefits
Flexible work hours
Great compensation package (salary, equity)
Meeting-light culture
Hiring Process:
The Abine hiring process typically takes 30-60 days from the initial job posting to complete. Our process is designed to get to know you as a person and professional. It is also designed to get a feel for how you might perform in the targeted role along with helping you to get a sense of what it might be like to work with our team. Please note that we may also consider you for other current and future positions we may be hiring for or have available. We aim to communicate where you stand in our hiring process upon the completion of each of our steps.",https://www.indeed.com/rc/clk?jk=9b006e3cdd568343&fccid=1fac92fe70fe20c2&vjs=3,"Remote in Boston, MA 02210 02210",Data Engineer,Data Engineer
162,"Otter Products, LLC","Overview:

Otter Products is currently recruiting for a Data Engineer to join our Data Management Team! This position can be based in one of our US office locations (Fort Collins, CO or San Diego, CA) or can be based 100% remotely in the US.
As a Data Engineer, you will focus on the complete analysis, design and development of BI solutions using Microsoft’s Business Intelligence Stack(Power BI and SQL Server). This role will partner with our global Business Team to determine end user database / reporting needs and requirements and turn those into actionable reports. Intrigued? Keep reading…
Our team is working on modernizing our data solutions and we need your help! As we build out a cloud solution for our data warehouse you will have the opportunity to leverage Azure for large scale data analyses, model development, model validation and model implementation. Are you an innovator with the ability to troubleshoot BI tools, systems, and software? If so, Otter Products may be a great place to grow your career.

About Otter Products: From our founder’s garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation. Our industry-leading brands, OtterBox, LifeProof and Liviri lead the way to help people do more and go more places with technology in hand. Otter Products is a global company of more than 900 Otters (employees) with offices in Fort Collins, Colo., San Diego, Calif., Hong Kong, Munich, Germany and Cork, Ireland. Our mission is simple – We Grow to Give which comes to life by way of our charitable arm, the OtterCares Foundation. From innovation and engineering to community and culture – growing to give is at the heart of everything we do. For more information visit otterproducts.com. Responsibilities:
Complete analysis, design and development of BI solutions using Azure SQL Server
Familiarity with Data Warehouse concepts
Experience coding ETL/ELT processes
Database development primarily in SSIS, Data Factory and SQL
Partner with the business to determine end user database/reporting needs and requirements
Generate ad hoc reports using Power BI or SSRS
Collaborate with other developers to create and implement best approach solution(s)
Troubleshoot Azure tools, systems, and software
Review queries for performance issues, making changes as needed
Collaborate with team to performance-tune Azure application as necessary
Assist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives
Participate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation
Support and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment
Other duties as required
Qualifications:
Bachelor’s degree required. Degree in Computer Science or Mathematics preferred. Experience in lieu of degree may be considered.
Minimum of two years of experience in an IT or analytical role required.
Experience in database development, report writing and/or statistics preferred.
Experience with Microsoft Stack, Azure and/or Power BI Preferred
#LI-Remote
EEO: Otter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law. For US Based Roles Only - Compensation Range Minimum: USD $80,000.00/Yr. For US Based Roles Only - Compensation Range Maximum: USD $109,000.00/Yr. Additional Total Rewards: Sign on bonus may be considered, if applicable, for relocation, Profit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info",https://www.indeed.com/rc/clk?jk=a818109215ee3036&fccid=959413ad3f4de450&vjs=3,Remote,Data Engineer,Data Engineer
163,Ryzen Solutions,"In this role, you will fuel perception system’s verification & validation efforts through mass simulation and data analysis. You will work with cross-functional teams to mine driving data, create and integrate statistical error models of the real world perception systems, automate the pipeline of error injected simulations, and draw conclusions from simulation. Your work will help identify areas for improvement in the perception stack to accelerate the development of software driving on public roads.Responsibilities: - Build and integrate statistical error models based on mined data- Automate the perception error integration pipeline- Simulate large-scale driving scenarios with errors learned by statistical models- Develop tools to quantitatively measure the AI stack performanceQualifications:- Python proficiency with an emphasis on pandas and numpy- Solid statistics background- Experience with automating workflows building software pipelines- Familiarity with the best practices, code reviews, and code quality of software companies including industry tools like GitHub and Reviewable- Prefer experience with debugging skills (ie debugging large ensembles of worker runs)Bonus qualifications:- Familiarity with perception, prediction and motion planning- Experience with complex system modeling, fault injection, and error analysis - autonomous vehicle domain preferred- Experience with C++ and protobufs- Experience with computing clusters or AWS/cloud computing- Experience with software verificationJob Types: Full-time, ContractPay: $40.00 - $60.00 per hourBenefits:401(k)Dental insuranceHealth insuranceVision insuranceSchedule:Monday to FridayExperience:Python: 5 years (Required)NumPy: 5 years (Required)Pandas: 6 years (Required)Work Location: One location",https://www.indeed.com/company/Northbound-LLC/jobs/Data-Engineer-15bf87c34c4fcd7a?fccid=f60f0f1edb9350c6&vjs=3,"Foster City, CA 94404 94404",Data Engineer,Data Engineer
164,KAGR LLC,"SUMMARY:The Data Engineer 1 will join a fun, dynamic team to help solve integration and data problems relating to sports and entertainment. This role will integrate data from various systems and also improve and expand the existing data processes as needed. The right candidate will be motivated to learn, contribute to the team and organization, and grow along with the business.DUTIES AND RESPONSIBILITIESData IntegrationUsing cloud technology, combine data from various sources, cloud and on-premise, based on requirementsPerform data cleansing and standardizationLoad data into a cloud data warehouse as projects dictateBusiness Intelligence & Data AnalysisAssist with preparing and loading data for Analysis and BI reports and dashboardsIdentify opportunities for new data sourcesCollaborate with analysts to come up with creative solutions to data challengesOngoing ResponsibilitiesImport and integrate new data sources based on business needProactively identify potential data problems, but react as needed to unexpected issuesImprove existing processes to streamline effortsHandle multiple projects and meet deadlinesMonitor, schedule, and maintain existing integrationsSpecial projects and assignments as business dictatesResponsible for the maintenance, creation and control of all personally identifiable information or any other information protected by any Confidentiality or Privacy Standards or Company policies that you have access or knowledge of, including but not limited to any state or federal regulations including HIPAASUPERVISORY RESPONSIBILITIESThis position has no supervisory responsibilities.SKILLS AND QUALIFICATIONSBachelors degree in Information Systems, Computer Science, or related field1-2 years of experience working with data using SQL or similar technologyVery high attention to detailFamiliarity with a data integration platform, such as Snaplogic, SSIS, or InformaticaFamiliarity with BI Visualization tools, such as TableauAbility to work on multiple projects in a fast-paced environmentStrong communication skills to all levels of technical expertisePHYSICAL DEMANDSSitting for extended periods of timeDexterity of hands and fingers to operate a computer keyboard, mouse, and other computing equipmentThe employee frequently is required to talk or hearThe employee is occasionally required to reach with hands and armsSpecific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focusReasonable accommodations may be made to enable individuals with disabilities to perform the essential functionsWORK ENVIRONMENTThe noise level in the work environment is usually moderateFast paced office environmentAbility to work nights and weekends as business dictatesCERTIFICATES, LICENSES, REGISTRATIONSNone requiredOTHER DUTIESPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.This company is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.Job Type: Full-timePay: From $70,000.00 per yearSchedule:Monday to FridayExperience:SQL: 1 year (Preferred)Informatica: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/rc/clk?cmp=New-England-Revolution%2F-Kraft-Soccer&ti=Data+Engineer&jk=b880da7cf8854adb&fccid=8eb0183f23720551&vjs=3,"Foxborough, MA 02035 02035+1 location",Data Engineer 1,Data Engineer
165,DSMH LLC,"Education Requirements:Minimum BS in information technology, computer science, computer engineering, software engineering, applied math, statistics etc. 5-7 years of experience is required. MS is preferred. 3-5 years of experience is required.Technical SkillsFamiliarity with database such as Snowflake, DB2, SQL Server, Oracle (2-3 of these are required)  * Programming languages - SQL(required), Python(required) and SAS(preferred)  * Experience working with platform integration tool like Snaplogic is preferred  * Experience working with AWS (required)Soft SkillsCommunication, Team-work, Problem Solving, Customer FocusDuration-12 monthsLocation-Peoria IL 61630Job Types: Full-time, ContractPay: $65.00 - $70.00 per hourSchedule:Monday to FridayExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)database-Snowflake,DB2,SQL- Server,Oracle,SQL,Python,SAS: 7 years (Preferred)Work Location: One location",https://www.indeed.com/company/DSMH-LLC/jobs/Data-Engineer-Database-Snowflake-2079763c195b283b?fccid=cec1996c9bfed425&vjs=3,"Peoria, IL 61630 61630","Data Engineer-database-Snowflake,DB2,SQL-Server,Oracle,SQL,Python,SAS-8479",Data Engineer
166,Brainjolt,"We're looking for a remote Data Engineer to support our team. The ideal candidate will creatively solve problems in evaluating our current Brainjolt data requirements across all of our departments (including Editorial, Video, and Business Development). They'll be driving growth by optimizing and evolving current processes that will directly impact our company's scalability. They'll be constantly learning about new platform resources and the financial costs associated with them. They'll be radically candid in communicating roadmaps, priorities and processes, across multiple teams. They'll own the process of proactive error detection to uphold the integrity of our data.

Here at Brainjolt, our sole mission is to battle boredom. Across our 25+ brands, we curate and create viral video content (including Snapchat's Bad Parenting Moments) as well as click-worthy articles on our HUGE web properties (including 22Words, 22BestThings, The Indy), with a collective audience size greater than the entire population of Thailand (69.8 million+) across all of Brainjolt's platforms . Furthermore, our growing partnerships with both globally recognizable and up-and-coming Amazon affiliates constantly surprise and delight our audiences with the latest gadgets we know. they'll. love.

Intrinsically, you understand how the internet is rapidly growing and evolving. You'll spend many of your days working multiple languages including Python or SQL in our current environment that includes GCP, BigQuery and AWS. Other days, you'll be contributing your solution-driven attitude towards finding the path of least resistance. While establishing best practices in larger team-driven projects will not fall solely on your shoulders, you will be a critical part of it. If this doesn't sound terrifying, you've likely been here before. And we can't wait to support what you'll do here next…

You Will:

Evaluate and help evolve the data requirements in Brainjolt's processes as well as departments
Proactively detect errors across our data and optimize current processes in order to uphold the integrity of our data
Work in our current environment of SQL and Python that includes GCP, BigQuery and AWS with an eye towards evolving how we function and scale
Monitor usage of platform resources and financial costs associated with them
Have an eye towards the future and scalability as we move forward. You will have an impact in the direction your department and ultimately the company will go.
Take highly technical processes and break them down for others so they are easily digestible
Display an attitude leaning toward solutions with path of least-resistance in mind
Communicate roadmaps, priorities, and process across multiple teams leaning into Radical Candor and a service based approach

You Are Ready To:

Proactively be in-sync with our communication channels on delivery (Slack, Asana, GitHub)
See things from multiple perspectives, understanding the ask of non-technical teammates and providing solutions they didn't know was possible
Pivot between simple projects to larger team-driven solutions for our data needs
Be a thought leader on industry-leading possibilities, happy to teach and educate, both internally and externally
Lift the team by leveling up everyone's skill with visibility over the processes you build & conquer
Switch from player to coach and back on a moments notice, with the ability to work independently and with many stakeholders
Place a heavy emphasis on on best practices rather than half baked temporary solutions

Technical Skill and platforms:

Python
SQL
GCP or AWS
GitHub
Asana or similar project management software
Nice to have:
Javascript
Hadoop/Spark/Docker/Kubernetes",https://www.indeed.com/rc/clk?jk=7e8811aaebc06ff8&fccid=d476e731e4d69a57&vjs=3,"Remote in Los Angeles, CA",Data Engineer,Data Engineer
167,bioMérieux sa,"Description
At BioFire Diagnostics, LLC. we make the world a healthier place by providing molecular solutions that lessen the time to medical results and empower healthcare professionals to make better diagnostic decisions.


We are proud to be part of the bioMérieux family and the 5,000+ team members across 12 sites and subsidiaries in North America committed to our mission to help save lives around the globe through the power of diagnostics.

BioFire Diagnostics, LLC. is a fast growing and profitable biotech company with a great opportunity for a Software Engineer that is interest in Data Engineering and Software Development. The candidate will be joining a team of DevOps engineers to support research and development activities at BioFire Diagnostics. The candidate must have strong analytic, programming and database skills. The candidate will be embedded in the software department but will have close collaboration with multiple departments at BioFire Diagnostics. Therefore, the candidate must have strong communication skills and desire to work in a highly collaborative environment.
Description:
BioFire Diagnostics, LLC. is a fast growing and profitable biotech company with a great opportunity for a Software Engineer that is interest in data engineering and software development. This engineer will be joining a team of DevOps engineers to support the research and development at BioFire Diagnostics. The candidate will be responsible for:
Working with our Data Analytics, Data Science and BioMath groups to gather user needs.
Collaborating with Software Development and DevOps to design, development and implementation of solutions to aggregate disparate internal data sources.
Collaborating with the analysis and software engineering teams to implement data analysis pipelines.
Collaborating with Data Scientists, Data Architects, Software Developers and DevOps on creating processes and tools for testing and validation of data and pipelines.
Collaborating with Software Development and DevOps to optimize, monitor, maintain and update data solutions (both underlying data structure and delivery method) for use by analysis teams and reporting systems.
The candidate must have strong analytic, programming and database skills. The candidate will be embedded in the software department but will have close collaboration with multiple departments at BioFire Diagnostics. Therefore, the candidate must have strong communication skills and desire to work in a highly collaborative environment.
We are looking for highly motivated individuals with the following:
A minimum of a B.S. in a quantitative discipline: Computer Science, Software Engineering, Computer Engineering, Information Systems, Statistics, Mathematics, Engineering, Data Science, Bioinformatics/Computational Biology, etc.
0+ years industry experience in software development, IT, IS or related fields
Excellent programming and problem-solving skills
Development experience with relational database structures, multi-dimensional databases and data warehouse design and architecture
Familiar with cloud hosted solutions
Good communication, writing and presentation skills",https://www.indeed.com/rc/clk?jk=081ff2ffc28176c6&fccid=2ae8e6ca0e3dc338&vjs=3,"Salt Lake City, UT","Software Engineer I, II or III - Data Engineer",Data Engineer
168,Madison Logic,"About Madison Logic:
Our team is reshaping B2B marketing and having fun in the process! As a truly global company, we take pride in the diverse backgrounds of our team. When joining Madison Logic, you are committing to giving 100% and always striving for more. Work with & learn from an incredible group of people who care about your success as much as they care about their own. Our team is at the heart of what we do and our success starts with you!
Remote work note: Please refer to the job posting detail to determine what (if any) remote work options apply to the specific job advertised. Not all positions are available in all countries/regions. Where applicable, remote work must be conducted from your home office located in a jurisdiction in which Madison Logic has the legal right to operate. It requires availability and responsiveness on a full-time basis from a distraction free environment with access to high-speed internet. Please inquire for more details.United Kingdom: Remote Work is available to all applicants residing & authorized to work in the UKUnited States: Remote Work is available to applicants who reside in, and are authorized to work from, the following states: Arizona, California, Colorado, Connecticut, Florida, Georgia, Illinois, Massachusetts, New York, Oregon, Texas, Utah, Virginia, Washington State.
Singapore: Remote Work is available to all applicants residing & authorized to work in Singapore
We are seeking a Data Engineer with a highly analytical mind who is skilled at understanding data and is able to translate it into actionable insights. In this role you will develop easy to understand reports, dashboards, and tools with the aim of optimizing and streamlining the way data is viewed. In this highly collaborative role, you will work closely with the architecture and data teams to reach business objectives.
What You'll Do:
Use SQL/Python to create reports, dashboards, and visualizations.
Aggregate/Model data and use that data to build reports in Domo
Analyze data to help improve business performance.
Identify the best data sources for a given analysis.
Develop processes for data mining, data modeling, and data production.
Offer insights and recommendations to improve data reliability and quality.
What You Have:
Bachelor’s degree in computer science, statistics or mathematics
5+ year’s experience with Python or Node.js
5+ year’s experience with SQL or mySQL
5+ year’s experience with cloud computing services (AWS)
Working understanding of big data analytics
Experience working with data cleaning and standardizing process
Benefits & Perks:
Opportunities for Advancement – As We Grow, You Grow!
Competitive Benefits including Medical, Dental, Vision, and FSA plans
Employer-paid Life, AD&D and STD insurance
401k with Company Match
Generous Paid Time Off including: 17 Vacation Days (to start!), Sick Time, Floating Holidays and Parental Leave
2 Paid Volunteer Days
""You-Do-You"" Monthly Cash Stipend
Work from Home Stipend
Legal & Financial Services Benefits
Wellness initiatives
An innovative, energetic culture and a fantastic team!
Expected Salary (Dependent Upon Experience & Location):
Base Salary Range: $110,000-$115,000
Additional Compensation: $7,500 - $10,000
Pay Transparency:
We are committed to paying our team equitably for their work, commensurate with their individual skills and experience. Salary Range and additional compensation, including discretionary bonuses and incentive pay, are determined by a rigorous review process taking into account the experience, education, certifications and skills required for the specific role, equity with similarly situated team members, as well as employer-verified region-specific market data provided by an independent 3rd party partner.
We will provide more information about our perks & benefits upon request.

Who We Are:
 Our Vision: We empower B2B organizations globally to convert their best accounts faster

Our Values: #TEAM #OWNIT #RESPECT #EXCEL #EMPOWER
Our Commitment to Diversity & Inclusion:
Madison Logic is proud to be an equal opportunity employer. We are committed to equal employment opportunity regardless of sex, race, color, religion, national origin, sexual orientation, age, marital status, disability, gender identity or Veteran status.
Privacy Disclosure:
All of the information collected in this form and/or by your application by submission of your online profile is necessary and relevant to the performance of the job applied for. We will process the information provided by you in this form, your CV (including physical and online resume profiles), by the referees you have noted, and by the educational institutions with whom we may undertake to verify your qualifications with, in accordance with our privacy policy and for recruitment purposes only.
For more information on how we process the information you have provided including relevant lawful bases (where relevant) please see our privacy policy which is available on our website (https://www.madisonlogic.com/privacy/).",https://www.indeed.com/rc/clk?jk=d124afc50eb4b11e&fccid=b812544c2237ac63&vjs=3,Remote,Data Engineer,Data Engineer
169,NN Tech,"JOB DUTIES AND RESPONSIBILITIES: Design, develop, test, and maintain ETL and integration solutions using the Informatica platformUtilize industry best practices for data warehousing with InformaticaDevelop scalable, reusable ETL jobs using the Informatica platformDevelop, document, and maintain procedures for programs and processesWork with other departments to understand requirements for the creation of data jobsHelp identify data quality issues and work with source systems to addressCoordinate and work with other technical staff to develop coding standards and quality assurance policies and proceduresMaintain change control and testing processes for modifications to ETL processesPerform on-call duties which may include weekends, holidays, and nights as neededProvide technical support as neededAssist end users with issues/questions relating to business intelligence, the data warehouse, and other processes supported by the teamEnsure systems conform to all internal security, compliance, and performance requirementsComplete all required BSA and OFAC training. Report all suspicious activity to SecurityOther duties may be required and assigned by the supervisorJob Types: Full-time, ContractPay: $100,000.00 - $110,000.00 per yearSchedule:8 hour shiftExperience:Informatica: 1 year (Preferred)SQL: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/NN-Tech/jobs/Senior-Data-Engineer-bd099626a0665502?fccid=907d8f41df56236a&vjs=3,"Midlothian, VA+1 location",Sr. Data Engineer,Data Engineer
170,Acunor,"Job Title: Data EngineerLocation: San Jose, CA Duration: FulltimeJob Description: We are looking for a Data Engineer who is an SME in Data Warehouse and BI with advanced knowledge of SQL, BigQuery, and Python. He/she will play a critical role in the development of our client's Enterprise Data and Analytics Platforms. This role will be responsible for expanding, optimizing, and monitoring our expanding data pipelines through thoughtful architecting, astute business logic, consistent data governance/testing, and continuous delivery.Skills: Experience in building & managing large databases.7/8+ years of experience in SQLKnowledge in BigQuery, Python, building data pipeline from third party API using JSON, etcHaving knowledge of Java/C++ is an advantageExperience in GCPCore Responsibilities: Responsible for building standards across data table, storage, naming & access controls for the teamResponsible for building data pipelines from third-party APIsResponsible for monitoring of data warehouse, support & debuggingResponsible reviewing codes, version controls & change list approvals & privacy reviewsWork with multiple data Engineering teams to ensure right signals are being populatedResponsible for documentation of the infrastructure design & applicationsResponsible to build thumb rules & monitoring mechanisms to ensure that team’s data tables are within the boundaries of legal & privacy policiesResponsible for building code optimization standardsResponsible for certification of all the data infrastructure assets with the certification council.Job Type: Full-timePay: $120,000.00 - $160,000.00 per yearSchedule:8 hour shiftExperience:SQL: 1 year (Preferred)Informatica: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Acunor-Infotech-LLC/jobs/Data-Engineer-ed01880594065e9f?fccid=f521a2fde61a6b30&vjs=3,"San Jose, CA 95113 95113 (Downtown area)",Data Engineer,Data Engineer
171,Volto Consulting,"US Citizen /Green Card Holder/ EAD onlyRequired Qualifications5 + Years’ hands on experience in data engineeringExperience working with Databricks and Apache Spark/PySpark * Experience with cloud-based data services, including data pipeline orchestration tooling (i.e. Azure Data Factory).Proficiency with complex SQL developmentExperience in modern DevOps practices (including Git, CI/CD)Experience in Data Modeling.Strong business acumen and adaptability to partner with the AI/ML product teams on innovative solutions to constantly changing business requirementsAny ETL experience in design, mapping and configuration in a complex environment processing large volumes of dataJob Types: Full-time, ContractPay: $50.00 - $65.00 per hourSchedule:8 hour shiftApplication Question(s):Kindly mention your Visa statusExperience:SQL: 5 years (Preferred)Data modeling and Data bricks: 5 years (Preferred)CI/CD: 5 years (Preferred)Spark: 5 years (Preferred)ETL: 5 years (Preferred)Data engineering: 5 years (Preferred)Work Location: Remote",https://www.indeed.com/company/Volto-Consulting/jobs/Data-Engineer-72364d5e5aca3cdc?fccid=242f54a0bd5d5b7f&vjs=3,Remote,Data Engineer,Data Engineer
172,Nexient,"About your future team

Nexient is on a mission: To rid the world of crappy software, one sprint at a time.

Every day, our 1000+ Agile developers, designers, and strategists empower each other to find passion in our work while we innovate the world. This culture of teamwork and curiosity is fueled by a product-minded approach to our work, crafting extraordinary custom software solutions and growing careers along the way.

Our clients range from some of America’s favorite brands in retail, healthcare, financial services to fast-emerging disruptors – You might have read about us in the NY Times. We’re also recognized as a Gartner Cool Vendor, HFS Hot Vendor, and America’s leading 100% U.S. Agile software services partner.

If you’re looking to change the trajectory of your career, make an impact on Day One, develop custom software on the best technology around – and maybe even make people’s lives easier - then you belong here, with us.
Required Skills and Experience:
Solid understanding of machine learning fundamentals
Optimization (gradient decent variants, hyperparameter tuning, regularization)
Experience in a variety of problems and domains, with depth in at least four (supervised, unsupervised, generative, control/RL, anomaly detection, timeseries, etc.)
Expert with traditional ML models (k-means, KNN, decision trees, SVM, Bayesian/graphical models, Gaussian process, etc.)
Mastery of Deep Learning fundamentals (CNN, RNN, attention/memory, Autoregressive) and extensions (Transformer, LSTM, ResNet, etc.)
Experience coding with popular frameworks
Python expertise (data structures, coding patterns) along with common ML, data, math and visualization libraries/platforms
Can architect large scale machine learning projects, while supporting team and client related activities (documentation, mentoring, planning, etc.)
Ability to set best practices for modeling/development
Configuring and working in different coding environments (local, notebooks, containers) and using standard software engineering workflows (testing, code management/Git, CI/CD)
Loading and saving large data sets and models efficiently
Customizing models and optimizing performance
Structuring code to make it modular and deployable (logging and metrics, adhering to common deployment interfaces/hooks)
Ability to analyze and explain papers/theory and adapt associated models
NLP specialization (NLU, NLI, QA, BERT, GPT, T5, XLNet, etc.)
Some MLOps and cloud environment experience (resource configuration, optimization)
Solid data engineering skills (large scale data processing in batch or stream, security and configuration best practices)
Why Nexient

As a vital member of our Engineering Practice, you will be part of one of our 100+ small cross-functional teams working side by side with some of the most talented developers, UX designers, analysts, quality engineers, and product managers out there.

From Silicon Valley to Ann Arbor, Columbus, and beyond – a career with Nexient will offer you:

Remote, hybrid, or onsite opportunities: Here, you decide the work arrangement that best suits your needs and preferences.

Fulfilling and challenging projects: You will be exposed to different scenarios, multiple types of clients, and the most complex business and technology initiatives.

Access to the most cutting-edge technologies: You’re always going to be advancing your skillset by learning and working with the newest and most relevant technologies available out there.

A positive and passionate culture: Boring and divided aren’t in our vocabulary. In fact, it’s amazing to see how, despite our exponential growth, the Nexient culture and community stay as warm, diverse, and close as ever.

Plenty of opportunities for you to explore and grow: We will empower you to dream as big as you want and support you to make your career what YOU want it to be.

Creative runway: We take pride in encouraging our people to question the status quo.

Our benefits program includes:
Medical, dental, and vision coverageGenerous PTO, plus holidays401(k) planLife insurance and short-term & long-term disabilityMaternity leaveEmployee Assistance ProgramFlexible spending accountsHealth savings account

Nexient is an inclusive, equal opportunity employer. At Nexient, your uniqueness makes us better. Your uniqueness is why you are valued, you are heard, you add to our culture, and you belong. Diversity & Inclusion is hardcoded into our DNA and at the core of everything we do. Together, not separately, we strive to become even more diverse to help fuel our innovation and connect us closer with the clients and communities we serve.",https://www.indeed.com/rc/clk?jk=6b64b3e647a4e41e&fccid=a2428e4814a1eff5&vjs=3,"Hybrid remote in Ann Arbor, MI 48108 48108",Senior Machine Learning Engineer/Data Scientist,Data Engineer
173,EarnUp,"The time is right for financial products to do better so you can make the most of your income. EarnUp is on a mission to create a financial system that works for everyone by transforming the loan payments ecosystem into a key driver for achieving financial wellness. We do this through our customer centric and data driven payments platform. Our platform offers flexibility in payment schedules while intelligently automating mortgage payments to help our users pay off loans faster. We are a dedicated and diverse team of hardworking innovators working to help people get out of debt. Come help us disrupt the $20 trillion debt market we have today and let's make a difference together.We are looking for a Data Engineer to join the data platform team. You will develop the next-generation data pipeline and data platform. You have a passion for delivering robust software systems and are driven to deliver incredible customer experiences.You WillDesign, build, manage, maintain, and monitor our data pipeline and data platform, including our analytics data lake and data warehouseConfigure and optimize services for performance and scaleAutomate data quality and system health checksBuild the infrastructure required for data extraction, transformation, and loading of data from a variety of data sources using SQL, data pipelines, etc.Build an analytics platform that utilizes the data infrastructure to provide actionable insights key business performance metrics and help drive data scienceDevelop the next generation data platform by combining the data from our transaction systems with data from other data platforms to create unique data modelsWhat You Bring3+ years professional experienceExpert in SQLExperience with Python or similar programming languageExperience of AWS infrastructure, cloud data platformsExperience of data pipelines and ETL ToolsExperience with Airflow / DBTExperience building data platforms for multi-tenant reporting & analyticsExperience defining a robust semantic model for ad-hoc reportingUnderstanding of data ingestion techniques like streaming or batch processingPassionate about data quality, testing and automationExperience with data cleansing and repairExperience with data security and protecting sensitive dataAbility to communicate clearly, both verbally and writtenExperience with financial applications preferredWhy You Will Love Working at EarnUpFast-paced, innovative and collaborative startup dedicated to providing opportunities to learn and growAn opportunity to help financially improve the lives of 20 million Americans living in debtCompetitive salary and meaningful equityFamily friendly benefits including generous paid parental leave and a company contribution toward dependent careFlexible hybrid work model (we are planning to return to the office this spring on a flexible schedule)Convenient SF HQ location minutes away from the Montgomery Bart Station100% paid employee health benefitsFlexible time off programs with no maximum and additional programs dedicated to self careCommuter and work from home benefits including a mobile phone stipendEmployee Resource Groups and active DEI Advisory CouncilFun events, celebrations, offsites and foodDiversity is in our DNA. At EarnUp, diversity is not only accepted and respected, it's celebrated and it is critical to our success. We foster an environment for exceptional people which includes a sense of belonging for all.EarnUp is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.**EarnUp is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations@EarnUp.comJob Type: Full-time",https://www.indeed.com/company/EarnUp/jobs/Data-Engineer-2bfef268008db778?fccid=e96cde2fd3cbc9b5&vjs=3,"Hybrid remote in San Francisco, CA+1 location",Data Engineer,Data Engineer
174,US Cellular,"-
ENG001839

The Data and Application Engineer position will lead and engineer the strategic direction for key application(s) that will result in the capability to mine and extrapolate big data and reporting from an Access Engineering data lake. The position is fully responsible for and will define and execute the strategic plan, requirements, development, integration, migration and testing strategy as well as deploy and maintain fully integrated systems that support the current and future state of Access Engineering. A key output of the position is action and metric oriented reporting. The position will ensure compliance with UScellular users base, contractor users, and extended users (outside users) authorized to read, write, and alter data in the system; overseeing the data governance and systems application change management policy. This is a unifying role within Access Engineering and to ensure success the position will work with business partners, peer engineering teams and vendors on an ongoing basis to navigate complex issues and offer systematic solutions.

Utilize Engineering Processes to define the strategic direction for key application(s) that support the current and future state of Access Engineering.
Lead and engineer the roadmap for key application(s) that will result in the capability to mine and extrapolate big data and reporting from an Access Engineering data lake.
Define and execute the requirements, development, integration, migration and testing strategy as well as deploy and maintain fully integrated systems that support the strategic plan for current and future state of Access Engineering. Define and produce action and metric oriented reporting.
Ensure compliance with UScellular users base, contractor users, and extended users (outside users) authorized to read, write, and alter data in the system; overseeing the data governance and systems application change management policy.
Develop and publish user-based systems configuration guides, training libraries, reports and data governance/compliance standards.
Facilitate user group compliance reviews of technical guidance and direction to UScellular users and vendor personnel in the proper use and configuration of the application(s), reporting and data cleanness routines.

Bachelor of Science degree in Engineering, MIS or related field or equivalent work or military experience.
Minimum five (5) years' progressively responsible work experience related to big data/data lakes.
Minimum five (5) years experience with OneVision.
Highly skilled in technical writing and ability to define, design and implement strategic visions and technical operational support processes.
Strong analytical, problem solving and negotiation skills.
Intermediate to extensive experience with MS Office and related tools.
Demonstrated high level of interpersonal and communication skills.
Ability to effectively present to Senior Leadership.
Ability to create an environment that motivates, inspires and respects others.
Ability to build team cohesiveness to achieve results.
Highly skilled in organization and planning.
Ability to self-start and be versatile in a dynamic environment.
Ability to promote a shared vision and customer focus.

Job

:

Engineering

Location(s)
:

Illinois-SCHAUMBURG_IL

Wisconsin-MADISON_WI, Iowa-CEDAR RAPIDS_IA, Illinois-CHICAGO_IL, Tennessee-KNOXVILLE_TN

U.S. Cellular® is an EEO employer and gives consideration to qualified applicants without regard to race/color/age/religion/sex/sexual orientation/gender identity/national origin/disability/veteran status, pregnancy or genetic information.",https://www.indeed.com/rc/clk?jk=2f07d3a258424506&fccid=e2590cd2d3946660&vjs=3,"Schaumburg, IL",Data and Application Engineer,Data Engineer
175,Chapter Medicare,"Why We Exist
Every morning, 10,000 Americans wake up and begin their first day of retirement. Chapter is re-inventing the way that Americans transition into retirement, starting with Medicare.
For most people, Medicare is boring, bureaucratic, and confusing. But it's important. If people wait too long to choose coverage, they risk life-time penalties from Uncle Sam, may need to undergo medical underwriting, or may pay out-of-pocket for 20%+ of medical costs plus prescriptions and most costs of dental, vision, and hearing.

What We Do
Our team and technology help retirees to navigate Medicare, including when and how to sign up, what specific coverage to choose, and how to maximize the benefits from their coverage.
Chapter has built industry-leading technology to help retirees save thousands of dollars on their healthcare. Our promise is simple: we want people to improve their health coverage while reducing what they pay for it.
How? Our platform searches every Medicare option available nationwide, while others search only a subset of plans, which is the norm in the Medicare market.
This full search is why we find savings and benefits that others miss.
Our Team
Our team is high-integrity and high-horsepower with a big heart. We are software engineers, illustrators, lawyers, and former management consultants. We have worked at organizations including Palantir, Axios, Latch, and McKinsey.
We are an equal-opportunity workplace. We are deeply committed to building an inclusive workplace for people of all races, ages, gender identities, sexual orientations, religions, and ethnicities.
The Role: Data Engineer
We're looking for a data engineer to join as a member of our core team. You will join a tight-knit team and be empowered to shape the direction of technology and culture of the company.
Some of the things you can look forward to at Chapter
You will build features and pipelines across the platform, from tools that collect and generate data to pipelines that transform and model data into usable, operational systems. We have the best data management platform in the world as the first startup to use Palantir's data management platform, Foundry. You will establish technical strategy and direction and will also support growing and developing a world-class engineering team. There are so many important problems to solve that directly impact the lives of the 10,000 Americans entering retirement every day.
What you’ll need to be successful
Strong proficiency with Python. Familiarity with Spark or PySpark is preferred but not required
Understanding of data pipeline best practices and data modeling techniques
Excitement to build scalable data collection mechanisms to unlock information previously siloed away
Comfort with ambiguity. While starting from a blank slate is challenging, you find it exciting, rewarding, and empowering. You are able to build products from the ground up
Passion and excitement for the cause: to re-define how Americans age and transition into their third chapter
Superlative positional empathy with a strong understanding of how technical decisions impact the user of what you’re building
We are an equal-opportunity workplace. We are deeply committed to building an inclusive workplace for people of all races, ages, gender identities, sexual orientations, religions, and ethnicities.",https://www.indeed.com/rc/clk?jk=1e8f6fbd1c12ac6b&fccid=4e1fc58b5fcec5db&vjs=3,"New York, NY",Data Engineer,Data Engineer
176,"Torch Technologies, Inc.","Job Description:The successful candidate will assist the 53d Electronic Warfare Group (53 EWG), 53d Wing, at Eglin AFB, Florida. The 53 EWG is the technical focal point for all electronic warfare (EW) support of warfighter systems for the Combat Air Forces (CAF). The mission of the 53 EWG is to develop and test mission data (MD) to defeat enemy radar and infrared guided missile systems, thus enhancing aircrew and aircraft survivability in combat. This mission includes operational EW testing, MD development/validation/verification, force development evaluation execution and facilitating foreign materiel exploitation.The successful candidate will conduct appropriate EW research, MD development and MD testing. The successful candidate will support EW system programming/reprogramming, work with EW system engineers to coordinate programming/reprogramming requirements, prepare validation and verification test plans, and organize and participate in MD configuration control boards. The successful candidate will assist with the collection, recording, and post-test analysis of data generated during MD testing. The successful candidate may be required to travel.Job RequirementsCandidates must have at least a Bachelors' degree in a technical/engineering. They must also have a demonstrated ability to recognize and analyze problems, conduct research, summarize results, and make appropriate recommendations. Applicants must have a working knowledge of computer systems and an understanding of Windows-based personal computers and Microsoft Office software and possess the ability to communicate effectively both orally and written. 'Highly desirable (but not required) attributes include a current active Secret security clearance and previous experience in RF integration, EM spectrum management, antenna design, and digital signal processing. Also desired (but not required) is knowledge of EW weapons systems development, test and evaluation, and systems engineering. Candidates without and active Secret security clearance must be a US Citizen able to obtain and maintain a Secret clearance.Preferred SkillsKnowledge, skills and attributes associated with this position(s) include: EW and the electromagnetic (EM) spectrum, computer software, threat warning, radio frequency (RF) jammers, electro-optical/infrared (EO/IR) jammers, expendables, threat analysis, foreign/US/radar/weapon systems, airborne EW computer software, avionics, systems integration, electronics engineering concepts, principles and practices applicable to a broad range of engineering assignments. Candidates must be analytical, methodical and detail-oriented.This position is located at Eglin Air Force Base, Florida.Job Type: Full-timePay: From $75,000.00 per yearBenefits:401(k)Dental insuranceHealth insurancePaid time offVision insuranceSchedule:Monday to FridayWork Location: One location",https://www.indeed.com/company/Torch-Technologies/jobs/Junior-Entry-Level-Mission-Data-Engineer-d43c26e75e72fe91?fccid=b0df153a019c5ca2&vjs=3,"Eglin AFB, FL 32542 32542",Jr/Entry Level Mission Data Engineer,Data Engineer
177,Ariel Investments,"Ariel Investments is a premier, boutique, asset management firm. Our primary goal is to drive exceptional investment returns by bringing diverse perspectives together. The only way to beat a benchmark is to not look like one. As value investors, our thinking is deliberate and unconventional. We offer an independent, patient investing approach and aim to deliver excellence in any environment. We uphold our fiduciary responsibility to every shareholder, no matter how big or small.

At Ariel, we strongly believe that teamwork yields results—which is why we have Co-CEOs. John Rogers and Mellody Hobson share a desire to cultivate leaders who are curious, focused and disciplined. We are nimble and efficient. Our drive is fanatical and intentional. Everyone plays their position and each contribution is critical to our firm's success. We seek subject matter experts who are unapologetically themselves. We encourage our employees to reach their full potential and we give them the runway to do so.

After nearly four decades of active investing, we remain committed to our clients, our teammates and our community. We strive to be best-in-class investors and pioneer a path for those who entrust us with their financial future.

Ariel Investments is looking for an entry level Data Engineer with experience in Microsoft technologies like Power BI, Power Apps and SQL. This role will need to own the development from end to end and the support and maintenance after. This individual will have an innovative approach to problem solving and a commitment to meeting deadlines.

Create high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition, and delivery. The role will involve working with all members of the IT team across different areas and understand the IT landscape and how it can evolve to help the business.

Proof of vaccination is required as a condition of employment.

Responsibilities will include:
Building reporting models per best practices
Develop visual reports, dashboards & apps using Power BI
Connecting to data sources, importing and transforming data
Working knowledge of ETL
Good understanding of SQL Queries and logic
Should have knowledge and experience in code design and requirements analysis
Experience
2 years (ideally 2 years or less) of development using SQL, ETL tools and reporting tools
Knowledge in any other reporting tools – SSRS, Tableau a plus
Experience with at least one end-to-end implementation required
Knowledge of data models, ETL, and data warehouse processes, preferred
Python or R experience is a plus
Knowledge of extracting data through API/web services a plus
Education
Bachelor in computer science or MIS - Required
Certificates, Licenses, Registrations
Any relevant Microsoft certifications – Preferred
Computer Skills Needed to Perform this Job
Working knowledge of Microsoft Office Suite
Competencies

Drive for Results: Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results

Customer Focus: Is dedicated to meeting the expectations and requirements of internal and external customers; gets firsthand customer information and uses it for improvements in products and services; acts with customers in mind; establishes and maintains effective relationships with customers and gains their trust and respect.

Interpersonal Savvy: Relates well to all kinds of people, up, down, and sideways, inside and outside the organization; builds appropriate rapport; builds constructive and effective relationships; uses diplomacy and tact; can diffuse even high-tension situations comfortably.

Priority Setting: Spends his/her time and the time of others on what's important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.

Business Acumen: Knows how businesses work; knowledgeable in current and possible future policies, practices, trends, and information affecting his/her business and organization; knows the competition; is aware of how strategies and tactics work in the marketplace.
Ariel celebrates diversity and practices inclusion as a way to get work done – it's in our DNA. As an equal opportunity employer, our employment decisions are based on business needs, job requirements and individual qualifications without regard to race, color, religion, age, sex (including pregnancy), sexual orientation, gender identity, national origin, ancestry, marital status, parental status, mental or physical disability, military or veteran status, or any other basis protected by federal, state, or local law. Ariel is committed to recruiting and retaining talented applicants, and to providing all employees with a workplace free from discrimination and/or harassment.",https://www.indeed.com/rc/clk?jk=a298a7c00ce56c12&fccid=43a31a47b02448a2&vjs=3,"Chicago, IL",Data Engineer I,Data Engineer
178,LOCKHEED MARTIN CORPORATION,"COVID-19 continues to significantly impact our employees, families and communities. With employee health and safety as our top priority, and as a federal contractor, Lockheed Martin is taking action to address the increased risk and uncertainty COVID variants pose in the workplace and ensuring we meet our commitments to national security.

To uphold safety for all employees, we will continue to request vaccination status for all Lockheed Martin employees including new hires. All current and newly hired employees who are unvaccinated will be required to adhere to onsite safety protocols.
Description:This position is for a Data Engineer within Lockheed Martin (LM) Chief Data & Analytics Office (CDAO) Analytics Center of Excellence (ACE) supporting Aeronautics.


The work location is the Aguadilla, Puerto Rico facility; full-time telecommuters will not be considered for this position.

Duties and responsibilities include, but are not limited to:
Exhibits a degree of ingenuity, creativity, and resourcefulness when collecting, and processing data for analysis while working as part of an Agile scrum teamApplies and/or develops workflows that clean, transform and aggregate unorganized data into databases or data sourcesResponsible for collecting, ingesting, processing, and storing large datasets from a wide variety of data sources and stakeholdersMaintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releasesCollaborate with data architects and data analysts to develop solutions across the entire data processing pipeline
Basic Qualifications:
Experienced in design or development of enterprise data solutions, applications, and integrationsKnowledge of modern enterprise data architectures, design patterns, and data toolsets and the ability to apply themAble to understand traditional business intelligence and data warehousing concepts (ETL, data modeling, and reporting)Has data engineering experienceStrong problem solving, conceptualization, and communication skillsDegree in Computer Science, Systems Engineering, or related field
Desired Skills:
§ Familiarity with CICD - Continuous Integration/ Continuous Development Pipeline (Jenkins and Gitlab)
§ Excellent organizational, time management, analytical and problem-solving skills
Demonstrate ability to establish strong team relationships and deliver an exceptional customer experience
BASIC QUALIFICATIONS:
job.Qualifications

Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.

As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.
EXPERIENCE LEVEL:
Experienced Professional",https://www.indeed.com/rc/clk?jk=cbf2d3bd292bd4d4&fccid=aeb15e43a6800b9d&vjs=3,"Aguadilla, PR 00603 00603+7 locations",Data Engineer Asc.,Data Engineer
179,Signify Health,"What We Do:

At Signify Health, we provide care for those who need it most. We understand the benefits of a personalized approach to clinical encounters and the importance of conversation, understanding, and sitting knee-to-knee with the patient in the best setting for care delivery: the home.

A Software Engineer focusing on Data Engineering develops systems to manage data flow throughout Signify Health's infrastructure. This involves all elements of data engineering, such as ingestion, transformation, and distribution of data.

What will you do?

Develop readable, well-tested applications, APIs, and libraries
Work with internal and external APIs for applications and data sources
Utilize cloud infrastructure and collaborate with SREs to build scalable systems
Implement application observability in the form of metrics, logging, and monitoring
Work with engineers across the organization
Collaborate closely with team members and product stakeholders

Requirements

4+ years of relevant software engineering work experience
Prior work with cloud-based systems
Expansive knowledge of RESTful API design and use
Familiarity with observability concepts and tooling
Meaningful experience with relational databases and SQL

Nice-to-have:

Experience with Go or Python
Experience with Amazon AWS services
Experience with non-relational databases
Bachelor's degree in Computer Science or a related field

About Us

Signify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals' clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we're able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved.",https://www.indeed.com/rc/clk?jk=82a8930299da14cc&fccid=7f3af35993e7f09f&vjs=3,"Remote in Philadelphia, PA+5 locations",Data Engineer (Remote),Data Engineer
180,Computer Staff,"We have been retained by our client in Houston, Texas to deliver a Senior Data Engineer on a direct-hire basis. This offer is regular full-time, salaried position with a big data engineering team with a massive amount of career opportunity, and large bonuses and stock offered. This package is amazing! Low turnover at this company.


We are seeking a Data Engineer with Python functional software design and software development skills to use data to discover insights, power innovation, and inform business decisions.

Desired skills:

a Data Engineer candidate who is partially a software engineer yes, but primarily has strengths in data processing/ data engineering with python.
We highly prefer a senior data engineer candidate who is confident of their competency with all three (3) of these:
1) - distributed processing with PySpark, and/or Dask
2) - experience in creating logical and physical data models
3) - knowledgeable of Data Lake storage formats such as Parquet, Delta Lake, and Apache Iceberg

We are seeking a candidate to partner with engineering-related stakeholders to understand their problems then translate complex data insights into actionable programs and initiatives. Join our client’s big data group, as part of the Data Engineering team, working closely with Data Scientists and engineering business domain experts to help solve real engineering related problems using advanced data analytics, machine learning, and artificial intelligence. This individual will provide analytical and technical contributions to the team to advance the data engineering practice within the organization.


Responsibilities:
Work directly with Business domain experts and Data Scientists to understand the data analytics objective and use Python data engineering and automation provide trusted data sets , that have been cleaned, and curated in a timely manner.
Collect, explore, and prepare big data for advanced analytics and machine learning .
Perform exploratory data analysis and present findings for further analysis.
Design and implement high quality analytical applications and data products.
Automate manual data flows for repeated use and scalability by using Python.
Develop data-intensive applications using Python with API’s and streaming data pipelines.
Convert research-based machine learning models into production-ready software written in Python .
Assists data analysts and data scientists with data extraction, feature engineering, query optimization, and data processing.
Implement data quality checks to ensure data accuracy, consistency, and reliability
Requirements:
The successful candidate will meet the following qualifications:
3+ years of recent, functional programming experience in Python
Experience performing exploratory data analysis using common statistical methods
Experience engineering data solutions working with big data, including sensor data or data from internet of things and performing time series analysis upon data.
Strong understanding of data structures and algorithms.
Experience with Pandas or with data structures and operations for manipulating numerical tables or performing time series analysis . Contribute to the mathematics aspects and how linear programming can help. Use native Python for analytics, and use Pandas for data manipulation and data analysis.
Experience in building mathematical software solutions such as optimization solvers and simulations
Some expertise in software engineering practices such as Design Principles and Patterns, Testing, CI/CD, and version control.
Domain experience in or education in any Engineering discipline such as Chemical Engineering, Mechanical Engineering, Electrical Engineering or Industrial Engineering or any oil and gas industry experience is preferred, but not required.
Employment Type: Direct Hire, regular full-time


Pay Rate: $140,000 to 170,000 per year salary


Benefits: bonus, life, health, dental, vision, 401k matching, paid vacation, paid holidays, paid sick days, performance bonus. This is an excellent benefits package. Large bonuses and stock offered to incentivize loyalty and long-term commitments.


Location: Houston, Texas


Immigration: US citizens and those authorized to work in the US are encouraged to apply. We are unable to sponsor H1b candidates at this time.


No third parties. No consulting firms. Principals only.",https://www.indeed.com/rc/clk?jk=03391eaaa644fa34&fccid=eb32d63d83652f51&vjs=3,"Houston, TX","Data Engineer, PySpark",Data Engineer
181,Acunor,"Job Title: Data EngineerLocation: San Jose, CA Duration: FulltimeJob Description: We are looking for a Data Engineer who is an SME in Data Warehouse and BI with advanced knowledge of SQL, BigQuery, and Python. He/she will play a critical role in the development of our client's Enterprise Data and Analytics Platforms. This role will be responsible for expanding, optimizing, and monitoring our expanding data pipelines through thoughtful architecting, astute business logic, consistent data governance/testing, and continuous delivery.Skills: Experience in building & managing large databases.7/8+ years of experience in SQLKnowledge in BigQuery, Python, building data pipeline from third party API using JSON, etcHaving knowledge of Java/C++ is an advantageExperience in GCPCore Responsibilities: Responsible for building standards across data table, storage, naming & access controls for the teamResponsible for building data pipelines from third-party APIsResponsible for monitoring of data warehouse, support & debuggingResponsible reviewing codes, version controls & change list approvals & privacy reviewsWork with multiple data Engineering teams to ensure right signals are being populatedResponsible for documentation of the infrastructure design & applicationsResponsible to build thumb rules & monitoring mechanisms to ensure that team’s data tables are within the boundaries of legal & privacy policiesResponsible for building code optimization standardsResponsible for certification of all the data infrastructure assets with the certification council.Job Type: Full-timePay: $120,000.00 - $160,000.00 per yearSchedule:8 hour shiftExperience:SQL: 1 year (Preferred)Informatica: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Acunor-Infotech-LLC/jobs/Data-Engineer-ed01880594065e9f?fccid=f521a2fde61a6b30&vjs=3,"San Jose, CA 95113 95113 (Downtown area)",Data Engineer,Data Engineer
182,"Aviation Technology Associates, LLC","Aviation Technology Associates (AVTECH) is looking for a Data Integrity Engineer 5 in Savannah, GA
Position Purpose:
Works on engineering projects containing critical problems, the solution of which requires major technological advances and extensive related development
Principle Duties and Responsibilities:
Essential Functions:
Responsible for an entire engineering project of major complexity within established scope
Makes technical decisions and recommendations that are recognized as authoritative and have an important impact on extensive engineering activities
Applies advanced technical principles, theories and concepts based on broad expertise or unique specialized knowledge
Leads the development of innovative principles and ideas
Acts independently to determine improved methods and/or procedures to optimize technical product, cost and schedule requirements
Works under consultative direction; uses independent judgment to accomplish objectives
Develops standards and guides for diverse engineering activities
Conceives and plans investigation of broad areas of importance for which engineering precedents are lacking in areas critical to the program
Perform other duties as assigned.
As an ideal candidate to join the AVTECH team, you will meet the following:
Education and Experience Requirements:
Bachelor's Degree in engineering or technical related curriculum applicable to the respective job position required or equivalent combination of education and experience sufficient to successfully perform the essential functions of the job.
13 yrs in specific technical discipline or 15 yrs broad eng experience in several technical disciplines with Bachelors. Experience credit considered for related adv degrees limited to 2 yrs for Masters and 4 yrs for PhD in fields applicable to this job.
Advanced degree in engineering related field preferred.
Required Unique Skills:
Experience with Catia V5 and SmarTeam preferred
Experience with Model Based design practices a plus
Prefer a background in Check, Design and/or Quality
Shift: First
The estimated pay range for this position is $72.00 to $77.00 per hour.

By submitting your resume, you are agreeing to have AVTECH submit an application on your behalf to the company we are hiring for. Must be a US citizen or permanent resident.",https://www.indeed.com/rc/clk?jk=3ad4645f5fb57931&fccid=460489d4c6bfe080&vjs=3,"Savannah, GA",Data Integrity Engineer 5,Data Engineer
183,InnoSoul Inc.,"Job id : UT-99674 (90090228)4pRemote Azure Data Engineer with Agile/Scrum, Data Lake/Data Warehousing, and education/financial domain/Ed-Fi/CEDS/SIF/GAAP experienceLocation: Salt Lake City, UT (BOE)Duration: 17 monthsInterview: Phone100% remote work is available, on site is preferred. Local candidates that can work onsite will have an advantage to non-local candidates that want only 100% remote work. Keep in mind Remote workers must live and work inside the US.Requirements:A. Degree – Candidate must possess a bachelor’s degree from an accredited collegein Computer Science, Information Technology, Business, or equivalentexperience.B. Functional Experience – Candidates must have 3+ years of experience with largesets of regulated data.C. Interpersonal Relationships – Candidates must demonstrate the ability to workwell with others of all personality types while demonstrating problem-solvingand the ability to prioritize tasks.D. Communication – Candidates must demonstrate the ability to communicate inverbal and written form with both technical and non-technical personnel.E. Initiative – Candidates must demonstrate success as a self-starting, hardworkingand inquisitive worker.F. Teamwork – Candidates must demonstrate the ability to work with cross-functional teams to deliver on a common goal.G. Agile/Scrum – Candidates must demonstrate experience in an agile productenvironment to include deep understanding and experience with agilemethodologies.H. Data Lake and/or Date Warehousing – Experience with planning andimplementing a data warehouse, data lake, or other data repositories isrequired.Bonus Skills:A. Certifications – Azure Data Engineering, Azure DBA, Azure Developer or othersrelated to the positionB. Domain Driven Design – Understanding and experience with Domain DrivenDesignC. Financial or Education Domains – Ed-Fi, CEDS, SIF, GAAP, etc.100% remote work is available, on site is preferred. Local candidates that can work onsite will have an advantage to non-local candidates that want only 100% remote work. Keep in mind Remote workers must live and work inside the US.As a Data Engineer, your responsibilities will include: Develops and maintains scalable data pipelines and builds out new APIintegrations to support continuing increases in data volume and complexity. Collaborates with analytics and business teams to improve data models that feedbusiness intelligence tools, increasing data accessibility and fostering data-drivendecision making across the organization. Implements processes and systems to monitor data quality, ensuring productiondata is always accurate and available for key stakeholders and business processesthat depend on it. Performs data analysis required to troubleshoot data related issues and assist inthe resolution of data issues. Works closely with a team of engineers, product managers, and analysts. Defines company data assets (data models), jobs to populate data models. Designs data integrations and data quality framework. Designs and evaluates open source and vendor tools for data lineage. Works closely with all business units and engineering teams to develop strategyfor long term data platform architecture. Other duties as assigned.USBOE Data Engineer CRF.docxE-RTR.docJob Types: Full-time, ContractPay: Up to $100.00 per hourSchedule:8 hour shiftWork Location: One location",https://www.indeed.com/company/InnoSoul-Inc./jobs/Azure-Data-Engineer-c4ef4044d35942ab?fccid=ff73216627495746&vjs=3,"Remote in Salt Lake City, UT 84111 84111",Azure Data Engineer,Data Engineer
184,Blutium,"Key Responsibilities: Build pipelines to manage audience data for personalised marketing campaigns, integrate audience data from various sources to build one view of the customerPerform experimentation at scale by tagging audiences based on complex hierarchy of customer, product, and regional factorsResponsible for development of the workflow orchestration & ETL pipelines within marketing's analytics and data science platforms. You will ensure that the data pipeline infrastructure meets the analysis, reporting, and data science needs of the marketing organization.Implement continued design, development and optimization of the marketing data pipelines & data prep infrastructure built on cutting-edge cloud technologies.Embrace an active team role to help design, implement, and launch efficient and reliable data pipelines moving data across several platforms including Data Warehouse, online caches, and real-time systems.Create data architecture that is flexible, scalable, and consistent for cross-functional use, and aligned to stakeholder business requirements.Deploy workflow orchestration and demonstrate expertise in data modeling, ETL development, and data warehousingValidate Data Engineering business data elements, organizational and business intelligence architecture designs for engineering functional areas from Dashboards, Data Lakes, Data Operations, ML - AI, and upstream/downstream intake and output processes,Qualification − BA/BS Degree in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.Certifications − NAExperience − 5+ years of industry experience in data engineering, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets − Work experience with developers and business areas to design, configure, deploy, and maintain custom ETL Infrastructure to support project initiatives − Experience in interacting with multiple teams, client stakeholders and able to fix the issues in a shorter span of time SkillsTechnical/Domain − Experience building and managing data pipelines and repositories in cloud environments such as Google Cloud, Microsoft Azure or AWS − Experience in Airflow is a must Page 3 of 4 Private & Confidential − Experience extracting/cleansing data and generating insights from large transactional data sets using Spark SQL, SQL, Python, and PySpark on cloud − Experience with optimizing Spark pipelines on Dataproc, Databricks or similar technologies. CompetenciesBehaviourl − Must have exceptional and effective communication, organization, and time management skills − Must be pro-active and determined (Initiates action, Analytical and problem solver)Job Types: Full-time, ContractPay: $100,000.00 - $110,000.00 per yearSchedule:8 hour shiftMonday to FridayExperience:airflow: 4 years (Required)Azure: 4 years (Required)AWS: 5 years (Required)Work Location: Remote",https://www.indeed.com/company/Blutium/jobs/Data-Engineer-067f5a64baa4bd44?fccid=07ba05388d24e41c&vjs=3,Remote,Data Engineer,Data Engineer
185,Swish Analytics,"Company Overview

Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. We're looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumer/enterprise clients.

Job Description

Swish Analytics is looking for a Senior Python Data Engineer to join our ever-growing team! Data Science is at the core of our business, so this team has true ownership and impact over developing core components of Swish's data products.

Duties

Design and build reusable components, frameworks, and libraries at scale to support the Data team
Researching and making recommendations on Python tools & libraries to improve reliability, performance, scalability, and maintainability of core products.
Identify and solve issues concerning data management to improve data quality
Clean, prepare and optimize data for ingestion and consumption
Analyze and profile data for the purpose of designing scalable solutions
Collaboratively review design, code, test plans and dataset implementation performed by other data scientists and engineers in support of maintaining programming standards
Implement automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development, and production deployment frameworks
Collaborate on the implementation of new data management projects and on the restructuring of the current data architecture
Mentor others on the team while owning independent deliverables.

Requirements

Bachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience
7+ years of relevant experience in large-scale software development
5+ years of data engineering experience.
Strong understanding of algorithms, data structures, data architecture, and technical designs.
Strong experience with relational SQL and programming languages such as Python
Experience building cloud-scalable, real-time and high-performance data solutions using best practices
Experience with source control tools such as GitHub and related CI/CD processes.
Experience working with Big Data streaming services such as Kafka, etc.
Experience provisioning RESTful APIs to enable real-time data consumption
Experience working in AWS environments etc.
Experience with workflow scheduling tools like Airflow.
Proven ability to quickly pick up new languages, technologies, and frameworks.
Strong problem solving and analytical approach.
Proven track record of partnering with teams in solving complex problems by taking a broad perspective to identify innovative solutions
Excellent written and oral communication skills to both technical and non-technical audiences.
Benefits
Health: Medical, Dental, and Vision coverage for all
Vacation: Flexible PTO for whatever life brings your way
Financial: 401K, HSA, Commuter, and more!
Sports: Watch parties, World Series tickets, pickup basketball...and everything in between",https://www.indeed.com/rc/clk?jk=75e21d4f460128c0&fccid=e632785cd75868e9&vjs=3,"Remote in San Francisco, CA 94133 94133",Data Modeling Engineer,Data Engineer
186,ClassLink,"Data Engineer
Would you like to join a rapidly growing and successful company?

Do you want to be a part of a team where customers say things like:

“I am amazed by how innovative this product is!”


Headquartered in Clifton, NJ, ClassLink is a leading education technology company serving schools around the world. We are looking for a Data Engineer to work with our team to create award winning solutions for our clients.

ClassLink develops and maintains a wide assortment of independent products and to increase our speed and quality we are developing our own internal platform/framework. This role will be a key part of our architecture team and will focus on setting standards across the entire organization and helping develop a platform that makes testing and security across all of our products seamless. As we refine the platform, this role will also work closely with all of the different teams to help them leverage the platform and to identify opportunities to extend the platform as features are developed in different projects
We would love to hear from you if:

You have at least 3 years of professional software engineering experience.
You are a self-starter, who enjoys working in a fast-paced environment.
You want to own the entire lifecycle for the components you build whether they are features or products


Responsibilities:

Build and write effective APIs and Applications that support the digital services ClassLink delivers
Use unit and integration testing to ensure systems meet business needs
Ship software that impacts the lives of millions of Americans
Work with development teams and product managers to ideate software solutions
Design client-side and server-side architecture
Build the front-end of applications through appealing visual design
Develop and manage well-functioning databases and applications
Test software to ensure responsiveness and efficiency
Troubleshoot, debug and upgrade software
Create security and data protection settings
Build features and applications with a mobile responsive design
Write technical documentation
Work with data scientists and analysts to improve software


Required skills/competencies:

Understanding of the fundamental technologies of the web: HTTP, SSL, HTML, CSS, JavaScript, and JSON
Strong understanding and experience with the S.E.A.N. stack - SQL, Express, Angular, Node.js
Experience in database performance tuning, query optimization and monitoring of MySQL databases is required.
Minimum 3 years of experience developing consumer-facing web applications, services, and APIs
Experience with Amazon Web Services, in particular EC2, S3, and RDS a plus
Experience and understanding of agile methodologies
Bachelor’s degree in computer programming, computer science, or a related field.
Focus on efficiency, user experience, and process improvement.
Excellent project and time management skills.
Strong problem solving and verbal and written communication skills.
Ability to work independently or with a group.
Proficient spelling, grammar, and communication skills
""Never doubt that a small group of thoughtful, committed citizens can change the world;

We also like to have fun along the way.

We are a team of passionate people striving to help improve classroom learning.",https://www.indeed.com/rc/clk?jk=a86664a0d8b8aa50&fccid=868617b9b7a14fb6&vjs=3,"Clifton, NJ 07011 07011",Data Engineer,Data Engineer
187,Marathon TS,"Marathon is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights.The Position is currently 100% remote, but need local candidates for occasional onsite meetings.The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients' toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.Responsibilities:Complete development efforts across data pipeline to store, manage, store, and provision to data consumersBeing an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practicesWrite code to ensure the performance and reliability of data extraction and processingSupport continuous process automation for data ingestAchieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testingWork with program management and engineers to implement and document complex and evolving requirementsHelp cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamworkCollaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialistsRequired Skills:Must be able to obtain a Public Trust Clearance7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formatsProficiency developing ETL processes, and performing test and validation stepsProficiency to manipulate data (Python, R, SQL, SAS)Strong knowledge of big data analysis and storage tools and technologiesStrong understanding of the agile principles and ability to apply themStrong understanding of the CI/CD pipelines and ability to apply themExperience with relational database, such as, PostgreSQLWork comfortably in version control systems, such as, Git RepositoriesDesired Skills:Experience creating and consuming APIsExperience with Client and knowledge of Client standards a plusCandidates will be given special consideration for extensive experience with PythonAbility to develop visualizations utilizing Tableau or PowerBIExperience in developing Shell scripts on LinuxDemonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutionsDemonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiencesJob Type: Full-timePay: $110,000.00 - $140,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insurancePaid time offTuition reimbursementVision insuranceSchedule:8 hour shiftApplication Question(s):Do you meet the US CITIZEN Clearance requirement?Are you local to the Washington DC metro area?Do you have experience in design, management, and solutioning of large, complex data sets and models?Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats?Do you have experience developing ETL processes, and performing test and validation steps?Do you have Proficiency to manipulate data (Python, R, SQL, SAS)?Work Location: One location",https://www.indeed.com/company/Marathon-TS/jobs/Data-Engineer-87a815180db72d5f?fccid=971bf8a08036ad35&vjs=3,"Remote in Washington, DC 20001 20001",Data Engineer,Data Engineer
188,Blue State,"The role

As a Data Engineer at Blue State, you'll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations. You'll design, build, and manage the systems and processes which form the underpinning of Blue State's analytics work, supporting and working alongside data analysts and campaign strategists. But you'll also work directly with Blue State's clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations.

Day-to-day responsibilities:

Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects
Work with Blue State's client organizations to solve difficult data migration, management, and integration challenges
Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes
Automate workflows and look for further opportunities to improve efficiency in our work
Always maintain a high level of data security and privacy
The team

You will be a part of the global Web and Product Development team working primarily with our creative agency on client projects. You'll work in either the NY or DC office.

Once Blue State offices reopen, on-site presence is strongly preferred at a minimum of two days a week. To enter our US offices or attend Blue State events, staff and visitors must be fully vaccinated against COVID-19, including with a booster shot when eligible. Exceptions for protected grounds will be reviewed on a case-by-case basis.

Top things we're looking for
Good foundational understanding of statistical analysis
Extensive experience working with SQL databases in an analytics or business intelligence context
Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools
Experience with ETL/ELT tools, processes, and best practices
Strong Python experience:
Python should be your go-to tool for solving problems. If the first thing you want to do when you have to do the same thing twice is write a Python script to automate it - we want you!
Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus
Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks
Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:
Designing data warehouse schemas for cross-channel marketing analytics
Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data
Building and running automated tasks within the GCP environment - e.g. Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler
Comfortable managing GCP IAM policies across projects and teams
Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if you've extended Google Sheets using Google Apps Script
Familiarity with Git and maintains good habits around code maintenance
Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)
Good at teaching others what you know.

At Blue State, diversity is a necessity, not a nice-to-have. We encourage those from underrepresented communities — women, people of color, LGBTQIA+, immigrants, indigenous folks, those with disabilities and people at all the intersections in between — to apply. Even if you don't think your current skill set checks every box below, but this role seems to align with your strengths, we want to hear from you.

The minimum starting salary for this position is $90,000; compensation is otherwise commensurate with experience.

The company

Blue State is a values-led creative and campaigns agency that partners with leading causes, companies, and campaigns to build better organizations for a better world. We drive real change, make good trouble, put people first and are constantly curious.

We believe that there is no force more powerful than people taking collective action on the things they care about. We don't pretend to have all the answers, but we know where to find them: in people. We listen, learn, and uncover new insights that often surprise us and our clients — and move us toward better results. Across clients including UNHCR, Amnesty International, Google, Tesco, Nesta, and Tate. We have offices in New York City, Washington DC, London, Oakland and Chicago.",https://www.indeed.com/rc/clk?jk=7359029ebacaafc6&fccid=4d9339102788fdca&vjs=3,"Washington, DC 20005 20005 (Downtown area)+2 locations",Data Engineer,Data Engineer
189,Cisco Systems,"*** Must be a US Citizen and have the ability to obtain a U.S. Government Security Clearance***
What You’ll Do
As part of the CX Public Sector High Touch Technical Support (HTTS) team, you will work with an outstanding team of Tier 2 and 3 Customer Support Engineers. The CX PS HTTS team specialization in one of or more core technologies while understanding the foundations of our network products, protocols, and effective methodologies. You will be part of a collaborative team to provide support during critical network issues as well as leading a caseload of lower critical issues.

Who You’ll Work With
The CX Public Sector team provides second/third level technical support at security classification to the Federal and SLED customer base via phone, email, web, and remote access for R/S, Security, Collaboration, Datacenter, Wireless, Service Provider (both hardware and software) to Cisco customers, partners, account teams, and other engineers via phone/email/ consultation to independently solve & debug product problems. HTTS in Public Sector provides our Federal, SLED, and other customers operating within the regulatory space Cisco’s leading-edge technical support and services natively; thus redefining the way our customers meet their critical mission and business needs.

Who You Are
Minimum Qualifications: Sole US Citizenship and clearable for Top Secret clearance Technology expertise - CCIE or equivalent experience Resolution leader, problem-solving - troubleshooting methodology Communication & Facilitation, Listening & Affirmation, Influence & Persuasion, Public speaking & Presentation, Coaching Introspective - understand own social style, strength, flaws, and relation to others Results Focus - ego suppression, objectivity Phenomenal teammate passionate about customer success
Required skills Expertise with at least 1 of the following: Nexus ACI Routing/Switching
Nice-to-have skills Nexus Dashboard Tetration
Skills in server knowledge: Cisco UCS, VMware, Windows Server, Unix, Linux, Python,
REQUIRED EDUCATION Four-year college degree in Computer Science (or similar)
OTHER REQUIREMENTS US Citizen TOP SECRET/SCI with CI Poly Preferred Located in RTP, working in Cisco office daily Travel less than 25% Cisco Network Certifications (i.e., CCNA, CCIE, CISSP, CCNP, CCDA, etc.) a nice to have

Why Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!

But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)

Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward.

So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!


LI-VL1

dicesvs",https://www.indeed.com/rc/clk?jk=2263a363ce4f9842&fccid=904ac33af4a58e1e&vjs=3,"Research Triangle Park, NC",Customer Delivery High Touch Technical Engineer - Data Center,Data Engineer
190,Patreon,"Do you believe that creators should have the ability to get paid for the value they give to their fans?We do, which is why we're building Patreon, a platform that powers membership services for creators with established followings. Patreon strives to provide creators with insight, education, and tools that make it possible to retain creative control while running their creative business, so creators can focus on creating and energizing their fanbases.We have payed out over $500 million directly to creators on our platform this year alone, and our user base has doubled. In order to support this level of growth, we are looking for a* Data Engineer*.What you will doMaintain and improve our data warehouse (Redshift) and data lake.Design and build batch and streaming pipelines using Airflow, Spark, Kinesis/Kafka, S3, Redshift, ElasticSearch, etc.Assist our Data Science and Analytics teams with DAGs, data modeling and data validation.Assist our product teams with designing and building out our Analytics Platform, ML Platform and Discovery Platform.Skills and experience you possess2+ years experience in a Data Engineering role4+ years experience developing in Python and/or Scala.4+ years working with databases and writing SQL.Experience with databases such as Redshift, Snowflake, BigQuery, ElasticSearch. Clickhouse etc.Experience building data pipelines using such tools as Airflow, Databricks/EMR, Kafka/Kinesis, Hadoop/Hive, S3, etc.Knowledge of infrastructure as code and configuration management systems, such as terraform, ansible, etc.Who You'll Work With: At Patreon, you'll join a high-performing and highly-empathetic team of people who proudly work on fulfilling our mission of funding the creative class. Our culture of creator-first, thoughtful teammates keeps work creative, stretching, and rewarding.Our Core Behaviors: Put Creators First. Patreon is nothing without our creators.Achieve Ambitious Outcomes. Set, measure, and accomplish goals that deliver massive value to our creators and patrons.Cultivate Inclusion. We want an environment that retains and engages the diverse teams we build.Bias Towards Action. When in doubt, we take the next best step, then course correct when needed. We go out of our way to fix problems when we see them. We take ownership seriously.Be Candid and Kind. Be extremely caring and extremely direct in all you do at Patreon, especially when it comes to giving positive and constructive feedback.Be Curious. You don't know it all, and that's the fun part. Everything gets better when you're curious. Things get more interesting, more clear, and more approachable. When you bring curiosity into the workplace, you're growing yourself, your teammates, and Patreon as a whole.Want to Learn More About Patreon?Check out TechCrunch's article about our focus on membershipListen to our CEO Jack and Co-Founder Sam chat with Guy Raz on NPR's How I Built ThisCheck to see if you know a Patreon teammate on LinkedInJob Type: Full-time",https://www.indeed.com/company/Patreon/jobs/Data-Engineer-6f8a80efcb0b84b6?fccid=2013d915195afdb5&vjs=3,"San Francisco, CA",Data Engineer,Data Engineer
191,Chapter Medicare,"Why We Exist
Every morning, 10,000 Americans wake up and begin their first day of retirement. Chapter is re-inventing the way that Americans transition into retirement, starting with Medicare.
For most people, Medicare is boring, bureaucratic, and confusing. But it's important. If people wait too long to choose coverage, they risk life-time penalties from Uncle Sam, may need to undergo medical underwriting, or may pay out-of-pocket for 20%+ of medical costs plus prescriptions and most costs of dental, vision, and hearing.

What We Do
Our team and technology help retirees to navigate Medicare, including when and how to sign up, what specific coverage to choose, and how to maximize the benefits from their coverage.
Chapter has built industry-leading technology to help retirees save thousands of dollars on their healthcare. Our promise is simple: we want people to improve their health coverage while reducing what they pay for it.
How? Our platform searches every Medicare option available nationwide, while others search only a subset of plans, which is the norm in the Medicare market.
This full search is why we find savings and benefits that others miss.
Our Team
Our team is high-integrity and high-horsepower with a big heart. We are software engineers, illustrators, lawyers, and former management consultants. We have worked at organizations including Palantir, Axios, Latch, and McKinsey.
We are an equal-opportunity workplace. We are deeply committed to building an inclusive workplace for people of all races, ages, gender identities, sexual orientations, religions, and ethnicities.
The Role: Data Engineer
We're looking for a data engineer to join as a member of our core team. You will join a tight-knit team and be empowered to shape the direction of technology and culture of the company.
Some of the things you can look forward to at Chapter
You will build features and pipelines across the platform, from tools that collect and generate data to pipelines that transform and model data into usable, operational systems. We have the best data management platform in the world as the first startup to use Palantir's data management platform, Foundry. You will establish technical strategy and direction and will also support growing and developing a world-class engineering team. There are so many important problems to solve that directly impact the lives of the 10,000 Americans entering retirement every day.
What you’ll need to be successful
Strong proficiency with Python. Familiarity with Spark or PySpark is preferred but not required
Understanding of data pipeline best practices and data modeling techniques
Excitement to build scalable data collection mechanisms to unlock information previously siloed away
Comfort with ambiguity. While starting from a blank slate is challenging, you find it exciting, rewarding, and empowering. You are able to build products from the ground up
Passion and excitement for the cause: to re-define how Americans age and transition into their third chapter
Superlative positional empathy with a strong understanding of how technical decisions impact the user of what you’re building
We are an equal-opportunity workplace. We are deeply committed to building an inclusive workplace for people of all races, ages, gender identities, sexual orientations, religions, and ethnicities.",https://www.indeed.com/rc/clk?jk=1e8f6fbd1c12ac6b&fccid=4e1fc58b5fcec5db&vjs=3,"New York, NY",Data Engineer,Data Engineer
192,Edrstaffing,"Responsibilities:

Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management

techniques


Scaling up from proof of concept to “cluster scale” (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both

infrastructure/architecture and problem structure


Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and

management


Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache

Arrow, Kafka, and a wealth of other open source data tools)


Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational

components


Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains

Skills & Qualifications


Bachelor’s degree in computer science or related technical field. Masters or PhD a plus.
6+ years experience engineering software and data platforms / enterprise- scale data warehouses, preferably with knowledge of open source Apache

stack (especially Apache Spark, Apache Arrow, Kafka, and others)


3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this

What we offer


Very competitive salaries
Medical, dental and vision insurance, disability/life insurance
401(k) Plan
Aggressive bonus structure and/or Space and Time token allocations (similar to stock options)
Very flexible PTO and paid holidays, and flexible workweek
Very flexible remote work options
A massive list of perks including discretionary add-on bonuses for hard work, attending exciting events/conferences/parties, we’re headquartered

on the beach near LA (but don’t mind you working remote) and we’ll likely

fly you out once a month to meet in person",https://www.indeed.com/rc/clk?jk=e80f8f9fd5282afd&fccid=9c64a5c780f4ce99&vjs=3,+1 locationRemote,Data Engineer,Data Engineer
193,Underdog Fantasy,"What is Underdog?Founded by a team of industry veterans, Underdog was born with the idea that bringing strategic innovation to sports gaming and entertainment is the key to success in an exceptionally competitive market. Underdog has quickly established itself as a leader in the space, and is committed to building the best sports gaming platform in the industry -- and supporting the most passionate community of sports fans while doing it!At Underdog, we’re not only about creating these awesome products, but also about growing our culture of passion, ownership, and fun! We believe that great companies are made out of great people. Our continual aim is to create an inclusive environment for everyone, at all levels, to achieve their highest potential at work.As our first Data Engineer hire, you will work with our CTO and DevOps team to help design and implement our data architecture using Google BigQuery.What you'll do: You will be responsible for maintaining our existing managed ETL pipeline and improving our existing data warehouse solution to denormalize our data modelsYou'll work closely with our product data analysts to determine appropriate data models for the company's requirementsYou'll Improve the performance of our data warehouse – Google BigQueryWho you are: You enjoy thinking about data architecture and how to optimize data modelsYou’re comfortable working with others, asking and answering questions from both technical and non technical colleaguesEmployee satisfaction is important to you and the idea of improving the performance of our queries and overall architecture is excitingYou’re detail oriented and like to ensure proper documentation and communication with appropriate teamsYou have extensive experience working with Google BigQueryUnderdog Sports is firmly committed to equity, inclusion, and diversity. Our unique culture was built on the foundation of a safe and inclusive environment for people of all backgrounds. We highly value the mental, physical, and emotional health of our employees, and are continuously asking ourselves: what can we do better?Underdog is an equal opportunity employer and doesn't discriminate on the basis of creed, race, sexual orientation, gender, age, disability status or any other defining characteristic.Our targeted compensation rate for this position is between $120,000 and $160,000, depending on experience, plus equity.Below you’ll find a few of our perks:-Unlimited PTO (we're extremely flexible with the exception of the first few weeks before & into the NFL season)-Health, Dental, Vision+-401k Match-FSA-Remote, In-Person, or Hybrid Scheduling-A $500 home office allowanceJob Type: Full-timePay: $120,000.00 - $160,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insurancePaid time offParental leaveVision insuranceSchedule:Monday to FridayCOVID-19 considerations:Vaccination required for in-person work in our Brooklyn, NY office. Otherwise remote.Work Location: Remote",https://www.indeed.com/company/Underdog-Fantasy/jobs/Senior-Data-Engineer-4878b2f5525a8e62?fccid=993be2c19dc26b1f&vjs=3,Remote,Senior Data Engineer,Data Engineer
194,UniversalCIS,"SUMMARY:
The data engineer is responsible for collecting company data and organizing into a usable (and useful) format for others to use. The data collected serves as the centerpiece to the company?s analytical capabilities.
The role is an independent contributor and does not have any team members reporting to him/her. The impact on the organization includes, but is not limited to:
Ensuring enterprise-wide data availability ? timely and accurate.
Create accessibility of data by organizing and managing database locations.
Managing and improving the infrastructure and architecture.

This role is a highly technical role where most time is spent designing architecture, writing computer programs, and improving analytical capabilities.
The Data Engineer is responsible for collecting, cleaning, and organizing data so that it can be used for enterprise analysis. This is important as the company looks to use data to make better decisions about everything from product development to marketing.
Shift Hours: M-F 8am-5pm EST.
Salary Range $88,000- $100,000
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Help develop, refine, and maintain the company?s data warehouse architecture.
Identify useful data sources, evaluate their usability, and determine how best to extract data and structure it for analysis.
Write computer scripts to extract, transform, and load data from various disparate systems (e.g., Sage Intacct, Meridian Link, Paychex, etc.) into a standardized, coherent source (e.g., the company?s data warehouse).
Create computer programs or scripts to automate manual tasks that are performed repeatedly or that require expertise to complete.
Identify opportunities to improve efficiency through automation.
Recommend improvements to existing systems based on findings from data analysis.
Conduct analyses to identify patterns, trends, or other notable information in datasets.
Develop data models to extract meaning from raw data that can be used for decision making purposes.
Develop reports, presentations, and data visualizations to present findings to senior management.
Design, build, and maintain databases to store large amounts of data in an organized fashion.",https://www.indeed.com/rc/clk?jk=d7d9e3ecbd5f8a53&fccid=ff19f774ab9110d4&vjs=3,"Salisbury, MD 21804 21804",Data Engineer,Data Engineer
195,"Aviation Technology Associates, LLC","Aviation Technology Associates (AVTECH) is looking for a Data Integrity Engineer 5 in Savannah, GA
Position Purpose:
Works on engineering projects containing critical problems, the solution of which requires major technological advances and extensive related development
Principle Duties and Responsibilities:
Essential Functions:
Responsible for an entire engineering project of major complexity within established scope
Makes technical decisions and recommendations that are recognized as authoritative and have an important impact on extensive engineering activities
Applies advanced technical principles, theories and concepts based on broad expertise or unique specialized knowledge
Leads the development of innovative principles and ideas
Acts independently to determine improved methods and/or procedures to optimize technical product, cost and schedule requirements
Works under consultative direction; uses independent judgment to accomplish objectives
Develops standards and guides for diverse engineering activities
Conceives and plans investigation of broad areas of importance for which engineering precedents are lacking in areas critical to the program
Perform other duties as assigned.
As an ideal candidate to join the AVTECH team, you will meet the following:
Education and Experience Requirements:
Bachelor's Degree in engineering or technical related curriculum applicable to the respective job position required or equivalent combination of education and experience sufficient to successfully perform the essential functions of the job.
13 yrs in specific technical discipline or 15 yrs broad eng experience in several technical disciplines with Bachelors. Experience credit considered for related adv degrees limited to 2 yrs for Masters and 4 yrs for PhD in fields applicable to this job.
Advanced degree in engineering related field preferred.
Required Unique Skills:
Experience with Catia V5 and SmarTeam preferred
Experience with Model Based design practices a plus
Prefer a background in Check, Design and/or Quality
Shift: First
The estimated pay range for this position is $72.00 to $77.00 per hour.

By submitting your resume, you are agreeing to have AVTECH submit an application on your behalf to the company we are hiring for. Must be a US citizen or permanent resident.",https://www.indeed.com/rc/clk?jk=3ad4645f5fb57931&fccid=460489d4c6bfe080&vjs=3,"Savannah, GA",Data Integrity Engineer 5,Data Engineer
196,FedEx Services,"Company: FedEx Services
Job Title: Data Engineer - Dataworks - All Levels (Remote)
Job Requisition Number: RC481202
Category: Information Technology
Pay Type: Exempt
Locations:
Memphis, Tennessee 38120
United States

Remote, Tennessee 38120
United States

Colorado Residents Only – Compensation: Monthly Salary $6908.82 - $14524.25
The estimate displayed represents the typical salary range or starting rate of candidates hired in Colorado. Factors that may be used to determine your actual salary may include your specific skills, your work location, how many years of experience you have, and comparison to other employees already in this role. This information is provided to applicants in accordance to the Colorado Equal Pay for Equal Work Act.
Duties for this role include but not limited to: supporting the design, build, test and maintain data pipelines at big data scale. Assists with updating data from multiple data sources. Work on batch processing of collected data and match its format to the stored data, make sure that the data is ready to be processed and analyzed. Assisting with keeping the ecosystem and the pipeline optimized and efficient, troubleshooting standard performance, data related problems and provide L3 support. Implementing parsers, validators, transformers and correlators to reformat, update and enhance the data. Provides recommendations to highly complex problems. Providing guidance to those in less senior positions.

Additional Job Details:
Data Engineers play a pivotal role within Dataworks, focused on creating and driving engineering innovation within Dataworks, helping define and build the Dataworks organization and facilitate the delivery of key business initiatives. S/he acts as a “universal translator” between IT, business, software engineers and data scientists, collaborating with these multi-disciplinary teams. Data Engineers will contribute to the creation of and adherence to technical standards for data engineering, including the selection and refinements of foundational technical components. S/he will work on those aspects of the Dataworks platform that govern the ingestion, transformation, and pipelining of data assets, both to end users within FedEx and into data products and services that may be externally facing. Day-to-day, s/he will be deeply involved in code reviews and large-scale deployments.

Essential Job Duties & Responsibilities:
Understanding in depth both the business and technical problems Dataworks aims to solve

Building tools, platforms and pipelines to enable teams to clearly and cleanly analyze data, build models and drive decisions

Scaling up from “laptop-scale” to “cluster scale” problems, in terms of both infrastructure and problem structure and technique

Delivering tangible value very rapidly, collaborating with diverse teams of varying backgrounds and disciplines

Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases

Interacting with senior technologists from the broader enterprise and outside of FedEx (partner ecosystems and customers) to create synergies and ensure smooth deployments to downstream operational systems

Skill/Knowledge Considered a plus:
Technical background in computer science, software engineering, database systems, distributed systems

Familiarity/fluency with distributed and cloud environments and a deep understanding of how to balance computational considerations with theoretical properties

Detailed knowledge of the Microsoft Azure tooling for large-scale data engineering efforts and deployments is highly preferred
Experience with designing and deploying large scale technical solutions, which deliver tangible, ongoing value

Direct experience having built and deployed robust, complex production systems that implement modern, data scientific methods at scale

Ability to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle, and to work through problems as they are still being defined

Demonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value

An ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impact

Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews
Ability to conduct data analysis, investigation, and lineage studies to document and enhance data quality and access

Use of agile and devops practices for project and software management including continuous integration and continuous delivery

Demonstrated expertise working with some of the following common languages and tools:

Spark (Scala and PySpark), HDFS, Kafka and other high-volume data tools

SQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearch

Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools

Minimum Qualifications:

Data Engineer II:

Bachelor's Degree in Computer Science, Information Systems, a related quantitative field such as Engineering or Mathematics or equivalent formal training or work experience. Two (2) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Strong knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Strong knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience as a member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Data Engineer III:
Bachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Five (5) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a senior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Data Engineer Advisor:
Bachelor’s Degree in Information Systems, Computer Science, or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Seven (7) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a leader or a senior member of multi-function project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Domicile / Relocation Information:
This position can be domiciled anywhere in the United States.
The ability to work remotely within the United States may be available based on business need.
Relocation assistance may be available based on business need.

Application Criteria / Deadline:
Upload current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire.
Employee Benefits: Medical, dental, and vision insurance; paid Life and AD&D insurance; tuition reimbursement; paid sick leave; paid parental leave, paid vacation, and additional paid time off; geographic pay ranges; 401K with Company match and incentive bonus potential; sales incentive compensation for selling roles.
FedEx. Where now meets next.
Our vision is to be the earth's most engaged advocates of connected commerce where open borders, new markets and fair, sustainable practices are the norm for the billions of personal supply chains being managed every day in our always on, mobile-first world. We stand for ease, access and opportunity. We lead purposeful innovation, champion entrepreneurs, advocate free trade and empower humans and their place in the era of autonomy and AI. We fight for our customers, a more sustainable planet and an ethical playing field.
FedEx inspires its more than 570,000 team members to remain focused on safety, the highest ethical and professional standards and the needs of their customers and communities. FedEx is committed to connecting people and possibilities around the world responsibly and resourcefully, with a goal to achieve carbon-neutral operations by 2040.
FedEx has been recognized on many different lists both for business success and for being a great employer:
Fortune ""World’s Most Admired Companies"" – 2021
Forbes ""Best Employers for Diversity"" - 2021
LinkedIn ""Top 100 Companies"" - 2021
TIME ""100 Most Influential Companies"" - 2021
World HRD Congress ""Best Gender Equality Workplace"" – 2021
InsiderPro ComputerWorld ""Best Places to Work for IT"" – 2021

Application Instructions/Deadline:
Upload current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire by close of business (5:00pm CST) on the date below. If the date below is blank, there is no specified closing date for this requisition.
03-14-2022
FedEx Services is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, genetics disability, or protected Veteran status.
FedEx Services does not discriminate against qualified individuals with disabilities in regard to job application procedures, hiring, and other terms and conditions of employment. Further, FedEx Services is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position, to perform the essential functions of the position in question, or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities, unless the accommodation will impose an undue hardship. If a reasonable accommodation is needed, please contact recruitmentsupport@fedex.com.",https://www.indeed.com/rc/clk?jk=ba129ecc0b06f6a8&fccid=2ae04b97d8829e58&vjs=3,"Remote in Memphis, TN+1 location",Data Engineer - Dataworks - All Levels (Remote),Data Engineer
197,Swish Analytics,"Company Overview

Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. We're looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumer/enterprise clients.

Job Description

Swish Analytics is looking for a Senior Python Data Engineer to join our ever-growing team! Data Science is at the core of our business, so this team has true ownership and impact over developing core components of Swish's data products.

Duties

Design and build reusable components, frameworks, and libraries at scale to support the Data team
Researching and making recommendations on Python tools & libraries to improve reliability, performance, scalability, and maintainability of core products.
Identify and solve issues concerning data management to improve data quality
Clean, prepare and optimize data for ingestion and consumption
Analyze and profile data for the purpose of designing scalable solutions
Collaboratively review design, code, test plans and dataset implementation performed by other data scientists and engineers in support of maintaining programming standards
Implement automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development, and production deployment frameworks
Collaborate on the implementation of new data management projects and on the restructuring of the current data architecture
Mentor others on the team while owning independent deliverables.

Requirements

Bachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience
7+ years of relevant experience in large-scale software development
5+ years of data engineering experience.
Strong understanding of algorithms, data structures, data architecture, and technical designs.
Strong experience with relational SQL and programming languages such as Python
Experience building cloud-scalable, real-time and high-performance data solutions using best practices
Experience with source control tools such as GitHub and related CI/CD processes.
Experience working with Big Data streaming services such as Kafka, etc.
Experience provisioning RESTful APIs to enable real-time data consumption
Experience working in AWS environments etc.
Experience with workflow scheduling tools like Airflow.
Proven ability to quickly pick up new languages, technologies, and frameworks.
Strong problem solving and analytical approach.
Proven track record of partnering with teams in solving complex problems by taking a broad perspective to identify innovative solutions
Excellent written and oral communication skills to both technical and non-technical audiences.
Benefits
Health: Medical, Dental, and Vision coverage for all
Vacation: Flexible PTO for whatever life brings your way
Financial: 401K, HSA, Commuter, and more!
Sports: Watch parties, World Series tickets, pickup basketball...and everything in between",https://www.indeed.com/rc/clk?jk=75e21d4f460128c0&fccid=e632785cd75868e9&vjs=3,"Remote in San Francisco, CA 94133 94133",Data Modeling Engineer,Data Engineer
198,Nexintech Inc,"Google Cloud Certified, Data EngineerLocation: Irving, TX - Basking Ridge, NJ – Piscataway, NJ – Tampa, FL – Alpharetta, GA (must be local to one of the locations – will join remote, may turn Hybrid or in offer after COVID)Duration: Long term ContractRate: $74/hr - $80/hr MaxJD/Requirements: Must have 5 yrs minimum hands on experience with the following:Writes complex SQL queries required to perform Data Acquisition and Ingestion required for Data pipelinesBuilds Data pipelines and does data engineering activities using technologies like Python, Hadoop, Spark etc.,Ensures the upkeep of the Hadoop Data Lake Platform by monitoring the Horton Works HDFSMonitors the Data Lake constantly and ensures that the appropriate support teams are engaged at the right timesWorks in an Agile/Scrum Environment, interacts with a scrum team as well as the Client StakeholdersUnderstands the client requirements from Agile scrum user stories and develops low level design required for the user storiesResult Oriented and able to match the pace of work demands of the Program through self-improvementJob Types: Full-time, ContractSalary: Up to $80.00 per hourExperience:SQL: 1 year (Preferred)Informatica: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: Multiple Locations",https://www.indeed.com/company/Nexintech-Inc./jobs/Google-Cloud-Certified-98307651571bedba?fccid=a8d894a8aed9a81a&vjs=3,"Hybrid remote in Jersey, GA+2 locations","Google Cloud Certified, Data Engineer",Data Engineer
199,Procter & Gamble,"Data Engineer
Information Technology (IT) at Procter & Gamble is where business, innovation and technology integrate to create a driven advantage for P&G. Our mission is clear - we deliver IT to help P&G win with the over 5 billion consumers we serve worldwide. Our IT professionals are diverse business leaders who apply IT excellence to deliver groundbreaking business models and capabilities for our 65 iconic, trusted brands.
From Day 1, you'll dive right in, take the lead, use your initiative, and build billion-dollar brands that help make everyday activities easier and make the world a better place. You'll be doing meaningful work that takes your career places you never envisioned. And you'll do this in creative workspaces where new insights thrive and where your technical expertise is recognized and rewarded.
The Opportunity
We are currently looking for a Data Engineer to join our Data & Analytics team in Cincinnati. In this role, you will be responsible for designing and building solutions using various modern Azure components & tools. You will lead delivery of E2E scalable data & analytics applications using best in class industry standards, actively coding and adapting them to ensure their resiliency and fit to requirements.
Description
Design and build scalable and resilient Data & Analytics solutions in Microsoft Azure - architect technical solutions to obtain, process, store and provide insights based on processed data
Develop, within existing designs of various solutions in Microsoft Azure, environments to help generate valuable insights
Work on automation and optimization of internal processes in Azure
Influence the future of these new technologies and the ways in which P&G uses them
Have a possibility to work with multifunctional and multinational teams within and outside of P&G
Focus on key business cases development within Data & Analytics + Azure
Manage agile projects using cloud and hybrid solutions

Qualifications
Qualifications:
Required:
Bachelor's degree in Computer Science, Computer / Systems / Industrial Engineering, Business / Management Information Systems or Software Development, Information Technology
Python and SQL programming skills
Cloud Understanding
Understanding of data modeling
Understanding of CI/CD

Preferred:

Understanding of Spark framework and distributed architecture features and challenges
Experience in implementing projects & solutions using Microsoft Azure stack (eg. Databricks, Azure Data Factory, Azure Analysis Service, SQL Server, Azure Synapse)
Knowledge of SCRUM and DevOps methodologies
Hands on experience in writing clean, effective code
The Value of a P&G Career
Ongoing coaching and career development - you will work with passionate people and have access to best in class training through our P&G Leadership Academy as well as day-to-day mentoring from your manager
We provide a market-competitive salary benchmarked against the finest companies, so you'll be able to spend your generous vacation time doing more of the things you love with the people you love
We offer a suite of benefits, including but not limited to: flexible working arrangements, generous paid vacation increasing with service, generous parental leave policies, group life insurance, health insurance, and dedicated support to help you find the right child care or elder care
Additional perks include discounted P&G products from our company shops and a discount platform offering you unbeatable savings on everything from groceries to exotic holidays
What's more, your financial package might include things like interest-free loans, a tax-advantageous share purchase plan, a contributory pension plan, and financial education and advisement on topics including purchasing real estate and generating wealth
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor.
Immigration sponsorship is not available for this role. As a general matter, Procter & Gamble does not sponsor candidates for nonimmigrant visas or permanent residency. However, Procter & Gamble may make exceptions on a discretionary basis. Any exceptions would be based on the Company's specific business needs at the time and place of recruitment as well as the particular qualifications of the individual.
Procter & Gamble participates in e-verify as required by law.
Qualified individuals will not be disadvantaged based on being unemployed
)",https://www.indeed.com/rc/clk?jk=291964bfcdd2004b&fccid=2da0dedf6df97194&vjs=3,"West Chester, OH 45011 45011+3 locations",Data Engineer II,Data Engineer
200,Nexient,"About your future team

Nexient is on a mission: To rid the world of crappy software, one sprint at a time.

Every day, our 1000+ Agile developers, designers, and strategists empower each other to find passion in our work while we innovate the world. This culture of teamwork and curiosity is fueled by a product-minded approach to our work, crafting extraordinary custom software solutions and growing careers along the way.

Our clients range from some of America’s favorite brands in retail, healthcare, financial services to fast-emerging disruptors – You might have read about us in the NY Times. We’re also recognized as a Gartner Cool Vendor, HFS Hot Vendor, and America’s leading 100% U.S. Agile software services partner.

If you’re looking to change the trajectory of your career, make an impact on Day One, develop custom software on the best technology around – and maybe even make people’s lives easier - then you belong here, with us.
Required Skills and Experience:
Solid understanding of machine learning fundamentals
Optimization (gradient decent variants, hyperparameter tuning, regularization)
Experience in a variety of problems and domains, with depth in at least four (supervised, unsupervised, generative, control/RL, anomaly detection, timeseries, etc.)
Expert with traditional ML models (k-means, KNN, decision trees, SVM, Bayesian/graphical models, Gaussian process, etc.)
Mastery of Deep Learning fundamentals (CNN, RNN, attention/memory, Autoregressive) and extensions (Transformer, LSTM, ResNet, etc.)
Experience coding with popular frameworks
Python expertise (data structures, coding patterns) along with common ML, data, math and visualization libraries/platforms
Can architect large scale machine learning projects, while supporting team and client related activities (documentation, mentoring, planning, etc.)
Ability to set best practices for modeling/development
Configuring and working in different coding environments (local, notebooks, containers) and using standard software engineering workflows (testing, code management/Git, CI/CD)
Loading and saving large data sets and models efficiently
Customizing models and optimizing performance
Structuring code to make it modular and deployable (logging and metrics, adhering to common deployment interfaces/hooks)
Ability to analyze and explain papers/theory and adapt associated models
NLP specialization (NLU, NLI, QA, BERT, GPT, T5, XLNet, etc.)
Some MLOps and cloud environment experience (resource configuration, optimization)
Solid data engineering skills (large scale data processing in batch or stream, security and configuration best practices)
Why Nexient

As a vital member of our Engineering Practice, you will be part of one of our 100+ small cross-functional teams working side by side with some of the most talented developers, UX designers, analysts, quality engineers, and product managers out there.

From Silicon Valley to Ann Arbor, Columbus, and beyond – a career with Nexient will offer you:

Remote, hybrid, or onsite opportunities: Here, you decide the work arrangement that best suits your needs and preferences.

Fulfilling and challenging projects: You will be exposed to different scenarios, multiple types of clients, and the most complex business and technology initiatives.

Access to the most cutting-edge technologies: You’re always going to be advancing your skillset by learning and working with the newest and most relevant technologies available out there.

A positive and passionate culture: Boring and divided aren’t in our vocabulary. In fact, it’s amazing to see how, despite our exponential growth, the Nexient culture and community stay as warm, diverse, and close as ever.

Plenty of opportunities for you to explore and grow: We will empower you to dream as big as you want and support you to make your career what YOU want it to be.

Creative runway: We take pride in encouraging our people to question the status quo.

Our benefits program includes:
Medical, dental, and vision coverageGenerous PTO, plus holidays401(k) planLife insurance and short-term & long-term disabilityMaternity leaveEmployee Assistance ProgramFlexible spending accountsHealth savings account

Nexient is an inclusive, equal opportunity employer. At Nexient, your uniqueness makes us better. Your uniqueness is why you are valued, you are heard, you add to our culture, and you belong. Diversity & Inclusion is hardcoded into our DNA and at the core of everything we do. Together, not separately, we strive to become even more diverse to help fuel our innovation and connect us closer with the clients and communities we serve.",https://www.indeed.com/rc/clk?jk=6b64b3e647a4e41e&fccid=a2428e4814a1eff5&vjs=3,"Hybrid remote in Ann Arbor, MI 48108 48108",Senior Machine Learning Engineer/Data Scientist,Data Engineer
201,Spotify,"Engineering
Data
At Spotify Advertising, our part of that mission is to build the next generation advertising platform for audio which can scale the freemium experience for hundreds of millions of fans and tens of thousands of advertisers. This scale brings unique challenges as well as tremendous opportunities to define the insights product for our business.
Location
New York or Remote Americas
Job type
Permanent
Our Megaphone team is looking for a Data Engineer to develop and own software solutions on our fast-paced podcast and ad services technology. This is a fantastic opportunity to join our rapidly growing Megaphone team at the center of the podcasting world.
What You'll Do
Work closely with key partners inside and outside of the company to develop best-in-class software
Conceptualize ways to monitor and alert on ad outages
Build new distributed data pipelines across ad experience
Work across the entire ads business platform, contributing to the improvement of many different pipelines
Apply a Data centric approach to all the platforms, pipelines and engineering activities
Immerse yourself in Scala/Python engineering for better data visibility and unearth viable channels for improvement
Become proficient in Java, Scio & Luigi (if not already)
Collaborate and connect with our team of engineers to ensure all data pipelines are robust and reliable across backend, frontend and machine learning etc
Who You Are
You love Data Engineering
You are Data centric (able to morph between a Data Engineer and Data Scientist)
You have a strong understanding of data systems
You are knowledgeable and passionate about improving and building new distributed data pipelines
You are familiar with current engineering practices such as distributed architecture
You are curious about new technologies and finding uses and insights for all types of data
You have experience working on and building distributed data pipelines that ingest huge amounts of data across multiple sources and brands
You have experience working with Scala and/or Java. AWS, GCP, Snowflake, Scio & Luigi would be encouraged bonuses
Ad Tech experience is a plus
Where You'll Be
We are a distributed workforce enabling our band members to find a work mode that is best for them!
Where in the world? For this role, it can be within the Americas region in which we have a work location
Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here
Working hours? We operate within the Eastern Standard time zone for collaboration
Our global benefits
Extensive learning opportunities, through our dedicated team, GreenHouse.
Flexible share incentives letting you choose how you share in our success.
Global parental leave, six months off - fully paid - for all new parents.
All The Feels, our employee assistance program and self-care hub.
Flexible public holidays, swap days off according to your values and beliefs.
Spotify On Tour, join your colleagues on trips to industry festivals and events.
Learn about life at Spotify
Spotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.
Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service.",https://www.indeed.com/rc/clk?jk=508b2b1f20712bae&fccid=fe404d18bb9eef1e&vjs=3,"Remote in New York, NY+20 locations","Data Engineer, Megaphone",Data Engineer
202,RPM Living,"Overview:

Looking for something different? We ARE that something different at RPM Living.
Dynamic and fast growth culture and multiple nationwide opportunities let YOU shape your future with us. Top industry pay and benefits, best industry practices, career training and education, people-first focus...… we show you the way to success.

Responsibilities:
Assembling large, complex sets of data that meet non-functional and functional business requirementsIdentifying, designing and implementing data structures and selecting various tools to accomplish internal process improvementsDesigning infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using Azure Cloud and SQL technologiesBuilding complete data pipelines and solutions with ability to understand and leverage best technology for given problem to appropriately accomplish the job.Working with stakeholders to evaluate and solve data-related technical issuesWorking with stakeholders to support their data infrastructure needs while assisting with data-related technical issues
Qualifications:
Bachelor's degree from four-year college or university; or one to two years related experience and/or training; or equivalent combination of education and experience.Demonstrated ability to work within a deadline, and balance competing priorities.Previous experience in real estate management preferred.Ability to analyze complex data and implement statistical solutions to business problemsExperience with ETL operations, such as REST APIs and ETL toolsHighly proficient in SQL and database architecture
Highly proficient with Python
Proficient with Git methodologyAzure experience preferred (logic apps, automation)
The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
RPM Living, LLC. is an Equal Employment Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.",https://www.indeed.com/rc/clk?jk=a322595975ac6f9c&fccid=aec302af63a31eea&vjs=3,"Austin, TX 78731 78731",Data Engineer,Data Engineer
203,Braintrust,"ABOUT US:

Braintrust (usebraintrust.com) is a user-controlled talent network, where you keep 100% of what you earn and actually get to own the platform. We've been onboarding some big clients and have the specific needs for the below specialist for one of our clients.
JOB TYPE: Freelance, Contract Position - No agencies (See notes below)

LOCATION: Remote - US Only (TimeZone: EST)

HOURLY RANGE: Our client is looking to pay $100 - $130 USD / HR

ESTIMATED DURATION: 40Hrs/Week - Long-term, ongoing project

US Only for this position


ABOUT US:

Braintrust (usebraintrust.com) is a user-controlled talent network, where you keep 100% of what you earn and actually get to own the platform. We've been onboarding some big clients and specifically need a Senior Engineer for our client.


ABOUT OUR CLIENT:

We're looking for a smart, driven and passionate engineer to be part of the observability platform team. The observability platform at our clients is composed of complex distributed systems and data pipelines built mainly using Grafana , InfluxDB, Elastic Stack (formerly ELK), Apache Kafka and Tremor (in-house event processing system built initially for our logging needs and now open-source!).


We collect 10+ billion log events per day and 17+ billion metrics generated by 20,000+ systems and 500+ homegrown applications across multiple geo locales and GCP regions, while supporting searches against these datasets for the purposes of engineering functions like monitoring, alerting, observability, high velocity software development and security incident event management.
On the Observability Platform team as a DevOps Engineer, you'll have plenty of opportunities to share your strengths as well as build others while contributing to various mature as well as emerging open-source projects. You will work in a global team, with on-premise and cloud-based deployments in an inclusive environment. If this sounds like fun to you, please continue reading and apply!


What You'll Do

Using SQL like language to query Influx DB
Create Grafana dashboard as InfluxDB and Data Dog as source
Assist developers in rebuilding/modifying their metrics dashboards in Datadog
Contribute to production support of existing Metrics pipeline
Continue to optimize the new Metrics pipeline by monitoring and optimizing the metrics.

What You'll Need

4+ years of experience in systems and software engineering, as well as SRE/DevOps paradigms
Experience in scripting languages used in the infrastructure space (Python, Bash etc.) as well as familiarity with version control systems such as Git
2+ years of hands-on experience with distributed technologies like Kafka and Time Series Databases (InfluxDB) or Datadog.
2+ years of working with configuration management and orchestration tools such as Puppet, Ansible and Terraform
2+ years of working on GCP cloud platforms
1+ years experience working with Grafana building dashboards
Efficient at prioritizing different tasks based on their relative importance in a fast-paced production environment

Good Things to Have

Experience with systems programming languages like Go or Rust
Experience contributing to and working with open source software
Strong written and verbal communication skills

Apply Now!
ABOUT THE HIRING PROCESS:

Qualified candidates will be invited to do a screening interview with the Braintrust staff. We will answer your questions about the project, and our platform. If we determine it is the right fit for both parties, we'll invite you to join the platform and create a profile to apply directly for this project.

C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we'd welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.",https://www.indeed.com/rc/clk?jk=f1eafb81207cb347&fccid=cffd065f9ff9e672&vjs=3,"San Francisco, CA",Senior Engineer -Observability Space (Data Dog InfluxDB Grafana),Data Engineer
204,Marathon TS,"Marathon is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights.The Position is currently 100% remote, but need local candidates for occasional onsite meetings.The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients' toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.Responsibilities:Complete development efforts across data pipeline to store, manage, store, and provision to data consumersBeing an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practicesWrite code to ensure the performance and reliability of data extraction and processingSupport continuous process automation for data ingestAchieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testingWork with program management and engineers to implement and document complex and evolving requirementsHelp cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamworkCollaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialistsRequired Skills:Must be able to obtain a Public Trust Clearance7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formatsProficiency developing ETL processes, and performing test and validation stepsProficiency to manipulate data (Python, R, SQL, SAS)Strong knowledge of big data analysis and storage tools and technologiesStrong understanding of the agile principles and ability to apply themStrong understanding of the CI/CD pipelines and ability to apply themExperience with relational database, such as, PostgreSQLWork comfortably in version control systems, such as, Git RepositoriesDesired Skills:Experience creating and consuming APIsExperience with Client and knowledge of Client standards a plusCandidates will be given special consideration for extensive experience with PythonAbility to develop visualizations utilizing Tableau or PowerBIExperience in developing Shell scripts on LinuxDemonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutionsDemonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiencesJob Type: Full-timePay: $110,000.00 - $140,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insurancePaid time offTuition reimbursementVision insuranceSchedule:8 hour shiftApplication Question(s):Do you meet the US CITIZEN Clearance requirement?Are you local to the Washington DC metro area?Do you have experience in design, management, and solutioning of large, complex data sets and models?Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats?Do you have experience developing ETL processes, and performing test and validation steps?Do you have Proficiency to manipulate data (Python, R, SQL, SAS)?Work Location: One location",https://www.indeed.com/company/Marathon-TS/jobs/Data-Engineer-87a815180db72d5f?fccid=971bf8a08036ad35&vjs=3,"Remote in Washington, DC 20001 20001",Data Engineer,Data Engineer
205,CloudPareto,"Who is CloudPareto
CloudPareto develops digital products to accelerate government innovation. One of the fastest-growing startups supporting the public sector, CloudPareto is rapidly expanding its engineering, consulting, and go-to-market teams as we begin to enter into our next phase of growth. We have big visions for the future and are on the lookout for team members interested in making an outsized impact in the market.
The Team You Will Work With
This team member is part of our Engineering Department that utilizes the latest development principles and creates genre-defining products to challenge the status quo. Our products and tools bring innovation, user-centric design, and critical thinking. The Data Scientist, Associate will be tasked with creating innovative, customer-centric models in the Federal market, pushing the envelope on Machine Learning capabilities.
Position Details & Benefits:
Compensation: Up to $180,000 (commensurate with fit)
US citizenship required, Full-time, Salaried, Exempt
90-100% Remote
Unlimited PTO
85%-100% company coverage for BCBS/Guardian options on Health, Dental, Vision, Life, Short-Term Disability
100% match up to 6% of 401k contribution, vested immediately
Qualifications:
3+ years of industry experience working with the AWS Data Storage technologies such as S3, DynamoDB, Aurora, and ECS as well as data analytics platforms such as Databricks
Technical and Data Experience
Proficiency in at least one high-level programming language: Java, Scala, Python, R
Experience with the AWS Data Storage technologies such as S3, DynamoDB, Aurora, Redshift, and ECS.
Experience developing ETL pipelines between various AWS Storage capabilities.
Experience modeling and deploying relational, NoSQL, and graph data models
Experience in monitoring of CRUD performance of databases through cloud-native and third-party applications/tools.
Create, scale, and maintain data pipeline architecture across a cloud-based ecosystem
Demonstrated success in implementing and enforcing data standards and best practices
Implement data pipeline architecture decisions based on appropriate performance, cost, and security optimization best practices
Teaming Experience
Experience with highly agile projects including comfort with uncertainty and ambiguity
Partner with upstream engineering teams and downstream analytics teams
Work with Engineers, Product Managers, and Analytics personnel to specify product data requirements and strive for greater functionality in our data systems
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-design infrastructure for greater scalability, etc.
Participate in true agile development such with ceremonies such as scrum, bi-weekly sprints, and demos
Exercise strong written, diagraming, and verbal communication skills to translate complex technical concepts into simplified explanations and expand upon open-ended requirements
CloudPareto and its employees operate to fulfill the following core values:
#WorldClass: We create world-class products for our world-class Government
#MasterCraft: We collectively strive, together, to be true masters of our respective crafts
#WorkBackwards: We see the future, strategically envision our end state, and work backward with intent
#DoBeautiful: We believe that if it's worth doing, it's worth doing beautifully and elegantly
#SoftwareScale: We scale through software to be more efficient, effective, and precise
#Perseverance: We overcome the impossible through brilliance, grit, and class
#CelebrateUs: We celebrate triumphs; we celebrate differences; we celebrate balance; we celebrate us
#OneTeam: We are one team and we play to win big
Gi2Hmlq5cD",https://www.indeed.com/rc/clk?jk=6512bd755587887a&fccid=c765e8a755ec4d66&vjs=3,Remote,Senior Data Engineer,Data Engineer
206,Modivcare,"Modivcare is looking for a Data Engineer to focus on driving the creation and execution of a business intelligence and data strategy. They will work directly with business and technology team members and team member to analyze and develop solutions that provide applicable insights. You will ensure frequent and quality deliverables are deployed. You will report to the Data Engineering department.

You will...
Form and recommend standards for achieving maximum efficiency of the BI/DW ecosystem.
Help create an ongoing BI/DW strategy inclusive of existing and available technology.
Promote self-service BI and visual discovery while changing the Excel-based culture.
Work with Reporting and Data Scientists to define requirements.
Develop BI solutions based on an agile SDLC.
We are excited to speak to some with the following...
Bachelor's degree or equivalent experience.
5+ year experience with database technologies, ETL technologies, BI technologies or Cloud technologies.
This is an entry-level role and will provide on-the-job training.
Healthcare or insurance experience.
We value our team members and realize the importance of benefits for you and your family. Modivcare offers a comprehensive benefits package including the following:
3 Medical Plans with Prescription Drug Benefits
2 Dental Plans
Vision Benefits
Employer Paid Basic Life Insurance and AD&D
Voluntary Life Insurance (Employee/Spouse/Child)
Health Care and Dependent Care Flexible Spending Accounts
Pre-Tax and Post -Tax Commuter and Parking Benefits
401(k) Retirement Savings Plan with Company Match
Paid Time Off
Paid Parental Leave
Short-Term and Long-Term Disability
Legal Services
Critical Illness, Accident, Hospital Indemnity and Cancer Insurance
Tuition Reimbursement
Relocation Services
Employee Discounts (retail, hotel, food, restaurants, car rental and much more!)
Modivcare. Because we care...always.

5 brands have joined together for one calling and we recognized an important need-to improve access to care by addressing the social factors that influence patient health outcomes. Our roots as a logistics leader have laid the foundation for our transformation into a tech-enabled healthcare company making better connections to care.

$80,740 - 143,347/annually

Modivcare requires all employees to be vaccinated for COVID-19. Modivcare is an equal opportunity employer and consistent with federal, state, and local requirements, will consider requests for exemptions from this policy as required under the law.

We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.",https://www.indeed.com/rc/clk?jk=e313c6b402482dd1&fccid=52fe4df079aab7f7&vjs=3,"Denver, CO 80237 80237 (Southeastern Denver area)",Data Engineer III,Data Engineer
207,itexpertUS,"Job Title: Data EngineerLocation: Remote for now; Dallas TX (Hybrid-Model post Pandemic)Travel/Relocation/Remote: YesRequired Qualifications: Strong background in data processing software engineering and can build high quality scalable data-oriented productsIndustry experience working with distributed data technologies e.g., Hadoop MapReduce Spark EMR etc for building efficient large scale data pipelinesStrong Software Engineering experience with-in depth understanding of Python Scala Java or equivalentStrong understanding of data architecture modeling and infrastructureExperience with building workflows ETL pipelines Experience with SQL and optimizing queriesProblem solver with attention to detail who can see complex problems in the data space through end-to-end Willingness to work in a fast-paced environmentMS BS in Computer Science or relevant industry experience Strongly recommended but optionalExperience building scalable applications on the Cloud Amazon AWS Google Cloud etcExperience building stream processing applicationsSpark streaming Apache Flink Kafka etcPreferred Qualifications: Experience with data modellingProficiency in Python, Spark and the Hadoop ecosystemBachelor’s degree or equivalent requiredExperience with SQL and relational databasesSelf-starter, motivated, and good communication skillsStrong sense of ownership and driven to manage tasks to completionJob Type: Full-timePay: $100,000.00 - $120,000.00 per yearSchedule:Monday to FridaySupplemental Pay:Bonus payCOVID-19 considerations:yesExperience:SQL: 1 year (Preferred)Informatica: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/ITExpertUS/jobs/Data-Engineer-50f6eac16fd46b06?fccid=5c02ca9af48d7631&vjs=3,"Texas City, TX",Data Engineer,Data Engineer
208,ABC Infotech,"Client is looking for a Big Data Developer to build end to end business solutions and to work with one of the leading healthcare providers.The ideal candidate must possess excellent background on Big Data development.The candidate must possess excellent written and verbal communication skills with the ability and collaborate effectively with domain and technical experts in the team RequirementsShould have thorough understanding of Hadoop conceptsGood python skills following software engineering best practices Comfort & familiarity with SQL and Hadoop ecosystem of tools including spark Understanding of foundational Machine Learning concepts and some Deep Learning basicsCandidate with a mixture of Software Development, Data Engineering and Data Science skillsStrong Software Development and Engineering skills with some basic knowledge in Machine Learning and Deep LearningAbility to write efficient code-- Good understanding of core Data Structures/algorithms is critical for engine development5+ years' experience in any flavor of SQL dealing with complex queries, analytics and data models.3+ years' experience in any of the modern programming languagesMust have 4+ years of experience in SparkShould have Python programming experienceShould have experience in AWS services like Glue, S3, Athena, EMRShould have experience in Windows PowerShell scriptingGood Analytical skills and experience in cross functional team environmentHealthcare domain knowledge is preferredThe work will entail heads down coding, testing, data analysis, component packing/deploymentStrong SQL, Teradata, Unix, Strong standard Big Data/Hadoop skillset (Hadoop hive, Sqoop, hdfs, etc.) String Spark/python/scala3+ solid yrs experience with the above skillsetResponsibilitiesDesign, code, install, and maintain appropriate systems software program.This is a lower-level development role - Which means absolute hands-on expertise is mandatoryIdentify, evaluate, tailor, and direct the implementation of vendor-supplied software packages.Ensure the maintenance of adequate software systems documentation.Conduct quality assurance activities such as peer reviews and test plan developmentWork on sprint team in agile, rapid development and deployment environmentJob Types: Full-time, Contract, Temporary, InternshipPay: $83,055.00 - $189,396.00 per yearBenefits:Health insuranceSchedule:8 hour shiftSupplemental Pay:Signing bonusExperience:SQL: 1 year (Preferred)Informatica: 1 year (Preferred)Data warehouse: 1 year (Preferred)Work Location: Remote",https://www.indeed.com/company/ABC-Infotech/jobs/Big-Data-Engineer-bfe5c9c3a5f1e25a?fccid=126d8912f73b4ad9&vjs=3,Remote,Big Data Engineer,Data Engineer
209,Rackner,"Title: Sr. AWS Data Engineer

Location: Remote


Who We Are:

Rackner is a software consultancy that builds cloud-native solutions for startups, enterprises, and the public sector. We are an energetic, growing consultancy with a passion for solving big problems for both startups and enterprises. We are always learning and incorporating new things. We stay on the bleeding edge, appraising new technologies and incorporating them if they provide additional value to our customers. Our customers hail from a diverse, ever growing list of industries. We work with small startups, hypergrowth companies, and some of the largest enterprises.


Essential Functions:

Experience developing cloud-based software services and solutions.
Experience creating and driving large scale ETL pipelines in AWS based environment.
Experience with integration of data from multiple data sources.
Experience creating and driving large scale big data analytics pipelines.
Strong software development and programming skills with focus on data using Java, Python or other object-oriented languages.
Must Have Python.
Strong software development and programming skills using Python(PySpark)
Experience with AWS big data technologies : S3, Glue, EMR, Kinesis, RDS, Redshift, Athena
Experience with Hadoop and Apache Spark cluster and management
Strong SQL and database development skills, using RDBMS such as MySQL, PostgreSQL, AWS RDS
Experience with NoSQL databases such as AWS DynamoDB
AWS Services:EC2, S3, RDS, RedShift, KMS, Athena, Glue, Elastic Search, Lambda and EMR
Must Have: Python, AWS
Terraform Experience required


Core Competencies:

Customer Service
Building Relationships
Business Knowledge / Organizational Acumen
Self-Motivation/Self Starter
Leading Self and Others


Benefits:

401K with employer matching 100 percent up to 6 percent of employee 401K contribution
15 Days PTO (Paid Time Off) prorated based on your start date
10 Days Holidays
Health insurance (medical, dental and vision)
Basic Life Insurance, Short-Term and Long-Term Disability
Weekly Pay schedule (Fridays)


Rackner provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",https://www.indeed.com/rc/clk?jk=d4fd1590f086a804&fccid=07355c6f4e26f6aa&vjs=3,"Remote in Silver Spring, MD+1 location",Cloud Data Engineer,Data Engineer
210,HelioCampus,"HelioCampus, located in Bethesda, MD, Chapel Hill, NC, and Philadelphia, PA, is a data-driven tech company that serves higher education institutions. We empower institutional effectiveness through assessment, benchmarking and decision support via an enterprise analytics platform. Through three complementary lines of business, HelioCampus helps institutions measure and evaluate their effectiveness in fulfilling their mission by connecting investments with financial and student learning outcomes. With data from every corner of campus, HelioCampus takes a comprehensive, data-informed approach towards continuous improvement and learner success as a methodology to run an institution.
We are seeking a Data Engineer to join our technical team working on challenging data integration projects. We value innovative employees who have a sense of ownership in their work and perform well in a fast-paced, collaborative team environment. You will work on a combination of exciting new projects and initiatives while also working to improve and maintain HelioCampus’ existing solutions (using cloud technologies). You will become an integral part of a fast-growing team of ETL, BI developers and analysts dedicated to the success of higher education. This position is currently fully remote due to COVID but may require occasional work from Bethesda, MD when return to work is possible.
Key Accountabilities & Responsibilities:

Perform: SQL development, unit testing, and deployment of ETL solutions
Analyze: Trends in performance to proactively prevent problems
Troubleshoot: ETL issues in real-time and diagnoses the root cause


Required Skills & Qualifications:

1+ year(s) in ETL/SQL and related experience
Located within a commuting radius to the Bethesda, MD office or willing to relocate prior to starting the position


Preferred Skills & Qualifications:

Understanding of data warehouse concepts and structures
Developing SQL queries, analyzing data, systems integration
Translation of requirements into SQL specifications
Flexible and works well in a fast-paced and changing environment
Analytical and the ability to think on the fly
Ability to explore and find solutions with minimal supervision


Compensation & Benefits:

HelioCampus offers employees a competitive salary along with paid time off, healthcare, vision, dental, 401(k) w/ company match, WFH flexibility, snacks, and a fun and collaborative work environment.
HelioCampus is an equal opportunity employer all qualified candidates will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.",https://www.indeed.com/rc/clk?jk=f143d6f2aa4062f0&fccid=e7ea1965aba22dc8&vjs=3,"Temporarily Remote in Bethesda, MD+2 locations",Data Engineer,Data Engineer
211,"Antra, Inc","We have essentially three separate roles for which I am recruiting. The JDs of each is given below. Please go through and apply as per your interest. You shall be contacted once you have applied.React Developer role (5+ vacancies)In this role as React Developer, you should be passionate about developing solutions to achieve business needs and will be involved in all aspects of the software development lifecycle including technical design, implementation, testing, deployment and support of cutting-edge applications.Responsibilities: · Provide value by integrating business rules and content in accordance with requirements· Work closely with Product Owners, Analysts and QA in an Agile environment to ensure quality, security and maintenance of applications, and to ensure code meets development standards and guidelines.· Support design, development, testing and deployment of software solutions using JavaScript/Typescript and Angular Framework for the client side development and using Java and Spring Framework to develop the server side business logic of the application.· Develop, document, and advocate SOLID software architecture practices· Contribute to a DevOps culture and development of continuous integration processes and tools.· Ability to jump between frontend and backend work· Adhering to established methodologies while continuously analyzing processes for improved performance and adaptability.· Contribute to the success of your team and companyJob Requirements: - Graduate degree or undergraduate degree in Computer Science, Computer Engineering, Information Technology, Information Systems, Software Development, Electrical and Computer Engineering , Electrical Engineering or relevant experience.- 0-3 years of software development with knowledge of software development life cycle- Knowledge of Object Oriented Programming with some experience in any OOP programming languages like Java.- Knowledge on fundamentals of HTML, CSS and JavaScript.- Knowledge of web services with JSON and REST, SOAP/XML is a plus.- Knowledge of any frameworks such as NodeJS, Angular or ReactJS is a plus.- Knowledge on JavaScript ES6 features such as classes, arrow functions, string templates, rest/spread operators, promises, async/await is plus.- Experience with designing and developing RESTful web services is a plus.- Be a self-starter with an ability to troubleshoot and seek answers to problems.- Have hands-on design and development of software applications/systems.- Proven analytical skills with excellent oral and written communication skills.- Responsible for coding, testing, and implementation of solutions within established standards and documentation guidelinesxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxJD for Java backend Developer (15+ vacancies)In this role, you will design, develop, modify, adapt and implement short- and long-term solutions to information technology needs through new and existing applications, systems, databases and applications infrastructure. You will review and interpret system requirements and business processes. You will code, tests, debug and implement software solutions.Responsibilities: · Utilize Java and related technologies to design, develop, test and deploy various large-scaled, distributed server side applications.· Be involved in all phases of software development life cycle.· Gather and analyze requirements from Product Managers/Owners.· Build tools to improve application reliability and quality, and programmer productivity.· Align teams designs with larger architecture objectives.· Assist team members in design discussions and decisions.· Perform code and design reviews with other team members.· Create and execute test cases based on test strategies and test plans· Work closely with Operations & Infrastructure groups to understand challenges in production environments.Qualifications: - Bachelor's degree in Computer Science or related fields.- 0 to 3 years of experience in Core Java programming.- Proficient in data structure, algorithm, object-oriented design and multithreading.- Experience in basic database design including SQL database or NoSQL databases.- Familiar with Java 8 new features like Lambda and Stream.- Knowledge about network protocols and network security.- Basic knowledge about design patterns.Preferred Qualifications: - Experience in Agile methodologies.- Experience in Web Application development.- Knowledge in distributed system design.- Experience in Cloud Platforms like AWS, Azure or similar.- Experience in DevOps and related tools like Jenkins, Maven, GitLab and shell script.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxJD for Azure Data Engineer (15+ vacancies)This role requires the design, development and implementation of data solutions to business problems. An Azure Data Engineer will be expected to perform duties such as: evaluating the performance of current data solutions, designing and implementing cloud and hybrid data solutions. Ability to adapt and learn new technologies per business requirements is also needed.Responsibilities: · Design and implement data solutions using industry best practices.· Performs ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards.· Monitor and maintain data pipelines proactively to ensure high service availability.· Works with Data Scientists and ML Engineers to understand mathematical models and optimize data solutions accordingly.· Continuous development through training and mentorship programs.· Create scripts and programs to automate data operations.You meet our “must haves” for this role if you have: · Minimum Bachelor’s degree in Data Science, Business Intelligence, Computer Science or related fields, or the equivalent combination of education, professional training, and work experience.· 0-3 years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala and R.· Experience working with relational databases such as SQL Server, Oracle and MySQL.· Experience working with NoSQL databases such as Redis, Mongo DB, Cosmos DB.· Excellent problem-solving skills and ability to learn through scattered resources.· Thorough understanding of the responsibilities and duties of a data engineer, as well as established industry standards/best practices and documentation guidelines.· Outstanding communication skills, and the ability to stay self-motivated and work with little or no supervision.· Authorization(s) to work lawfully in the United States.Plus, if you meet any the of requirements: · Experience with cloud based data technologies.· Experience with distributed systems utilizing tools such as Apache Hadoop, Spark or Kafka.· Working experience in Agile Scrum environments.· Experience with source control tools such as Git, SVN and TFS.Job Types: Full-time, ContractPay: $60,000.00 - $75,000.00 per yearBenefits:Dental insuranceHealth insuranceLife insurancePaid time offRelocation assistanceVision insuranceSchedule:8 hour shiftDay shiftMonday to FridayCOVID-19 considerations:All staff members are adhering to COVID appropriate behavior while on premises at office, not limited to wearing of masks, partial attendance at office, proper social distancing, sanitizing common places and oneself regularly, etc.Ability to commute/relocate:Centreville, VA: Reliably commute or willing to relocate with an employer-provided relocation package (Preferred)Application Question(s):As this is an entry level position, would you be willing to avail free in-house training specific to the job, before project placement?Work Location: Multiple Locations","https://www.indeed.com/company/Antra,-Inc/jobs/Entry-Level-Software-Data-Engineer-Position-1df1885f1867907c?fccid=550311fe7e2165d9&vjs=3","Fairfax, VA",Entry level software and data engineer positions,Data Engineer
212,Carrot Fertility,"About Carrot:

Carrot Fertility is the leading global fertility benefits provider for employers and health plans, built to support employees through their lifelong fertility healthcare journey. Companies use Carrot to customize a fertility benefit that provides employees financial, medical, and emotional support as they pursue parenthood, reducing healthcare costs and resulting in better clinical outcomes.
The Role:

Carrot is seeking a Data Engineer to join our rapidly growing Business Intelligence function as it scales to meet - and anticipate - the needs of the organization and its clients: we solve mission-critical, complex data and analytics problems and we need your help.


This is an executional role providing essential support across the organization. Specific job responsibilities will include:

Building and maintaining core data infrastructure, with an emphasis on making systems robust and scalable via tests, CI/CD, monitoring and alerting, etc.
Transforming and loading large volumes of data into our data warehouse
Automating regular reporting requests and identifying opportunities to automate other manual tasks
Improving and extending tooling available to data analysts for their work

The Team:
The Business Intelligence team is a highly cross-functional team that is central to Carrot's long-term success. The team is led by our Director of Business Intelligence and includes data engineers, data scientists, and business intelligence analysts.

Minimum Qualifications:

2+ years Python development experience
2+ years SQL experience
Experience working with common data formats (JSON, XML, etc.)
Experience working with version control systems (Git preferred)
Strong understanding of relational modeling
Basic knowledge of data warehouse administration

Preferred Qualifications:

Experience building end-to-end ETL or ELT pipelines
Experience working with workflow orchestration tools (e.g. Airflow)
Experience working with AWS cloud services

Why Carrot?

Founded in 2016, Carrot now supports 400+ companies in more than 60 countries across North America, Asia, Europe, South America, and the Middle East. Carrot has been honored by CB Insights as one of the world's most promising private digital health companies, named to Fierce Healthcare's Fierce 15 list highlighting the most promising healthcare companies, and recognized by Fast Company as a World Changing Ideas honoree, which spotlights companies that are tackling society's biggest problems.",https://www.indeed.com/rc/clk?jk=8301c221dc812051&fccid=c7e823bd82156eed&vjs=3,"Remote in San Francisco, CA",Data Engineer,Data Engineer
213,"Antra, Inc","Antra started its services in the year 2008, since then we have been diligently serving every need of our clients professional and software solutions. With our unique amalgam of innovation and inspiration, backed by Agile processes, we deliver world-class services with our core values to our clients. We identify unique strategies to transforming organizations through picking their sweet spot for innovative thinking and helping them in implementing in-depth cultural shift; ultimately augmenting the innovation process to the next level – fostering the innovation race that never ends.This role requires the design, development and implementation of data solutions to business problems. An Azure Data Engineer will be expected to perform duties such as: evaluating the performance of current data solutions, designing and implementing cloud and hybrid data solutions. Ability to adapt and learn new technologies per business requirements is also needed.Responsibilities: Design and implement data solutions using industry best practices.Performs ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards.Monitor and maintain data pipelines proactively to ensure high service availability.Works with Data Scientists and ML Engineers to understand mathematical models and optimize data solutions accordingly.Continuous development through training and mentorship programs.Create scripts and programs to automate data operations.You meet our “must haves” for this role if you have: Minimum Bachelor’s degree in Data Science, Business Intelligence, Computer Science or related fields, or the equivalent combination of education, professional training, and work experience.0-3 years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala and R.Experience working with relational databases such as SQL Server, Oracle and MySQL.Experience working with NoSQL databases such as Redis, Mongo DB, Cosmos DB.Excellent problem-solving skills and ability to learn through scattered resources.Thorough understanding of the responsibilities and duties of a data engineer, as well as established industry standards/best practices and documentation guidelines.Outstanding communication skills, and the ability to stay self-motivated and work with little or no supervision.Willing to relocate to any US location on Antra projects location.Authorization(s) to work lawfully in the United States (OPT/CPT is acceptable).Plus, if you meet any the of requirements: Experience with cloud-based data technologies.Experience with distributed systems utilizing tools such as Apache Hadoop, Spark or Kafka.Working experience in Agile Scrum environments.Experience with source control tools such as Git, SVN and TFS.The intent of this position description is to provide a representative summary of the major duties and responsibilities performed by incumbent(s) in this position. Incumbent(s) may not be required to perform all duties in this description and incumbent(s) may be required to perform position-related tasks other than those specifically listed in this description.Antra Inc. is an equal opportunity employer, providing equal employment opportunities (EEO). All employees and applicants for employment will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity or expression, pregnancy, protected veteran status or other status protected by law. This policy applies to all terms and conditions of employment.Job Types: Full-time, ContractPay: $60,000.00 - $68,000.00 per yearBenefits:401(k)Dental insuranceHealth insuranceHealth savings accountLife insurancePaid time offProfessional development assistanceReferral programRelocation assistanceVision insuranceSchedule:Monday to FridayAbility to commute/relocate:Washington, DC: Reliably commute or planning to relocate before starting work (Preferred)Education:Bachelor's (Preferred)Work Location: Multiple Locations","https://www.indeed.com/company/Antra,-Inc/jobs/Entry-Level-Azure-Data-Engineer-4df2d21157b6b060?fccid=550311fe7e2165d9&vjs=3","Washington, DC+1 location",Entry-Level Azure Data Engineer,Data Engineer
214,Dexai Robotics,"Dexai is revolutionizing the food industry. Alfred, our robot sous-chef, combines a sleek, hygienic robot and with unique machine-learning algorithms to provide seamless and efficient automated food-preparation for our customers.
We are a team of hardworking and easygoing engineers, roboticists, and business professionals, and are looking for exceptional people who enjoy solving difficult problems and want to teach robots how to make food.
Dexai is looking for a Software Engineer to join our team. The ideal candidate has experience in writing big data systems or on machine learning infrastructure.
We have:
A team of experienced engineers and roboticists from both academia and industry, who love teaching as much as they love learning
We utilize robot arms equipped with RGBD sensors to apply various ML techniques like Reinforcement Learning to novel problems
Our code is primarily written in C++/Python/Kotlin
An idea meritocracy, always looking for ways to improve our system
An inviting office space with free snacks, team lunches, and frequent social events
You have:
Expert knowledge in SQL databases and high-performance big data solutions (e.g. Spark, Hadoop)
Proficiency in CI/CD and automated workloads for code, data, and models
Experience with supporting full cycles of ML development and deployment (e.g. via RESTful API as a service)
Working python skills to support custom data pipelines
Working knowledge of cloud native system (we use AWS)
Passion for continuous learning and staying on the cutting-edge
Desire to hold work products to highest standards and automate enforcement
Self-initiative in an Agile environment
You might also have:
Experience with Kubernetes
Experience with workflow management, e.g. Airflow, Kubeflow
Experience with distributed reinforcement learning
Contributions to open-source projects
We welcome candidates from any location, and are open to remote roles.",https://www.indeed.com/rc/clk?jk=54debd34653a71af&fccid=04ea82cfce19ecb8&vjs=3,"Boston, MA",Data Engineer,Data Engineer
215,Remote,"About Remote

Remote is solving global remote organizations' biggest challenge: employing anyone anywhere compliantly. We make it possible for businesses big and small to employ a global team by handling global payroll, benefits, taxes, and compliance (learn more about how it works). We're backed by A+ investors and our team is world-class, literally and figuratively, as we're all scattered around the world.

Please check out our public handbook to learn more about our culture. We encourage folks from all ethnic groups, genders, sexuality, age and abilities to apply. You can also check out independent reviews by other candidates on Glassdoor. If this job description resonates with you, we want to hear from you!

All of our positions are fully remote. You do not have to relocate to join us!

How we work

We love working async and this means you get to do your own schedule.

The Remote Engineering team uses a simples approach to productivity and task management and you can read more about it in The Remote Flow.

We empower ownership and proactivity and when in doubt default to action instead of waiting.

The position

As a data engineer, you will be the link between data producers and data consumers at Remote. You'll primarily focus on building out our data pipeline to unify our various data sources in a compliant manner. That being said, you should also be able to jump in as needed and help deliver consumable data to internal users.

Requirements

Must have (professional experience)

3+ years of experience with SQL (we use PostgreSQL at Remote)
3+ years experience with data pipeline tools, e.g. Meltano or Stitch
Experience with BI Tools e.g. Metabase, Looker, or Tableau
Key responsibilities
Maintain our data pipeline by scheduling extractors within Meltano, and handling errors.
Writing custom extractors in Python for our Meltano ELT pipeline.
Writing transformations using DBT.
Identify and address data quality issues.
Build documentation around our tools.
Work with stakeholders to get them the data they need while maintaining safe data access.
Work with stakeholders to build the necessary data pipelines to get people the data they need.
Building a clear vision for the needs of the data team, and how to improve our process.
Remote Compensation Philosophy

Remote's Total Rewards philosophy is to ensure fair unbiased compensation and fair pay along with competitive benefits in all locations in which we operate. We do not agree to or encourage cheap-labour practices and therefore pay a minimum annual salary of USD 40,000 per year, in all locations throughout the world. Actual compensation may vary based upon geographical location, experience, and/or skill level. However, it will never be below our minimum global compensation mentioned.

Benefits

You can learn more about the benefits we're offering to all internal employees at Remote by visiting our public Benefits & Perks Handbook page.

Practicals
You'll report to: Head of Automation
Team: Automation
Location: Anywhere in the World
Start date: As soon as possible
Application process
(async) Profile review
Interview with recruiter
Interview with future manager
(async) Small challenge
(async) Challenge Review
Interview with team members (no managers present)
 Prior employment verification check(s)
(async) Offer
How to apply

Please fill out the form below. Thank you!",https://www.indeed.com/rc/clk?jk=89d9adce43db4a62&fccid=dceb3c3fdc10eef9&vjs=3,"Remote in Washington, DC",Data Engineer,Data Engineer
216,Blue State,"The role

As a Data Engineer at Blue State, you'll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations. You'll design, build, and manage the systems and processes which form the underpinning of Blue State's analytics work, supporting and working alongside data analysts and campaign strategists. But you'll also work directly with Blue State's clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations.

Day-to-day responsibilities:

Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects
Work with Blue State's client organizations to solve difficult data migration, management, and integration challenges
Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes
Automate workflows and look for further opportunities to improve efficiency in our work
Always maintain a high level of data security and privacy
The team

You will be a part of the global Web and Product Development team working primarily with our creative agency on client projects. You'll work in either the NY or DC office.

Once Blue State offices reopen, on-site presence is strongly preferred at a minimum of two days a week. To enter our US offices or attend Blue State events, staff and visitors must be fully vaccinated against COVID-19, including with a booster shot when eligible. Exceptions for protected grounds will be reviewed on a case-by-case basis.

Top things we're looking for
Good foundational understanding of statistical analysis
Extensive experience working with SQL databases in an analytics or business intelligence context
Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools
Experience with ETL/ELT tools, processes, and best practices
Strong Python experience:
Python should be your go-to tool for solving problems. If the first thing you want to do when you have to do the same thing twice is write a Python script to automate it - we want you!
Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus
Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks
Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:
Designing data warehouse schemas for cross-channel marketing analytics
Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data
Building and running automated tasks within the GCP environment - e.g. Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler
Comfortable managing GCP IAM policies across projects and teams
Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if you've extended Google Sheets using Google Apps Script
Familiarity with Git and maintains good habits around code maintenance
Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)
Good at teaching others what you know.

At Blue State, diversity is a necessity, not a nice-to-have. We encourage those from underrepresented communities — women, people of color, LGBTQIA+, immigrants, indigenous folks, those with disabilities and people at all the intersections in between — to apply. Even if you don't think your current skill set checks every box below, but this role seems to align with your strengths, we want to hear from you.

The minimum starting salary for this position is $90,000; compensation is otherwise commensurate with experience.

The company

Blue State is a values-led creative and campaigns agency that partners with leading causes, companies, and campaigns to build better organizations for a better world. We drive real change, make good trouble, put people first and are constantly curious.

We believe that there is no force more powerful than people taking collective action on the things they care about. We don't pretend to have all the answers, but we know where to find them: in people. We listen, learn, and uncover new insights that often surprise us and our clients — and move us toward better results. Across clients including UNHCR, Amnesty International, Google, Tesco, Nesta, and Tate. We have offices in New York City, Washington DC, London, Oakland and Chicago.",https://www.indeed.com/rc/clk?jk=7359029ebacaafc6&fccid=4d9339102788fdca&vjs=3,"Washington, DC 20005 20005 (Downtown area)+2 locations",Data Engineer,Data Engineer
217,Saama Technologies Inc,"The Collibra Analyst/Modeler will support the development of the organization’s metadata operating model
This position will design the build and collaborate to enhance and maintain the data catalog, policy and standards library, data lineage, data quality, privacy, and risk dashboards
The Collibra Analyst/Modeler will also translate metadata usage into business-enabled processes
This role will promote data accountability, validate compliance, and track data governance effectiveness through scorecards and status reports
Create data management processes to be automated in Collibra
Perform business requirements analysis for business rules, business processes, and documents associated with population, maintenance, and completeness of metadata
Design, maintain and support application, business glossary, business intelligence, data integration, data modeling, and database management resources in Collibra
Design Collibra Privacy model and processes
Design processes and metamodel linkage between technical Business Glossaries and Collibra assets
Validate completeness of data lineage and design processes for implementation to certify Tier 1 CDEs and Reports
Provide ongoing support for metadata implementation, testing and integration
Design and maintain metadata standards and procedures
Create requirements and solution architecture for all Collibra implemented and manual processes, including issues, catalog, lineage, privacy data quality, and risk
Devise improvements to current procedures and develop methods for increasing efficiency, accuracy, and performance of metadata solutions
Designing Dashboard and Tableau integrations for Collibra platform
Conduct periodic reviews of the data catalogue to ensure compliance with data governance standards and policies
Participate in a variety of activities related to data governance, data strategy implementation, data modeling, data requirements, risk assessment, identification of data challenges, modeling, data manipulation, and/or requirement elicitation",https://www.indeed.com/rc/clk?jk=79cdafec3a78fc6b&fccid=f7cbfaf9ddc3fe8a&vjs=3,"Phoenix, AZ 85021 85021 (North Mountain area)+3 locations",Data Engineer - Collibra,Data Engineer
218,Marmon Aerospace & Defense,"Responsibility: Maintain product manufacturing specifications.Create new or revise specifications including base document and ERP data.Manage customer and industry specification files, and communicate changesCoordinate new product approvalsEnter new product conformance to Company Quality, Customer and Industry specifications and standards.Specify new raw material items and associated items for product completion.Ensure product conformance to Company Quality, Customer and Industry specifications and standards.Product design and manufacturing specifications.Perform miscellaneous job-related duties as assigned.Provide technical guidance designs into CAD software and prepare cost estimates and product drawings.Qualifications: MS WordBasic Computer KnowledgeGood math and technical skillsDesired Qualifications: 3+ years wire and cable experienceAS DegreeAutocad experienceJob Type: Full-timePay: From $55,000.00 per yearSchedule:8 hour shiftWork Location: One location",https://www.indeed.com/company/Marmon-Aerospace-&-Defense-&-Kitco-Fiber-Optics/jobs/Data-Engineer-1ac667a9adf0414d?fccid=167c394d4fb1b054&vjs=3,"Naples, FL 34104 34104",Data Engineer,Data Engineer
219,IT Alliances Inc,"HiThis is Prince from IT-Alliances Inc I have a job requirement on full-time/permanent. Interested can send me the updated resume.Title: - Data Engineer.Location: - Dallas, TX.Duration: - Fulltime/Permanent.Job Description:Strong background in data processing & software engineering and can build high-quality, scalable data oriented products.Industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, EMR, etc..) for building efficient, large-scale data pipelines.Strong Software Engineering experience with in depth understanding of Python, Scala, Java or equivalentStrong understanding of data architecture, modeling and infrastructureExperience with building workflows (ETL pipelines)Experience with SQL and optimizing queries.Problem solver with attention to detail who can see complex problems in the data space through end to endWillingness to work in a fast paced environmentMS/BS in Computer Science or relevant industry experience.Strongly recommended (but optional)Experience building scalable applications on the Cloud (Amazon AWS, Google Cloud, etc..)Experience building stream-processing applications (Spark streaming, Apache-Flink, Kafka, etc..)Job Summary & ResponsibilitiesDesign and develop data ingest and transform processesDevelop data models to provide standardized reporting solutions to the firmDevelop automation, governance and reporting solutions to provide firm and regulatory mandated controlsWork as part of a global team using Agile software methodologiesPartner with Marcus risk, product, acquisition and servicing teamsUse Marcus data to drive change throughout the Marcus businessSkills And Experience We Are Looking For: -Bachelor’s degree or equivalent required.Minimum 5 years of relevant professional experience.Experience with SQL and relational databases.Self-starter, motivated, and good communication skills.Strong sense of ownership and driven to manage tasks to completionPreferred Qualifications: -Experience with data modellingProficiency in Python, Spark and the Hadoop ecosystemJob Type: Full-timeSalary: $100,000.00 - $140,000.00 per yearSupplemental Pay:Bonus payWork Location: RemoteSpeak with the employer+91 732.666.0012",https://www.indeed.com/company/IT-Alliances-Inc/jobs/Data-Engineer-2485cff79ebe217c?fccid=1703af95672f1f50&vjs=3,Remote,Data Engineer,Data Engineer
220,Synaptein Solutions,"Position: Data Engineer

Location: Thousand Oaks, CA (Remote Position Till Covid)

Duration: 11 + Months


Job Description:

Proven proficiency with scripting languages such as Python, Ruby, and JavaScript, and build tools such as Jenkins, Maven, Ant, Gradle or Ivy
Experience with Spark, Hive, Kafka, Kinesis, Spark Streaming, and Airflow, Azure DataBricks.

Mandatory Skills:
Python
DataBricks
Pig
Hive
Spark
Aws

Regards,

Anup Kumar

Office – 703-310-7171",https://www.indeed.com/rc/clk?jk=9497189b69b1436b&fccid=e5a1823a3cd67c3b&vjs=3,"Thousand Oaks, CA",Data Engineer,Data Engineer
221,Experian,"Company Description
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been name in the 100 “World’s Most Innovative Companies” by Forbes Magazine. In this era of disruptive digital transformation, delivering new and innovative customer experiences is a top priority for businesses worldwide. At Experian, a worldwide decisioning leader, we make this a reality by uniting data, advanced analytics and decision strategies with real-time operational execution and strong decision governance to deliver meaningful customer outcomes and positive business results.

Job Description

Carry out the extraction, formulation and manipulation of data to create the structured samples necessary to address business requirement.
Interpret data specifications
Ensure quality and accuracy of own work
Produced analysis is accurate and completed to agreed timescales, resulting in positive feedback from client/customer
Follow Business Line procedures and processes throughout the data extraction, formulation and manipulation process, ensuring all work is produced to the agreed specification, and meets all requirements
Take responsibility for the production of high quality results documentation
This role is responsible for carrying out elements of data manipulation and reporting, quality assurance, documentation and internal/client communication. The role holder is able to work independently.
Identify and resolve data problems, producing appropriate analysis to verify results
Identify potential gaps in own technical/business knowledge that might compromise the deadlines. Work with Project Managers or Consultant to plan timely steps to fill gaps
Challenge existing processes and recommend improvements
Ensure the quality of all data sent to clients/customers (e.g. project development samples; retrospective bureau data and Bureau Score data (if applicable))
Analyze and confirm the integrity of source data to be evaluated
Input into the review and assessment of client/customer requirements to produce a data project specification
With direction, produce data specifications
Liaise with other internal and external project management groups to ensure project is delivered as part of complete solution
Input into the accurate assessment of the work that needs to be undertaken to define and maintain a project plan
Plan, organize and complete own workload to meet project commitments within agreed schedules
Provide pro-actively status reports and progress updates to Project Manager or Consultant as required, for all projects/tasks
Contribute to the preparation of regular project status reports to both internal and external parties.
Proactively identify potential risks in achieving project timescales, taking pro-active action to minimize risk
Recognize when issues or problems need to be escalated
Maintain accurate time recording to projects in the appropriate internal time tracker software
Effectively use project management/planning tools
Proactively use time recording data to identify inefficiencies in project work and recommend improvements
Implement appropriate quality control procedures for all analysis reporting and scoring
Reduce error rate and re-run frequency. Meeting both the analytical and business objective

Qualifications

Willingness to learn data extraction, formulation and manipulation methods
Willingness to develop strong investigation skills. Ability to identify and correct data problems
PC/MF literate with experience in IT packages. Knowledge of statistical (e.g., SAS) packages desirable
Knowledge of Excel VBA
An innovative and inquisitive mind, focused on addressing and solving data problems
Common sense/logical approach to mathematical reasoning
Strong communication skills, including proficiency in writing reports and presenting technical work
Strong personal planning and time management skills
Strong interpersonal skills
Strong collaboration and teamwork
Commercially focused
Language in relevant marketplace
English language good knowledge
Additional Information

During this pandemic, all Experian employees are working remotely. Once it’s safe to do so, we’ll slowly return to our offices however we are in no rush to do this. The safety of our employees is of utmost importance.
Competitive pay and comprehensive benefits package
Flexible work schedule and relaxed dress code
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian U.S. employees are required to be fully vaccinated for COVID-19.
Experian Careers - Creating a better tomorrow together
Find out what its like to work for Experian by clicking here",https://www.indeed.com/rc/clk?jk=314cbd35d87dd902&fccid=75a3a5a15b202084&vjs=3,"Costa Mesa, CA 92626 92626+1 location",Data Engineer,Data Engineer
222,Legal Services Corporation,"ABOUT LSC:
Established by Congress in 1974, the Legal Services Corporation (LSC) is the country’s single largest funder of civil legal aid for low-income Americans. LSC currently funds 132 independent nonprofit legal aid organizations with more than 800 offices throughout the nation. LSC’s mission is to promote equal access to justice and provide grants for high-quality civil legal assistance to low-income Americans.
The Office of Data Governance and Analysis builds data products to help the access to justice community make better decisions about programs and services that impact the lives of low-income Americans. ODGA data scientists, engineers, and research associates use a variety of software and analytical technologies to provide internal stakeholders and civil legal aid community with high-quality analyses and tools.
This position will require working from an office in Washington, D.C., but is teleworking until LSC returns to in-office work. The successful candidate must live in the Washington, D.C. metropolitan area.
LSC requires that all employees be up to date with COVID-19 vaccines, including any booster dose(s) when eligible, except as required by law. Any employment offer will be contingent upon satisfactory proof that you are up to date with your COVID-19 vaccines, including any booster dose(s) when eligible, subject to reasonable accommodations for medical or religious reasons, and/or as otherwise required by applicable law.
BASIC FUNCTION:
Under the direction of the Chief Data Officer, or their designee, the Data Engineer is responsible for technical operation of cloud architecture for the Civil Court Data Initiative. The Data Engineer works closely with research associates, data scientists, web developers, and other staff to support the ongoing operation, continued expansion, and usage of enterprise data. This is a one-year contract position.
LSC’s Civil Court Data Initiative: LSC launched the Civil Court Data Initiative in 2019 to collect civil court records in near real-time to provide legal aid providers, researchers, and policymakers with the data necessary to understand and address national civil legal issues. LSC has collected over 20 million civil court records filed since 2016 in over 20 states, covering over 30% of the U.S. population. This database includes information on eviction, debt collection, domestic violence, and other civil legal issues. LSC’s Office of Data Governance and Analysis [ODGA] is developing web applications, such as an eviction tracker, to deliver analytics about legal needs to legal aid providers and other audiences.
PRINCIPAL DUTIES AND RESPONSIBILITIES:
Collaborates with ODGA staff to determine the technical requirements needed to make high-quality civil court data accessible to various end users.
Develops, tests, and maintains software applications for collecting and parsing unstructured and semi-structured data in a serverless cloud environment.
Contributes to the development of the organization’s first data discovery platform to make data accessible for technical and non-technical audiences.
Identifies new opportunities to automate and simplify workflows to improve team productivity.
Develops source code and data documentation for software and datasets, with an emphasis on scalable and automated solutions.
Contributes to reports, blogs, visualizations, and other material, as necessary.
Performs other related duties, as assigned.
COMPETENCIES REQUIRED:
General:
Must demonstrate initiative and be an effective problem-solver; willingness to explore and learn new technological tools and software; excellent customer-service focus; strong and effective organizational and time-management skills; strong oral and written communication skills to communicate effectively with broad and diverse audiences including staff, grantees, and applicants; demonstrates accuracy and thoroughness and monitors work to ensure quality; unimpeachable integrity and personal ethics; adaptable and able to handle multiple projects simultaneously and adjust to shifting and changing priorities under tight deadlines; strong attention to detail; works well independently and with others as a team member; makes sound, well-informed, and objective decisions; influences, motivates, and challenges others; understands LSC’s mission, functions, and operations; identifies and understands trends that affect access to legal services for low-income persons.
Technical/Specialized:
A bachelor’s degree. Must have at least one year of experience related to data science or software development for a Data Engineer I position or at least three years of experience related to data science or software development for a Data Engineer II. Experience building ETL/ELT processes, data pipelines, and data management solutions (dbt, Great Expectations) in AWS preferred. Experience in any of the following areas highly preferred: open-source data orchestration platforms (Prefect, Airflow), data discovery and metadata engines, and Infrastructure as a Service (IaaS) tools (AWS CDK, Terraform).
SALARY AND BENEFITS:
Data Engineer I Salary: $74,058; Step 1
Salary Range: $74,058 - $77,049.
Data Engineer II Salary: $85,207; Step 1
Salary Range: $85,207 - $89,294
The employee is eligible to participate in LSC’s medical plan, 403(b) thrift plan, and other employee benefits.
APPLICATION PROCESS:
You must apply on our website. To have your application considered, you must answer all questions and provide all requested information. Incomplete applications will not be reviewed. Your application must include your résumé and a cover letter explaining why you are a good fit for this position. Apply at: www.lsc.gov/about-lsc/careers.
Note: If you need an accommodation during the application process, please notify OHR at jobapplicant@lsc.gov.
DIVERSITY, EQUITY, AND INCLUSION STATEMENT:
We are committed to diversity, equity and inclusion. Our differences fuel excellence, and we strive to create an environment where every individual is valued and feels empowered to bring their full, authentic self to work. We are building a community rooted in openness and trust where colleagues have the resources to grow, thrive and fully contribute to achieving equal access to justice.
LSC is an equal opportunity employer.",https://www.indeed.com/rc/clk?jk=b3bb736f1460974c&fccid=6b74217f01b3c5c2&vjs=3,"Washington, DC 20007 20007 (Georgetown area)",Data Engineer I or II (Exempt),Data Engineer
223,Spoiler Alert,"RoleSpoiler Alert is looking for a Data Engineer to drive the quality and accessibility of data across our company. The primary responsibility of this role is ensuring that teams across the company are reliably sourced with accurate data from our automated data pipelines. You will partner with teams and team leads across Spoiler Alert to ensure their primary metrics are always sourced with accurate data. As an early member of our data team, you will have a material impact on fostering our company's DataOps culture to be secure, scalable, and simple.Today our data stack includes MongoDB, Fivetran, dbt, BigQuery, and Looker. Our platform is trusted by global CPG leaders; our solution has helped to recover more than 200 million pounds of food. Your data engineering at Spoiler Alert will enable wins for customers, Spoiler Alert, and the food supply chain at large. This is a unique opportunity to help uncover data insights that make an impact on climate change and access to affordable food.ResponsibilitiesOwn existing data architecture on GCP infrastructure that serves real-time and analytical data requirementsCollaborate with stakeholders across the business including data, product, customer success, and leadership teams to support their data delivery needsDefine data engineering best practicesBuild data pipelines necessary for supporting business and analytics applicationsIdentify, design and implement internal process improvements including re-designing infrastructure to support product strategy and business needsPartner with data analysts to automate processes that improve data sets for analytical and reporting needsQualifications3-6 years experience working with cloud based data pipelines and data automation toolsExpert in translating data objectives between business and technical stakeholdersSkilled at designing data solutions using cloud platforms like GCPComfortable building and maintaining data pipelines involving MongoDB, Fivetran, DBT, BigQuery and Google Pub/SubProfessional experience with SQL and comfortable scripting in Python or a similar programming languageFamiliar with the Looker ecosystem including building, maintaining, and debugging Looker modelsExperience with container management frameworks like Docker and KubernetesAbout Spoiler AlertSpoiler Alert is a Boston-based software company helping perishable CPG brands manage excess and slow-moving inventory. Working exclusively at the manufacturing plant or distribution center level, Spoiler Alert offers a best-in-class B2B sales platform that enables food & beverage brands to manage their liquidation processes across a private network of discount retailers and nonprofit channels - with a heavy focus on maximizing value recovery, strengthening customer relationships, and increasing the effectiveness of supply chain managers tasked with handling these typically manual sales processes. Founded by alumni of MIT, Spoiler Alert works with some of the world's largest brands, including Campbell's, Danone, HelloFresh, KeHE Distributors, and Kraft Heinz, along with a growing network of discount channels committed to increasing affordable food access.While we care deeply about our customers' financial and operational improvements, our team's hearts and minds are motivated by the impact our work has in addressing some of the largest environmental and social opportunities of our generation - at the intersection of food waste, resource deficiency, climate change, affordable nutrition, and hunger. We couldn't be more excited to be combining critical technology breakthroughs - including workflow automation, augmented intelligence, wholesale ecommerce, traceability, and supply chain management - to an industry that is so critical for daily life.Born out of MIT in 2015, Spoiler Alert is headquartered in Boston's Back Bay neighborhood and is backed by some of the nation's leading food, agriculture, and supply chain investors. For more about Spoiler Alert, visit spoileralert.com or our social channels: Twitter & LinkedIn.BenefitsSpoiler Alert is committed to providing equal employment opportunities for all applicants and employees. We offer a remote-first work environment, with the flexibility to work remotely or in-person from our Boston headquarters. Employee benefits include the following:Competitive salary and stock option packagesSubsidized health, dental, and vision insurance plansFlexible spending and dependent care accountsClimate-focused, pre-tax 401(k) and post-tax Roth 401(k) options12-16 weeks of paid parental leave + flex return to work15+ company holidays, including an EOY rest week20 vacation days and 10 sick days per yearFlex Fridays (no meetings after 2PM)Paid volunteer outings and civic engagement leaveLearning & career development stipendsHome office stipendHealth & wellness reimbursementsRemote first work environmentWe are a diverse team committed to a safe and inclusive work environment, free of discrimination based on race, color, religion, sex, gender identity or expression, sexual orientation, genetics, national origin, ancestry, age, medical condition, mental or physical disability, handicap or veteran status, or marital status.Please note that applicants should be authorized to work in the United States. Though we are interested in every qualified candidate, we are unable to offer visa sponsorship at this time.Job Type: Full-time",https://www.indeed.com/company/Spoiler-Alert/jobs/Data-Engineer-edfcb1e6550e8b66?fccid=e41b1426fe300235&vjs=3,"Boston, MA+1 location",Data Engineer,Data Engineer
224,US Lyndon B. Johnson Space Center,"Duties
Exercises technical direction in the development and operations phases of Mission Control Center Systems.
Manages systems requirements and analyzes test results to determine if requirements are met from a performance and operational standpoint.
Helps manage the day-to-day operation the Mission Control Center, providing status briefs to management. Leads the resolution of technical issues to ensure the facility is ready to support daily operations.
Coordinates facility upgrades, modifications, and schedules, non-standard configuration requests, waivers for the facility, Configuration Control Process, facility priorities, and facility utilization requests with affected Partner programs.
Serves as a systems expert and technical adviser to Branch, Division, and Directorate leadership; ensures upper management is informed as to major problem areas and their possible effects on the Center and Programs.
Contributes to the preparation and justification of the yearly budget for Planning, Programming, Budgeting, and Execution (PPBE) for the group.

Requirements
Conditions of Employment
This position is open to U.S. citizens, nationals or those who owe allegiance to the U.S
Position subject to pre-employment background investigation
You must meet qualifications requirements by the closing date of this announcement
Please see the ""Additional Information"" section for important information about COVID-19 vaccination requirements for federal employees.
Qualifications
In addition to the Basic Education Requirement (in the Education section below), to qualify for this position you must meet the requirements below and have one year of specialized experience equivalent to the next lower grade, which has equipped you with the particular competencies needed to successfully perform the duties of the position described above.

To qualify at the GS-13 level, you must have one year of directly related specialized experience equivalent to the GS-12 level:
Applying principles pertaining to the development of operational concepts, plans, and procedures for developing spacecraft ground systems hardware and software systems;
Assessing operations and maintenance of spacecraft mission control center systems in order to ensure continuance of operations;
Utilizing state of the art architecture and operations of voice, command, telemetry, tracking, and video systems supporting a mission control center for either crewed or un-crewed spaceflight vehicles.
Your resume must include a clear and detailed narrative description, in your own words, of how you meet the required specialized experience. Experience statements copied from a position description, vacancy announcement or other reference material constitutes plagiarism and may result in disqualification and losing consideration for the job.
Education
Basic Education Requirement: You must have successfully completed a bachelor's degree with a major in a) engineering from a college or university that has at least one ABET accredited engineering program or b) Physical Science, Mathematics, Life Science, Computer Science*, or other field of physical science.

Note: A computer science curriculum must include 30 semester hours of course work in any combination of mathematics, statistics and computer science. Of the 30 semester hours, 15 must be in any combination of statistics and mathematics which includes differential and integral calculus.

If you did not complete a qualifying bachelor's degree, you may be eligible if you have obtained a graduate degree in an AST qualifying field, as listed above.

Degrees in engineering technology are not considered qualifying for this position.

Engineering degrees earned within the United States: Engineering degrees earned within the United States must be from a college or university that has at least one ABET accredited engineering program. To find out if a school has at least one ABET accredited program, please visit http://www.abet.org.

Engineering degrees earned outside the United States: Engineering degrees earned outside the United States must be recognized by a Mutual Recognition Agreement (MRA), often known as accords. These are non-governmental agreements among organizations that accredit academic degree programs. MRAs recognize the substantial equivalence of mature accreditation systems and programs accredited by signatory organizations within their jurisdictions. For a listing of Signatories, please visit, https://www.abet.org/global-presence/mutual-recognition-agreements/is-your-program-recognized/.

Science and other related degrees earned within the United States: Science and other related degrees must have been awarded from colleges or universities that are accredited by recognized accrediting organizations. For a list of schools that meet this criteria, go to http://ope.ed.gov/accreditation/.

Science and other related degrees earned outside the United States: If you are using education completed in foreign colleges or universities to meet the qualification requirements, you must show that the education credentials have been evaluated by a private organization that specializes in interpretation of foreign education programs. These education credentials must be deemed equivalent to that gained in an accredited U.S. education program; or full credit has been given for the courses at a U.S. accredited college or university. For further information, visit: https://www2.ed.gov/about/offices/list/ous/international/usnei/us/edlite-visitus-forrecog.html.

All degrees must have been received in the year of, or any year subsequent to the original date of accreditation.
Additional information
If you are selected, NASA may request information regarding your vaccination status for the purposes of implementing workplace safety protocols, such as masking, physical distancing, testing, travel, and quarantine.
Due to COVID-19, NASA is currently in an expanded telework posture. If selected, you may be expected to temporarily telework, even if your home is located outside the local commuting area. Once employees are permitted to return to the office, you will be expected to report to the duty station listed on this announcement within 30 days. At that time, you may be eligible to request to continue to telework one or more days a pay period depending upon the terms of the agency's telework policy.

Additional selections may be made for similar positions across NASA within the local commuting area(s) of the location(s) identified in this announcement. By applying, you agree to have your application shared with interested selecting official(s) within NASA. CTAP/ICTAP will be cleared for any additional selection from this announcement.

If you have special priority selection rights under the Agency Career Transition Assistance Program (CTAP) or the Interagency Career Transition Assistance Program (ICTAP), you must:

Indicate your eligibility when applying for a position. The questionnaire asks you to identify your ICTAP/CTAP eligibility.Meet the minimum qualifications requirements for the positionSubmit proof that you meet the requirements for CTAP/ICTAP as indicated in 'Required Documents'

For additional information about CTAP/ICTAP eligibility, click here - https://www.opm.gov/policy-data-oversight/workforce-restructuring/employee-guide-to-career-transition/#ictap.


Benefits

A career with the U.S. government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Opens in a new windowLearn more about federal benefits.
Review our benefits
Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time or intermittent. Contact the hiring agency for more information on the specific benefits offered.
How You Will Be Evaluated
You will be evaluated for this job based on how well you meet the qualifications above.
Direct Hire Authority: These positions will be filled through the Office of Personnel Management's Direct Hire Authority. Category rating and veterans' preference will not be considered in evaluating applicants. For more information on Direct Hire Authority, please see: OPM Direct Hire Fact Sheet.

Veterans: Under the provisions of Direct Hire Authority, veterans' preference does not apply. However, applicants who are eligible for veterans' preference are encouraged to include that information in their application and submit supporting documentation (i.e. DD-214, or other substantiating documents). For more information on please see: Veterans' Preference information on the FedsHireVets website.

You will be evaluated for this position based on how well you meet the qualifications and eligibility requirements listed in this vacancy announcement. To determine your qualifications and referral status, we may review your resume and supporting documentation and compare it against your responses to the vacancy questionnaire. Overstating your qualifications and/or experience in your application materials or application questionnaire may result in your removal from consideration.

You will be assessed on the following competencies:

Systems Integration
Computer Systems and Engineering
Flight and Ground Data Systems
Software Engineering
NASA considers paid and unpaid experience, including volunteer work done through National Service programs (e.g., Peace Corps, AmeriCorps) and other organizations (e.g., professional; philanthropic; religious; spiritual; community, student, social). Volunteer work helps build critical competencies, knowledge, and skills and can provide valuable training and experience that translates directly to paid employment. You will receive credit for all qualifying experience, including volunteer experience.
Benefits

A career with the U.S. government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Opens in a new windowLearn more about federal benefits.
Review our benefits
Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time or intermittent. Contact the hiring agency for more information on the specific benefits offered.
Required Documents
As a new or existing federal employee, you and your family may have access to a range of benefits. Your benefits depend on the type of position you have - whether you're a permanent, part-time, temporary or an intermittent employee. You may be eligible for the following benefits, however, check with your agency to make sure you're eligible under their policies.
A complete application package includes a resume, required documents and completion of the vacancy announcement questionnaire. Please see this guidance: What to include in your resume. Your resume should describe your specialized experience and support your answers to the vacancy announcement questionnaire.

The following documents are required:

Resume
Transcript

While a transcript is the preferred proof of qualifying education, you may submit any of the following items:
An unofficial transcript
A copy of an official transcript
A copy of a degree (i.e. a picture or scan)
A list of completed/credited courses that includes, the title, course code, and number of semester, quarter or classroom hours for each course (i.e., BUAD- 101-Introduction to Business, 3 Semester hours)
You will lose consideration if you do not submit proof of your education. Documents must fully support the education requirements listed above. Incomplete documents or documents that do not show completion of required degree program or coursework may result in disqualification.

There may be other supporting documents (licenses, certification, veterans preference, etc.), depending on your answers to the questionnaire and job announcement description, that you may need to submit.

If you are a surplus or displaced employee (CTAP and ICTAP), submit proof that you meet the requirements for CTAP/ICTAP. This includes copies of your agency notice, most recent Performance Rating and most recent Notification of Personnel Action (SF-50) noting current position, grade level, and duty location.

Official documents are required at the time of appointment for verification of eligibility and qualifications.
How to Apply
A complete application package must be submitted by 11:59 PM (ET) on the closing date of the announcement to receive consideration.

To begin the application process, click on the 'Apply' link. You will need to be logged into your USAJOBS account to apply. If you do not have a USAJOBS account, you will need to create one before beginning the application.

Follow the prompts to select your resume and/or other supporting documents to be included with your application package. Your uploaded documents may take several hours to clear the virus scan process.

After acknowledging you have reviewed your application package, complete the ""Include Personal Information"" section as you deem appropriate and click to continue with the application process. You will be taken to the online application which you must complete in order to apply for the position. Complete the online application, verify all required documentation is included with your application package, and submit the application.

If you are unable to apply online or need to fax a document you do not have in electronic form, view the following link for information regarding an Alternate Application.

If you have questions about this announcement, you may contact the agency toll free at the phone number located below. Be advised - application materials faxed, emailed, and/or mailed to Johnson Space Center will not be accepted for this announcement.
Agency contact information
NASA Shared Services Contact Center
Phone
1-877-677-2123
Fax
1-866-779-6772
Email
nssc-contactcenter@mail.nasa.gov
Address
Johnson Space Center
2101 NASA Parkway
Houston, TX 77058
US

Next steps
Once you submit your application package, you will receive an acknowledgement email. Throughout the process you will receive regular status updates through USAJOBS. To verify the status of your application, log into your USAJOBS account (https://my.usajobs.gov/Account/Login), all of your applications will appear on the Welcome screen. The Application Status will appear along with the date your application was last updated. For information on what each Application Status means, visit: https://www.usajobs.gov/Help/how-to/application/status/.

If you are found qualified, you may be referred to the hiring manager for further consideration. Whether or not you are contacted for an interview depends upon the location of the position and the judgment of the hiring manager.

If you are selected, you will be notified by phone or email with a tentative job offer. If you fail to meet the conditions of employment or any other pre-employment requirements, such as missing a scheduled appointment, we may rescind a tentative job offer.

An official, written job offer will be issued once all requirements have been verified.
Fair and Transparent
The Federal hiring process is setup to be fair and transparent. Please read the following guidance.
Equal Employment Opportunity (EEO) Policy
Reasonable accommodation policy
Financial suitability
Selective Service
New employee probationary period
Signature and false statements
Privacy Act
Social security number request

Required Documents
A complete application package includes a resume, required documents and completion of the vacancy announcement questionnaire. Please see this guidance: What to include in your resume. Your resume should describe your specialized experience and support your answers to the vacancy announcement questionnaire.

The following documents are required:

Resume
Transcript

While a transcript is the preferred proof of qualifying education, you may submit any of the following items:
An unofficial transcript
A copy of an official transcript
A copy of a degree (i.e. a picture or scan)
A list of completed/credited courses that includes, the title, course code, and number of semester, quarter or classroom hours for each course (i.e., BUAD- 101-Introduction to Business, 3 Semester hours)
You will lose consideration if you do not submit proof of your education. Documents must fully support the education requirements listed above. Incomplete documents or documents that do not show completion of required degree program or coursework may result in disqualification.

There may be other supporting documents (licenses, certification, veterans preference, etc.), depending on your answers to the questionnaire and job announcement description, that you may need to submit.

If you are a surplus or displaced employee (CTAP and ICTAP), submit proof that you meet the requirements for CTAP/ICTAP. This includes copies of your agency notice, most recent Performance Rating and most recent Notification of Personnel Action (SF-50) noting current position, grade level, and duty location.

Official documents are required at the time of appointment for verification of eligibility and qualifications.
Help
This job is open to
The public
U.S. Citizens, Nationals or those who owe allegiance to the U.S.",https://www.indeed.com/rc/clk?jk=8e9f940973029f9f&fccid=dd616958bd9ddc12&vjs=3,"Temporarily Remote in Houston, TX 77058 77058","Computer Engineer, AST, Data Systems (Direct Hire)",Data Engineer
225,DBAce Tech LLC,"Title : AWS Data EngineerLocation : Herndon, Virginia ( Initially Remote )Duration : 12+ MonthsStart date : ASAPKey skills required for the job are: 7+ years of relevant experience.Must have AWS data AND application experiencePySpark/SparkPQL skillsBig DataStrong Python or Java skillsAWS experienceDatabase systems (SQL and NoSQL)Data warehousing solutionsETL toolsData APIs.Understanding the basics of distributed systems.Knowledge of algorithms and data structures.Desired qualificationsKnowledge of Machine Learning concepts such as KNN, Random Forest, Naïve Bayes, Neural Networks, and deploying them on Sagemaker is a plus.EDUCATIONAL REQUIREMENTBachelor degree in Computer Science, Information Systems or related fieldSkills: Big DataETLJavaPythonJob Types: Full-time, ContractPay: $60.00 - $70.00 per hourSchedule:8 hour shiftAbility to commute/relocate:Herndon, VA: Reliably commute or planning to relocate before starting work (Required)Experience:AWS: 5 years (Preferred)SQL: 5 years (Preferred)Data warehouse: 5 years (Preferred)Python: 5 years (Preferred)Work Location: One location",https://www.indeed.com/company/DBAce-Tech-LLC/jobs/Aws-Data-Engineer-c152204e9219fa73?fccid=879f52a3c537a9f8&vjs=3,"Herndon, VA","AWS Data Engineer / Herndon, Virginia ( Initially Remote )",Data Engineer
226,SwagUp,"ABOUT US
Swag...everyone loves to receive it
Whether it be welcoming new hires into their dream job, thanking customers for their loyalty, celebrating a community milestone, or engaging event attendees, when done right, swag is the glue that bonds brands together with the people that matter to them. But no one enjoys putting this stuff together...
The $30 Billion branded swag supply chain is a complete mess, filled with too many products, too many intermediaries, too little quality control, and not enough technology.
SwagUp is disrupting the industry by streamlining the end to end supply chain and making it available through a single platform utilizing powerful APIs and interfaces (e.g. dashboard). As the fastest growing swag company, we have made great strides towards our mission of eliminating the friction in the swag creation and distribution process. BUT we've just begun to scratch the surface…
And that's where YOU come in…
MISSION
SwagUp is on the hunt for a Data Engineer who can build data pipelines, data warehouses, data marts, and transform data into a useful format for analysis. On day one, you will join our team in designing and building pipelines that will source data from various sources in real-time, store this data, then transform the data into meaningful result sets and build visualizations on top of it for various teams to use for analysis and strategic planning.
The ideal candidate doesn't see themselves as just an engineer; they are also an architect who enjoys building. Data drives the decisions we make everyday at SwagUp. We are looking for an engineer that will be able to understand the data and make actionable recommendations that will set us on the path for success.
RESPONSIBILITIES
Working with other team members to design, build, and deploy reliable data pipelines from scratch.
Maintaining our existing data pipelines.
Optimizing our data warehouse & creating data marts for fast efficient querying for our technology and reporting infrastructure.
Run standardized queries to extract data from data lakes and other applications .
Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale.
Arbitrate critical decisions correctly considering data best practices, system realities & numerous stakeholders' feedback.
REQUIREMENTS
5+ years of experience in a Data Engineer and/or Data Architect role, ideally in an e-commerce environment
Strong SQL skills and knowledge of data warehouse principles & visualization tools
Experience building data pipelines from scratch
Strong experience with Cloud Data Warehouses is a must (within them, Google Cloud Stack is preferred)
Experience with data manipulation & processing of large & complex data sources on cloud systems
Experience with data transformation tools such as dbt or similar tools
Experience with Google Cloud Functions, Data Proc, Data Prep or similar tools.
Experience with Python, R or another scripting language
Experience in statistical models & analysis
Excellent quantitative and analytical skills with ability to draw conclusions based on data
Self-starter & able to work in a fast-paced environment with quick turnaround & minimal guidance
Strong problem solving skills and ability to provide creative solutions
WHAT SUCCESS LOOKS LIKE
Streamline the ingestion of raw data and leverage data into meaningful insight
Utilize data architecture knowledge to build data pipelines & build data warehouse from scratch
Design data models for both optimal storage and retrieval
Coordinate and collaborate with the internal teams to provide the strategic data needed to drive the business forward
Goals are a target, but the objective is to exceed them while meeting deadlines
PERKS
Wake up each morning proud of the place you work and the amazing companies you get to partner with
Join the team at a time when you can help shape the future of the company
Unlimited PTO, we are all adults, you're in control
Medical, Dental, Vision and Life insurance
Lots of swag!
Equipment provided to set up a successful work environment
Engaging team-building activities to make a remote-first workforce feel connected
Being part of an organization that rewards results and truly believes in promoting from within, at SwagUp your growth potential is uncapped!
Salary: $120,000 - $150,000
SwagUp welcomes and celebrates talent from all backgrounds and perspectives. Our success is directly correlated to our people, and we believe our team should reflect the diversity of the companies we partner with. As an organization, we strive to foster an inclusive, diverse environment where we all work towards a common goal! #tothemoon",https://www.indeed.com/rc/clk?jk=4e9e54a48c9f127d&fccid=20c9017f2e039661&vjs=3,"Remote in New York, NY",Data Engineer,Data Engineer
227,GOIN,"Agency Note: We are accepting 1099 & W-2 only.
Contract / Contract to hire
Remote is considered for exceptional candidates
Big Data Engineer
Requirements:

KEY ACCOUNTABILITIES
Leads, grows, and develops a team of data engineers who write, deploy and maintain software used to build, integrate, manage, maintain, and quality-assure data.
Creates positive engagement and drives an inclusive work environment with teams and partners through the quality of interactions and collaboration across multiple business entities.
Effectively works with cross-disciplinary collaborators and partners across multiple business entities.
Architects and designs reliable and scalable data infrastructure.
Advocates for and ensures their team adheres to software engineering standard methodologies (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),
Responsible for deploying secure and well-tested software that meets privacy and compliance requirements.
Actively mentors others.
ESSENTIAL EDUCATION
BS degree in computer science or related field
ESSENTIAL EXPERIENCE AND JOB REQUIREMENTS
Experience (typically 2+ years) leading, growing and developing a data engineering team
Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
Experience in Spark, pySpark
Development experience in one or more programming languages (e.g. Python, Go, Java)
Advanced SQL knowledge
Experience designing and implementing large-scale distributed systems
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Strong collaborator management and ability to lead large organizations through influence
Continuous learning and improvement approach
DESIRABLE CRITERIA
Experience in retail and/or supply chain data
Experience in Azure data platforms",https://www.indeed.com/rc/clk?jk=498f684a9fdf8285&fccid=2a4548d7a2e19e88&vjs=3,"Irving, TX 75063 75063",Big Data Engineer,Data Engineer
228,Dutech Systems,"JOB DESCRIPTION: · Hands-on development experience with the following tools and technologies: Snowflake, Data model, Amazon Lambda & Glue, and Qlik· Development experience with T-SQL & PostgreSQL· Ensure data compliance and working within modern standards to ensure that the data services meet or exceed all regulatory expectations· Collaborate with existing data team and internal customers and partners to design, develop and deliver solutions and key business insights· Hands-on experience working with complex end-to-end data solutions for massively scaled products and services· Good understanding of data consumption patterns by business users· A strong communicator with great organizational skills; there is a high level of cross team collaboration, information from many vertical flows into this data pipeline, and there are a variety of stakeholders involved· Self-starter and innovator who likes to solve complex problems· Have an attitude to take on any assigned task and perform related work as assigned· Work in the US with the capacity to work from home a minimum of 40 hours a week during our regular business hoursII. CANDIDATE SKILLS AND QUALIFICATIONSMinimum Requirements: Candidates that do not meet or exceed the minimum stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity.YearsRequired/PreferredExperience4RequiredExperience with Datalake, Datawarehouse, Data model or Snowflake3RequiredExperience with Data Development with SQL/T-SQL development2RequiredExperience with Reporting, Analytics and Dashboards1RequiredExperience working in an agile development environment as part of a sprint team1RequiredExperience with data streaming technologies such as Kafka or Elastic1PreferredExperience with messaging technologies such as MQ or RedisJob Types: Full-time, ContractSalary: $60.00 - $65.00 per hourSchedule:Monday to FridayExperience:Datalake, Datawarehouse, Data model: 5 years (Preferred)Data Development with SQL/T-SQL: 3 years (Preferred)Data warehouse: 1 year (Preferred)Reporting, Analytics and Dashboards: 2 years (Preferred)agile development environment: 1 year (Preferred)data streaming technologies: 1 year (Preferred)Work Location: One location",https://www.indeed.com/company/Dutech-Systems/jobs/Data-Developer-Engineer-d5a74eeeabf99a65?fccid=28f9f4b36219e977&vjs=3,"Austin, TX",Data Developer Engineer,Data Engineer
229,Index Exchange,"We shaped the earliest forms of ad tech, and we're looking for the technical expertise to help shape its future. Our customers have unique problems that can be solved at scale, and that's where the technical skills of our team make a real difference. Our exchange handles more than 220 billion requests every day (which is even more than the New York Stock Exchange), and every member of our engineering team has an enormous amount of autonomy in building code that supports that level of scale. Through the transparency of our technology, dedication to innovation, and long-standing customer relationships, we lead through change.

We have more than 500 Indexers around the globe dedicated to building a safe and transparent marketplace that provides a trusted experience for consumers.

What's it like to work at Index?
Index is an exciting and fast-paced place to work. We're built on our values of change, support, learning and teaching, trust, and intention. We pride ourselves on our independence and openness, not only in our technology, but in our teams, too. Our diverse and inclusive culture celebrates how we can leverage our unique differences to help drive Index forward.

Our culture of success is truly supportive and collaborative. In working together across our teams, we're continually investing in the people and technology to solve the industry's most complex problems. As we extend the promise of ad tech to every channel, we're looking for talented engineers to help advance Index, and the industry, forward.


Are you ready to join the programmatic evolution?

The work we do requires complex problem solving, the ability to write high quality code in a very large and scalable deployment environment, an understanding of Agile software development, Continuous Integration, well-developed communication skills and the ability to collaborate with a diverse group of people. We're looking for the right combination of personality and ingenuity to push our product to the max, inspire those around you and have fun doing it
Data is a big deal at Index Exchange and a Data Engineer will play a critical role. Our advertising exchange generates multiple terabytes of auction-related information each day and our Data Engineering team builds tools and infrastructure to help manage this vast amount of data. Working within our larger R&D team, our Data Engineers use a specialized suite of tech and hardware to pipe, parse and maintain our data, including the oversight of a multi-petabyte cluster.

Working on the bleeding edge of exciting technology, you're afforded the opportunity to experiment with new tools and attempt radically different approaches than traditional software engineering affords. Every day with the Data team is different and each project presents its own set of new and exciting challenges. Things shift very quickly in our industry and we rely on the Data team to keep us ahead of the curve and moving in the right direction.


Here's What You Need:

Problem Solver: You are curious and loves exploring multiple approaches to find the most efficient, scalable solution and solve a problem
Collaborative: You work well with other people
Passionate: A passion for Big Data and an interest in the latest trends and developments constantly researching new tools and data technologies
Self-starter: You are comfortable helping your team get things done

Here's What You'll be Doing:

Design, implement, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services
Identify, design, and implement system performance improvements
Identify, design, and implement internal process improvements
Automate manual processes and optimize data delivery

What We're Looking For:

You may or may not tick off every box, and that's ok. Each person brings a different background and different skills. If you think you are a good match for what we are looking for tell us why, and tell us what you are doing to improve yourself and we'll see what we can do to help!

A degree in Computer Science/Engineering or related field
3-5 years of experience as a Software Engineer/Data Engineer working in a large and scalable deployment environment
In-depth knowledge of Hadoop, Spark, and Big Data frameworks.
Experience in programming languages such as Scala, Java, Python.
Experience with building stream-processing systems, using solutions such as Spark-Streaming
Experience processing large amounts of structured and unstructured data
Expertise in algorithms, performance optimization, design patters, and memory optimization
A strong desire to work with and learn from the talented people around you – we greatly value people and interactions
Excellent problem-solving skills, the ability to think differently, and an understanding that compromise, negotiation, and practicality are important qualities
Experience working with Linux environments and resilient and scalable platforms is an asset.
Knowledge in any of the following are assets:
Kubernetes, Kafka, Docker, Aerospike
Why You'll Love Working Here:

Comprehensive health, dental, and vision plans at no cost to you
Time off and flexible work schedules
Retirement plan with a 5% company match
Stock options and equity packages
Generous parental leave
Monthly wellness stipend plus fitness discounts and quarterly wellness group activities
Home office stipend
Community engagement opportunities and donation-matching program
Annual virtual company retreats and regular community-led team events

Equal employment opportunity

At Index Exchange, we believe that successful products are built by teams just as diverse as the audience who uses them. As such, we are committed to equal employment opportunities. We celebrate diversity of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, or veteran status. Additionally, we realize that diversity is deeper than any status or classification—diversity is the human experience. For those who show grit, passion, and humility—Index will welcome you.

Accessibility for applicants with disabilities

Index Exchange is committed to working with and providing access and reasonable accommodations to applicants with disabilities. Please let us know if you'd like to request a reasonable accommodation.
COVID-19 guidance: We have re-opened offices in various cities following local guidelines, but are continuing to maintain a flexible work environment.

#Ll-LP1

#LI-Remote",https://www.indeed.com/rc/clk?jk=dea476a72bf9b354&fccid=041c6d99171d1471&vjs=3,"Remote in New York, NY",Data Engineer,Data Engineer
230,Synaptein Solutions,"Position: Data Engineer

Location: Thousand Oaks, CA (Remote Position Till Covid)

Duration: 11 + Months


Job Description:

Proven proficiency with scripting languages such as Python, Ruby, and JavaScript, and build tools such as Jenkins, Maven, Ant, Gradle or Ivy
Experience with Spark, Hive, Kafka, Kinesis, Spark Streaming, and Airflow, Azure DataBricks.

Mandatory Skills:
Python
DataBricks
Pig
Hive
Spark
Aws

Regards,

Anup Kumar

Office – 703-310-7171",https://www.indeed.com/rc/clk?jk=9497189b69b1436b&fccid=e5a1823a3cd67c3b&vjs=3,"Thousand Oaks, CA",Data Engineer,Data Engineer
231,Chugach Government Solutions,"About Us:

When you work at Chugach Government Solutions (CGS), you join a proud legacy of supporting missions while sustaining culture.
The federal division of Chugach Alaska Corporation, CGS has been supporting critical missions as a government contractor for over 25 years. Our focus is to support facility maintenance, IT/technical services, construction and education. We are proud to have built, and continue to foster, an incredibly talented team spanning across the globe in hundreds of different fields – each team member proud to serve our country with first-class business services, while also making a difference for our Chugach shareholders.
At CGS, empowering employees is a part of our core, and that focus is one of the ways we build and foster high-performing teams. We empower our employees through competitive compensation and benefits package, professional growth opportunities, truthful communication, and more!
If you are looking for an opportunity to serve something bigger than yourself; if you want your day job to be one that creates meaningful value; if you are looking for an environment that highly values employees and respects individual differences – then Chugach Government Solutions may be the right fit for you!
Job Overview:

This position is responsible for building and operationalizing the data necessary for enterprise data and analytics initiatives. This position includes building, managing, and optimizing data pipelines and then moving the data pipelines effectively into production for key data and analytic consumers. This position is also responsible to provide key input and recommendations for implementing enterprise software to ensure compliance with data governance and data security.
Responsibilities:
Essential Duties & Job Functions:
Acts as part of a team involved in the development and maintenance and operation of enterprise applications for a global organization with international business with $500-700M in revenues.
Builds data pipelines and manages the data flows. These data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases.
Architecting, creating, and maintaining data pipelines.
Use innovative and modern tools, techniques, and architectures to automate the most-common, repeatable, and tedious data preparation and integration tasks partially or completely.
Assists with renovating the data management infrastructure to drive automation in data integration and management.
Strong collaboration skills to work with varied stakeholders within the organization.
Apply data and/or domain understanding in addressing new data requirements.
Train counterparts in these data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
Applies their data knowledge and understanding to address all data requirements and initiatives.
Ensures compliance and data governance according to the data governance standards.
Promotes the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals.
Travels to support onsite projects as needed.
Performs related work as assigned.
Job Requirements:
Mandatory:
Associate’s Degree in business information systems (IS), finance, computer science, technology, engineering, or another related field or three (3) years’ application and/ or computer systems support.
At least three (3) years of experience working in cross-functional teams and collaborating with business stakeholders.
Experience with government contracting and regulations governing data integrity.
Basic skills in reporting, system maintenance, table and code setup, user access security, and upgrades.
Possess or be able to obtain with first 90 days a DoD 8570 IAM-III or IAT III Certification.
Must be able to speak, read, and comprehend English to perform contract requirements and comply with emergency procedures.
Valid state Driver’s License with acceptable driving record pertinent to the position.
Must be U.S. Citizen or permanent resident.
Reasonable Accommodation:
CGS will provide reasonable accommodations, according to applicable state and federal laws, to all qualified individuals with physical or mental disabilities.
Equal Employment Opportunity:
Chugach is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, gender, sexual orientation, gender identity or expression, age, pregnancy, disability, genetic factors, protected veteran status or other characteristics protected by law.",https://www.indeed.com/rc/clk?jk=9d972d2b3235b9c5&fccid=3e5272adb478341f&vjs=3,+2 locationsRemote,JUNIOR DATA ENGINEER,Data Engineer
232,System1,"System1 is looking for engineers with production data experience to join the Data Engineering team. This team is the horizontal layer that supports business intelligence, optimization, machine learning, and external & internal reporting. We process and report on billions of events and user attributes per day, gathered from an extremely heterogeneous and large set of data streams.Our bread and butter is Python. We utilize a range of technologies including AWS SNS, SQS, EC2, S3, Secrets Manager, Redis, Apache Airflow, Docker, Flask, as well as PostgreSQL and Snowflake for our data warehousing.What You Can Expect on the TeamPrototype, develop, optimize, and expand an ecosystem of services that work with data: backend services for retrieving, managing, and monitoring data; streaming and batch data processing; frameworks for data ingestionEvaluate, recommend, and perform proof-of-concepts for new services and frameworksGuide architectural design, anticipating needs and highlighting tradeoffs in performance and functionalityParticipate in peer code reviews and produce high quality documentationTake projects through the full engineering lifecycle: designing, ticketing, building, testing, deploying, and debugging tools and productsHelp grow a team and work with a tight knit group of engineers and data stakeholdersFlexibility in working with different teams, products, and their data needsWhat You Will BringBachelor’s in Computer Science or equivalent professional experienceStrong experience with Python developmentDemonstrated experience working and reasoning with large datasets (Experience with PostgreSQL and Snowflake a plus)Experience developing backend APIs in a python framework (Experience with Flask a plus)Understanding of NoSQL datastores like DynamoDB and RedisExperience with Linux and the AWS ecosystemExperience with Docker a plusExperience with Airflow a plusWhat We Have to OfferCompetitive PTO11 Company HolidaysUntracked sick time100% covered Medical, Dental, Vision for employees401k w/matchPaid professional developmentLeadership & growth opportunitiesVirtual company and team building events#BI-Remote#LI-RemoteJob Type: Full-time",https://www.indeed.com/company/OpenMail-LLC/jobs/Software-Engineer-3a164597767df006?fccid=96f9f0750334d953&vjs=3,"Remote in Los Angeles, CA+2 locations","Software Engineer, Data",Data Engineer
233,progressive cliff llc,"Position: Azure Data EngineerPosition type:  Contract ( 6-12 months )Position Location:  (Remote)Position Start Date: ASAPJob Description:Azure data engineer with Python expertise.( 8-10yrs).5+ years in python development working with Azure/AWS/big data technologies.3+ years in working on the big data stack, Azure, ADLS, Databricks and Data lake.- Write custom python code to establish connections to Azure via Azure CLI.Proficient in data science, and statistical python framework - pandas, numpy, etc.- Work on Azure python Sdk, git Sdk and modules of Azure Api's.Knowledge of Azure resources like datafactory, pipelines & datasets is a must.Lead and develop python code snippets for Azure deployment.- Knowledge of devops, CI & CD.Knowledge of Spark, Hive, Hadoop desirableAny cloud or python certificationsJob Types: Full-time, Part-time, ContractPay: From $60.00 per hourSchedule:8 hour shiftExperience:Azure: 5 years (Preferred)AWS: 3 years (Preferred)Work Location: Remote",https://www.indeed.com/company/progressive-cliff-llc/jobs/Azure-Data-Engineer-992258ae793d3c22?fccid=f896d34ffde88e38&vjs=3,Remote,Azure Data Engineer,Data Engineer
234,"Amazon Data Services, Inc.","1. B.S./B.A. degree with 4 years of relevant controls work experience or A.S. and 6 years of relevant controls work experience
2. 2 years of experience designing, configuring, programming, installing or servicing one of the following products: Siemens: Apogee, Honeywell: Building Controls, Trend: Building Controls, Schneider Electric: Power Logic/Struxureware, Rockwell Automation: FactoryTalk, Ignition, Tridium: Niagra, Eaton: Foreseer, ABB: Decathalon, Johnson Controls: Metasys
3. Minimum two (2) years of experience in project coordination with multiple teams or vendors to meet project requirements.

Job summary
As part of the global controls team, you will work with highly motivated experts and innovators in the data center industry. You will be responsible for innovating, deploying, optimizing, and maintaining the automation systems within the data centers. Automation systems consist of building management system (BMS) and electrical power monitoring system (EPMS). Using Amazon leadership principles, you will develop new processes and standards while innovating in the controls space.

As a controls engineer you will:

Perform controls root cause analysis for service of controls for AWS data centers.
Manage scope of Building Management System (BMS) and Electrical Power Monitoring System (EPMS) for construction of AWS data centers.
Schedule and supervise Quarterly maintenances for BMS and EPMS.
Provide vendor management.
Develop scope of work for site improvement projects.
Work under tight project timelines.
Manage multiple stakeholder deliverables, requirements and navigate difficult situations.
Review controls sequence of operation and provide feedback to design.
Financially manage both construction and service contracts.
Provide technical assistance and support to internal customers during life cycle of the data center.
Participate in a global on-call schedule.
Ensure data center controls are deployed in accordance with scope of work (SOW) and specifications.
Review and provide feedback on mechanical, electrical, and plumbing (MEP) drawings.
Act in a project sponsor capacity.
Grow in technical ability by learning a multitude of different automation platforms.
Develop building automation controls drawings and wiring diagrams.
Size controls valves and damper actuators and select controls devices.
Develop controls bill of material (BOM).
Develop and modify controls logic programming.
Develop and modify graphical user interface.

Preferred qualifications

M.S. In an engineering-based discipline or ten 10+ years related experience, or an equivalent combination of education and experience.
Six (6) years of experience in either design programming, or startup/commissioning.
Four (4) years of experience in project management.
Experience with controls in data centers or other critical facilities.
Networking administration experience.
MQTT communication protocol experience.
Experience in any of the following systems: Siemens, Eaton, Tridium, Distech, Struxureware, APC, Ignition, or ABB.
Project Management Professional (PMP)
Solid foundation in communication protocols.
Windows power user skill set, proficient with Microsoft Office Suite and project management software.
Demonstrated understanding of engineering documentation, electrical diagrams and standard operating procedures.
Excellent communication skills, teamwork, organizational and problem-solving skills. Ability to manage multiple stakeholder deliverables, requirements and navigate difficult situations.

Amazon is a committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.",https://www.indeed.com/rc/clk?jk=a6cce02d0401968f&fccid=fe2d21eef233e94a&vjs=3,"Arlington, VA","Controls Engineer, AWS Data Centers Controls Team",Data Engineer
235,Ubisoft,"Company Description
Home to the North American Ubisoft headquarters since 1996, Ubisoft San Francisco opened its development studio in 2009 and since developed Rocksmith® and Rocksmith 2014, to teach millions of people how to play guitar and bass. With a song library of over 1,500 songs and 10 years of support the Rocksmith franchise helped millions of players around the world on their musical journey. In 2017, the studio released the critically acclaimed South Park™: The Fractured But Whole™, in collaboration with Trey Parker, Matt Stone, and South Park Digital Studios. The studio is currently working on innovative new titles with Rocksmith+ and XDefiant.

Job Description
The Studio SF Data Science team is based in San Francisco and is tasked with implementing data and Machine Learning tools to support the needs of our live games. This includes activities as varied as telemetry design & implementation to help us understand player behavior, developing recommender systems and data features to deliver enriching tailored experiences to players, as well as developing data tools and pipelines to support the development teams’ needs.
WHAT YOU'LL DO
Collaborate with the music development team to define tooling needs
Design and build data tools to support the development teams operations
Automate and maintain batch and real-time pipelines that collect and process data
Develop microservices to support our data operations
Implement and monitor data quality indicators across the data pipeline
Consult on technical architecture and carry out proof of concept exercises

Qualifications
WHAT YOU'LL BRING
Strong interest in data engineering and architecture
Established expertise in Python
Established expertise with SQL
Experience working with Spark
Experience working with cloud platforms (ex. AWS, GCP, Azure, etc.)
Experience with data orchestration and routing tools (ex. Airflow, NiFi, Prefect, etc.)
Experience working with data visualization software (Tableau, R Shiny, etc.) a plus
Familiarity with DevOps concepts/practices (Containerization, CI/CD methodology, Infrastructure as Code)
Strong interest in the gaming industry

Additional Information
WHAT YOU'LL GET
Medical, dental, and vision coverage
Employer HSA contribution to fully cover your deductible (with enrollment in a HSA eligible medical plan)
401k match
Paid vacation, holiday and wellness time
Fitness, physical challenge, and eSports competition reimbursements
Ginger emotional support app for you and your adult dependents with up to 10 sessions per year of video-based therapy and psychiatry services
Membership to One Medical
Rocket Lawyer online legal service membership
Paid parental leave
Paid time off to volunteer
Pawternity – paid time off to bond with a newly adopted pet
Sabbatical leave available after 5 years of employment
Access to all of the latest Ubisoft games for PC
Generous discount on new Ubisoft games
All your information will be kept confidential according to EEO guideline
Ubisoft is committed to creating an inclusive work environment that reflects the diversity of our player community. We are an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, ethnicity, religion, gender, sexual orientation, age or disability status.
Skills and competencies show up in different forms and can be based on different experiences, that's why we strongly encourage you to apply even though you may not have all the requirements listed above.
At Ubisoft, you can come as you are. We embrace diversity in all its forms. We’re committed to fostering a work environment that is inclusive and respectful of all differences.
We do not accept any unsolicited resumes submitted by staffing or placement agencies or recruiters
Ubisoft is composed of over 20,000 talented people located in 55+ development studios and offices across the globe. With more than 80% of our teams dedicated to creation, Ubisoft is the biggest in-house creative force in the industry.
Ubisoft’s worldwide network of video game development studios and business offices work hand-in-hand each day to deliver rich and innovative gaming experiences that reflect the creativity and diversity of their teams. This cross-studio collaboration model means every team member has the opportunity to participate in challenging projects based on iconic brands including Assassin's Creed®, Tom Clancy's Rainbow Six Siege®, Tom Clancy's The Division®, For Honor®, Rabbids®, Rayman®, Watch Dogs®, Far Cry®, The Crew®, Just Dance® and more. Ubisoft is a global leader in the video games and entertainment software industry.
Ubisoft is committed to enriching player’s lives with original and memorable gaming experiences. We create worlds where people can express themselves, explore and discover new possibilities.
Are you looking to work with enthusiastic experts who are tackling game-changing challenges in entertainment and beyond? At Ubisoft, we offer an open environment where bright ideas are given a chance to shine, and everyone is eager to share knowledge.
Take entertainment to the next level. Join us and create the unknown.",https://www.indeed.com/rc/clk?jk=3ca60ba15199eb0c&fccid=237908de095b6446&vjs=3,"San Francisco, CA 94107 94107 (Financial District/South Beach area)",Associate Data Engineer,Data Engineer
236,Experian,"Company Description
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been name in the 100 “World’s Most Innovative Companies” by Forbes Magazine. In this era of disruptive digital transformation, delivering new and innovative customer experiences is a top priority for businesses worldwide. At Experian, a worldwide decisioning leader, we make this a reality by uniting data, advanced analytics and decision strategies with real-time operational execution and strong decision governance to deliver meaningful customer outcomes and positive business results.

Job Description

Carry out the extraction, formulation and manipulation of data to create the structured samples necessary to address business requirement.
Interpret data specifications
Ensure quality and accuracy of own work
Produced analysis is accurate and completed to agreed timescales, resulting in positive feedback from client/customer
Follow Business Line procedures and processes throughout the data extraction, formulation and manipulation process, ensuring all work is produced to the agreed specification, and meets all requirements
Take responsibility for the production of high quality results documentation
This role is responsible for carrying out elements of data manipulation and reporting, quality assurance, documentation and internal/client communication. The role holder is able to work independently.
Identify and resolve data problems, producing appropriate analysis to verify results
Identify potential gaps in own technical/business knowledge that might compromise the deadlines. Work with Project Managers or Consultant to plan timely steps to fill gaps
Challenge existing processes and recommend improvements
Ensure the quality of all data sent to clients/customers (e.g. project development samples; retrospective bureau data and Bureau Score data (if applicable))
Analyze and confirm the integrity of source data to be evaluated
Input into the review and assessment of client/customer requirements to produce a data project specification
With direction, produce data specifications
Liaise with other internal and external project management groups to ensure project is delivered as part of complete solution
Input into the accurate assessment of the work that needs to be undertaken to define and maintain a project plan
Plan, organize and complete own workload to meet project commitments within agreed schedules
Provide pro-actively status reports and progress updates to Project Manager or Consultant as required, for all projects/tasks
Contribute to the preparation of regular project status reports to both internal and external parties.
Proactively identify potential risks in achieving project timescales, taking pro-active action to minimize risk
Recognize when issues or problems need to be escalated
Maintain accurate time recording to projects in the appropriate internal time tracker software
Effectively use project management/planning tools
Proactively use time recording data to identify inefficiencies in project work and recommend improvements
Implement appropriate quality control procedures for all analysis reporting and scoring
Reduce error rate and re-run frequency. Meeting both the analytical and business objective

Qualifications

Willingness to learn data extraction, formulation and manipulation methods
Willingness to develop strong investigation skills. Ability to identify and correct data problems
PC/MF literate with experience in IT packages. Knowledge of statistical (e.g., SAS) packages desirable
Knowledge of Excel VBA
An innovative and inquisitive mind, focused on addressing and solving data problems
Common sense/logical approach to mathematical reasoning
Strong communication skills, including proficiency in writing reports and presenting technical work
Strong personal planning and time management skills
Strong interpersonal skills
Strong collaboration and teamwork
Commercially focused
Language in relevant marketplace
English language good knowledge
Additional Information

During this pandemic, all Experian employees are working remotely. Once it’s safe to do so, we’ll slowly return to our offices however we are in no rush to do this. The safety of our employees is of utmost importance.
Competitive pay and comprehensive benefits package
Flexible work schedule and relaxed dress code
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian U.S. employees are required to be fully vaccinated for COVID-19.
Experian Careers - Creating a better tomorrow together
Find out what its like to work for Experian by clicking here",https://www.indeed.com/rc/clk?jk=314cbd35d87dd902&fccid=75a3a5a15b202084&vjs=3,"Costa Mesa, CA 92626 92626+1 location",Data Engineer,Data Engineer
237,Cybba,"Overview
We are searching for a Data Ingestion Engineer to add to our global support team. Are you thorough, detail-oriented, and excited about the opportunity to code Marketing assets in the digital space? Under the direction of the tech team, you will work closely with various departments to implement new data projects.
For this role, we are open to both full-time and contractor candidates. This role will be fully remote and is able to be located anywhere.
How you will make a difference:
Diagnose software issues and resolve escalated customer complaints using established processes
Identifying new data we need and developing the ingestion processes
Experience with or able to quickly pick up: Microsoft Office suite, Freshdesk, Hubspot, Jira, API documentation, and other data concepts
Connecting and ingesting data source from 3rd party APIs and integrations
Improve and manage existing ingestion scripts
Designing and refactoring database schema
Maintaining and improving code base
Automating data workflow at source
Identify and solve technical problems ranging in degree of complexity
Documentation of implementation and technical processes / technical guides
Building internal tools contributing to process improvement, technical escalation, configuration updates, and bug fixes

Requirements:
Must-haves:
Python
SQL/Postgres
Strong Analytical Skills & work ethic
Bachelor’s degree in Computer Engineering, Computer Science, related field, or equivalent experience
3+ years in backend development experience, ingestion or implementation-specific experience
AWS experience: EC2, ECS, Athena, Redshift, API Gateway, Lambda, S3
Experience working with 3rd party APIs and integration
Software:
Python skill level – Advanced
SQL skill level – intermediate
Comfortable with CLI
Jira and Confluence
1+ year AWS work experience
Strong candidates will also be proficient with:
Git, BitBucket or other source control tools
Machine Learning
SSH/STFP/FTP
Database design and data manipulation methodologies
The Company
Cybba is a full-service performance marketing company that creates powerful solutions for brands looking to grow. Our inspired marketing tools help solve complex challenges while delivering meaningful results. To support our continued growth, we are always looking for passionate and talented individuals to join our dynamic, hardworking team.
Cybba is committed to creating a diverse work environment and is an equal opportunity employer. We consider all applicants for employment with no limit.
KTVTkauEdU",https://www.indeed.com/rc/clk?jk=28ccf2d2b54cfccd&fccid=f104df80bdb0641c&vjs=3,Remote,Data Ingestion Engineer,Data Engineer
238,Experian,"Company Description
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been name in the 100 “World’s Most Innovative Companies” by Forbes Magazine. In this era of disruptive digital transformation, delivering new and innovative customer experiences is a top priority for businesses worldwide. At Experian, a worldwide decisioning leader, we make this a reality by uniting data, advanced analytics and decision strategies with real-time operational execution and strong decision governance to deliver meaningful customer outcomes and positive business results.

Job Description

Carry out the extraction, formulation and manipulation of data to create the structured samples necessary to address business requirement.
Interpret data specifications
Ensure quality and accuracy of own work
Produced analysis is accurate and completed to agreed timescales, resulting in positive feedback from client/customer
Follow Business Line procedures and processes throughout the data extraction, formulation and manipulation process, ensuring all work is produced to the agreed specification, and meets all requirements
Take responsibility for the production of high quality results documentation
This role is responsible for carrying out elements of data manipulation and reporting, quality assurance, documentation and internal/client communication. The role holder is able to work independently.
Identify and resolve data problems, producing appropriate analysis to verify results
Identify potential gaps in own technical/business knowledge that might compromise the deadlines. Work with Project Managers or Consultant to plan timely steps to fill gaps
Challenge existing processes and recommend improvements
Ensure the quality of all data sent to clients/customers (e.g. project development samples; retrospective bureau data and Bureau Score data (if applicable))
Analyze and confirm the integrity of source data to be evaluated
Input into the review and assessment of client/customer requirements to produce a data project specification
With direction, produce data specifications
Liaise with other internal and external project management groups to ensure project is delivered as part of complete solution
Input into the accurate assessment of the work that needs to be undertaken to define and maintain a project plan
Plan, organize and complete own workload to meet project commitments within agreed schedules
Provide pro-actively status reports and progress updates to Project Manager or Consultant as required, for all projects/tasks
Contribute to the preparation of regular project status reports to both internal and external parties.
Proactively identify potential risks in achieving project timescales, taking pro-active action to minimize risk
Recognize when issues or problems need to be escalated
Maintain accurate time recording to projects in the appropriate internal time tracker software
Effectively use project management/planning tools
Proactively use time recording data to identify inefficiencies in project work and recommend improvements
Implement appropriate quality control procedures for all analysis reporting and scoring
Reduce error rate and re-run frequency. Meeting both the analytical and business objective

Qualifications

Willingness to learn data extraction, formulation and manipulation methods
Willingness to develop strong investigation skills. Ability to identify and correct data problems
PC/MF literate with experience in IT packages. Knowledge of statistical (e.g., SAS) packages desirable
Knowledge of Excel VBA
An innovative and inquisitive mind, focused on addressing and solving data problems
Common sense/logical approach to mathematical reasoning
Strong communication skills, including proficiency in writing reports and presenting technical work
Strong personal planning and time management skills
Strong interpersonal skills
Strong collaboration and teamwork
Commercially focused
Language in relevant marketplace
English language good knowledge
Additional Information

During this pandemic, all Experian employees are working remotely. Once it’s safe to do so, we’ll slowly return to our offices however we are in no rush to do this. The safety of our employees is of utmost importance.
Competitive pay and comprehensive benefits package
Flexible work schedule and relaxed dress code
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian U.S. employees are required to be fully vaccinated for COVID-19.
Experian Careers - Creating a better tomorrow together
Find out what its like to work for Experian by clicking here",https://www.indeed.com/rc/clk?jk=314cbd35d87dd902&fccid=75a3a5a15b202084&vjs=3,"Costa Mesa, CA 92626 92626+1 location",Data Engineer,Data Engineer
239,"Antra, Inc","We have essentially three separate roles for which I am recruiting. The JDs of each is given below. Please go through and apply as per your interest. You shall be contacted once you have applied.React Developer role (5+ vacancies)In this role as React Developer, you should be passionate about developing solutions to achieve business needs and will be involved in all aspects of the software development lifecycle including technical design, implementation, testing, deployment and support of cutting-edge applications.Responsibilities: · Provide value by integrating business rules and content in accordance with requirements· Work closely with Product Owners, Analysts and QA in an Agile environment to ensure quality, security and maintenance of applications, and to ensure code meets development standards and guidelines.· Support design, development, testing and deployment of software solutions using JavaScript/Typescript and Angular Framework for the client side development and using Java and Spring Framework to develop the server side business logic of the application.· Develop, document, and advocate SOLID software architecture practices· Contribute to a DevOps culture and development of continuous integration processes and tools.· Ability to jump between frontend and backend work· Adhering to established methodologies while continuously analyzing processes for improved performance and adaptability.· Contribute to the success of your team and companyJob Requirements: - Graduate degree or undergraduate degree in Computer Science, Computer Engineering, Information Technology, Information Systems, Software Development, Electrical and Computer Engineering , Electrical Engineering or relevant experience.- 0-3 years of software development with knowledge of software development life cycle- Knowledge of Object Oriented Programming with some experience in any OOP programming languages like Java.- Knowledge on fundamentals of HTML, CSS and JavaScript.- Knowledge of web services with JSON and REST, SOAP/XML is a plus.- Knowledge of any frameworks such as NodeJS, Angular or ReactJS is a plus.- Knowledge on JavaScript ES6 features such as classes, arrow functions, string templates, rest/spread operators, promises, async/await is plus.- Experience with designing and developing RESTful web services is a plus.- Be a self-starter with an ability to troubleshoot and seek answers to problems.- Have hands-on design and development of software applications/systems.- Proven analytical skills with excellent oral and written communication skills.- Responsible for coding, testing, and implementation of solutions within established standards and documentation guidelinesxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxJD for Java backend Developer (15+ vacancies)In this role, you will design, develop, modify, adapt and implement short- and long-term solutions to information technology needs through new and existing applications, systems, databases and applications infrastructure. You will review and interpret system requirements and business processes. You will code, tests, debug and implement software solutions.Responsibilities: · Utilize Java and related technologies to design, develop, test and deploy various large-scaled, distributed server side applications.· Be involved in all phases of software development life cycle.· Gather and analyze requirements from Product Managers/Owners.· Build tools to improve application reliability and quality, and programmer productivity.· Align teams designs with larger architecture objectives.· Assist team members in design discussions and decisions.· Perform code and design reviews with other team members.· Create and execute test cases based on test strategies and test plans· Work closely with Operations & Infrastructure groups to understand challenges in production environments.Qualifications: - Bachelor's degree in Computer Science or related fields.- 0 to 3 years of experience in Core Java programming.- Proficient in data structure, algorithm, object-oriented design and multithreading.- Experience in basic database design including SQL database or NoSQL databases.- Familiar with Java 8 new features like Lambda and Stream.- Knowledge about network protocols and network security.- Basic knowledge about design patterns.Preferred Qualifications: - Experience in Agile methodologies.- Experience in Web Application development.- Knowledge in distributed system design.- Experience in Cloud Platforms like AWS, Azure or similar.- Experience in DevOps and related tools like Jenkins, Maven, GitLab and shell script.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxJD for Azure Data Engineer (15+ vacancies)This role requires the design, development and implementation of data solutions to business problems. An Azure Data Engineer will be expected to perform duties such as: evaluating the performance of current data solutions, designing and implementing cloud and hybrid data solutions. Ability to adapt and learn new technologies per business requirements is also needed.Responsibilities: · Design and implement data solutions using industry best practices.· Performs ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards.· Monitor and maintain data pipelines proactively to ensure high service availability.· Works with Data Scientists and ML Engineers to understand mathematical models and optimize data solutions accordingly.· Continuous development through training and mentorship programs.· Create scripts and programs to automate data operations.You meet our “must haves” for this role if you have: · Minimum Bachelor’s degree in Data Science, Business Intelligence, Computer Science or related fields, or the equivalent combination of education, professional training, and work experience.· 0-3 years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala and R.· Experience working with relational databases such as SQL Server, Oracle and MySQL.· Experience working with NoSQL databases such as Redis, Mongo DB, Cosmos DB.· Excellent problem-solving skills and ability to learn through scattered resources.· Thorough understanding of the responsibilities and duties of a data engineer, as well as established industry standards/best practices and documentation guidelines.· Outstanding communication skills, and the ability to stay self-motivated and work with little or no supervision.· Authorization(s) to work lawfully in the United States.Plus, if you meet any the of requirements: · Experience with cloud based data technologies.· Experience with distributed systems utilizing tools such as Apache Hadoop, Spark or Kafka.· Working experience in Agile Scrum environments.· Experience with source control tools such as Git, SVN and TFS.Job Types: Full-time, ContractPay: $60,000.00 - $75,000.00 per yearBenefits:Dental insuranceHealth insuranceLife insurancePaid time offRelocation assistanceVision insuranceSchedule:8 hour shiftDay shiftMonday to FridayCOVID-19 considerations:All staff members are adhering to COVID appropriate behavior while on premises at office, not limited to wearing of masks, partial attendance at office, proper social distancing, sanitizing common places and oneself regularly, etc.Ability to commute/relocate:Centreville, VA: Reliably commute or willing to relocate with an employer-provided relocation package (Preferred)Application Question(s):As this is an entry level position, would you be willing to avail free in-house training specific to the job, before project placement?Work Location: Multiple Locations","https://www.indeed.com/company/Antra,-Inc/jobs/Entry-Level-Software-Data-Engineer-Position-1df1885f1867907c?fccid=550311fe7e2165d9&vjs=3","Fairfax, VA",Entry level software and data engineer positions,Data Engineer
240,Adobe,"locations
San Jose
Lehi
time type
Full time
posted on
Posted Today
job requisition id
R123629
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
The Opportunity
We are looking for top-tier analytics engineer to join the DX Marketing Measurement & Operations team! Data has become a strategic asset to the organization in driving all aspects of business decision-making, personalizing customer experiences in real-time, and driving the future of Adobe's business.
This position will play a key role on the DX MM&O team and focus on building data products and infrastructure to support data and insights needed to realize business objectives. You will be part of a learning culture, where partnership and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued!
What you'll Do
We are seeking a hardworking contributor who will drive data engineering projects to build data assets and software applications.
You should be able to provide technical direction and system architecture expertise for individual initiatives. You will collaborate with key partners from Marketing Insights, Measurement COE, Marketing Technology, and the Central Data Services teams and apply your technical proficiency across different stages of the software development process, including requirements elicitation, application architecture definition, and design. You will play a substantial role in crafting high-level design artifacts. You will also develop high-quality code for different modules, lead validation for all types of testing, and support activities related to implementation and transition.
What you need to succeed
BS in Computer Science
5+ years of Data Engineering experience
Experience with SQL Server, Hadoop, Hive, Sqoop, HDFS, Spark, Scala
Familiar with cloud environments (AWS, Azure, OpenStack)
Working knowledge of Linux and a high-level scripting language, such as Python.
Strong attention to detail and excellent analytical capabilities.
Exceptional communication, documentation, and presentation skills along with the ability to stay ahead of the latest information management tools and technologies.
Ability to work within timelines and prioritize and execute tasks.
Collaborate with internal teams and vendors to fix and improve products and drive optimization and automation.
Document development phases and monitor systems.
Ensure software is up to date with the latest technologies.
DevOps knowledge is an added advantage.",https://www.indeed.com/rc/clk?jk=87e37baff12492dc&fccid=f89deb5a97c7738a&vjs=3,"San Jose, CA+7 locations",Data Engineer - DX Marketing,Data Engineer
241,CloudPareto,"Who is CloudPareto
CloudPareto develops digital products to accelerate government innovation. One of the fastest-growing startups supporting the public sector, CloudPareto is rapidly expanding its engineering, consulting, and go-to-market teams as we begin to enter into our next phase of growth. We have big visions for the future and are on the lookout for team members interested in making an outsized impact in the market.
The Team You Will Work With
This team member is part of our Engineering Department that utilizes the latest development principles and creates genre-defining products to challenge the status quo. Our products and tools bring innovation, user-centric design, and critical thinking. The Data Scientist, Associate will be tasked with creating innovative, customer-centric models in the Federal market, pushing the envelope on Machine Learning capabilities.
Position Details & Benefits:
Compensation: Up to $180,000 (commensurate with fit)
US citizenship required, Full-time, Salaried, Exempt
90-100% Remote
Unlimited PTO
85%-100% company coverage for BCBS/Guardian options on Health, Dental, Vision, Life, Short-Term Disability
100% match up to 6% of 401k contribution, vested immediately
Qualifications:
3+ years of industry experience working with the AWS Data Storage technologies such as S3, DynamoDB, Aurora, and ECS as well as data analytics platforms such as Databricks
Technical and Data Experience
Proficiency in at least one high-level programming language: Java, Scala, Python, R
Experience with the AWS Data Storage technologies such as S3, DynamoDB, Aurora, Redshift, and ECS.
Experience developing ETL pipelines between various AWS Storage capabilities.
Experience modeling and deploying relational, NoSQL, and graph data models
Experience in monitoring of CRUD performance of databases through cloud-native and third-party applications/tools.
Create, scale, and maintain data pipeline architecture across a cloud-based ecosystem
Demonstrated success in implementing and enforcing data standards and best practices
Implement data pipeline architecture decisions based on appropriate performance, cost, and security optimization best practices
Teaming Experience
Experience with highly agile projects including comfort with uncertainty and ambiguity
Partner with upstream engineering teams and downstream analytics teams
Work with Engineers, Product Managers, and Analytics personnel to specify product data requirements and strive for greater functionality in our data systems
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-design infrastructure for greater scalability, etc.
Participate in true agile development such with ceremonies such as scrum, bi-weekly sprints, and demos
Exercise strong written, diagraming, and verbal communication skills to translate complex technical concepts into simplified explanations and expand upon open-ended requirements
CloudPareto and its employees operate to fulfill the following core values:
#WorldClass: We create world-class products for our world-class Government
#MasterCraft: We collectively strive, together, to be true masters of our respective crafts
#WorkBackwards: We see the future, strategically envision our end state, and work backward with intent
#DoBeautiful: We believe that if it's worth doing, it's worth doing beautifully and elegantly
#SoftwareScale: We scale through software to be more efficient, effective, and precise
#Perseverance: We overcome the impossible through brilliance, grit, and class
#CelebrateUs: We celebrate triumphs; we celebrate differences; we celebrate balance; we celebrate us
#OneTeam: We are one team and we play to win big
Gi2Hmlq5cD",https://www.indeed.com/rc/clk?jk=6512bd755587887a&fccid=c765e8a755ec4d66&vjs=3,Remote,Senior Data Engineer,Data Engineer
242,City of Chesapeake,"Position Information
Working Title of Vacant Position Software Engineer I - Data Analyst

Job Type Full Time

Posting Type Public

Number of Vacancies 1

Department INFORMATION TECHNOLOGY

Division IT SYSTEM DEV

Requisition Number 2021416P

Number of hours worked per week 40

Work Schedule
8AM – 5PM; Monday – Friday; Employees may be expected to work hours in excess of their normally scheduled hours in response to short-term department needs and/or City-wide emergencies.

Work Site Location 300 Shea Dr

Position Driving Requirement N - Never

Pay Basis Semi-Monthly

Advertised Salary
$64,885 – $85,973/yearly; Depending on Qualifications

Job Description
Do you visualize patterns in data? The City of Chesapeake is looking for a Software Engineer to help transform and provide insight into the business. As a Software Engineer, you will perform thorough analysis, design, and develop or modify information systems, applications, and programs. You will work within broad policy and organizational guidelines, independently plan and implement projects, and report the progress of major activities through periodic conferences and meetings. You will design data modeling/analysis services used to mine enterprise systems and applications for knowledge and information that enhances business processes. You will be responsible for assisting in building, deploying, and maintaining data support tools, metadata inventories, and definitions for database file/table creation.
Typical Tasks:
Interpret data, analyze results using statistical techniques and provide ongoing reports
Develop and implement databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality
Acquire data from primary or secondary data sources and maintain databases/data systems
Identify, analyze, and interpret trends or patterns in complex data sets
Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
Work with management to prioritize business and information needs
Locate and define new process improvement opportunities
Consults with users and other information technology staff to identify user problems and design new or existing systems; assists with the preparation of a timeline and project plan for the development or enhancement of new/existing software application programs.
Analyzes user requirements, develops solutions, and installs and documents software solutions.
Develops or modifies new or existing data flows of moderate complexity and scope.
Designs, develops, tests, debugs, implements, and documents both new and existing data-related systems to meet business functional requirements and industry best practices.
Integrates and implements software packages and coordinates necessary training for users.
Assists other developers in the development of applications of a more complex nature.
Performs other related duties as assigned or requested


Required Qualifications
VOCATIONAL/EDUCATIONAL REQUIREMENT: Requires a bachelor’s degree or any equivalent combination of education and experience in computer science or a closely related field. EXPERIENCE REQUIREMENT: In addition to satisfying the vocational/education standards, this class requires a minimum of two years of full-time experience in software development and engineering. SPECIAL CERTIFICATIONS AND LICENSES: None. SPECIAL REQUIREMENTS: Employees may be expected to work hours in excess of their normally scheduled hours in response to short-term department needs and/or City-wide emergencies.

Preferred Qualifications
A successful Software Engineer candidate will have various skills and qualifications that help them solve problems using large amounts of data.
Coding skills in languages such as SQL, Python, C#, and/or R
Analytical and problem-solving skills
Experience with statistical software (e.g., Stata, SPSS)
Knowledge of data gathering, cleaning and transforming techniques
Reporting and data visualization skills using software like Power BI, Tableau, or others BI Tools
Understanding of data warehousing and ETL techniques
Proficiency in Microsoft Excel
Ability to set and meet deadlines
Experience with statistical software
Technical writing skills
Excellent attention to detail
Strong written/verbal communication skills
Ability to QA and troubleshoot data

Posting Detail Information
Job Open Date 01/13/2022

Job Close Date 03/21/2022

Open Continuous No

Special Instructions to Applicants

ADA Checklist
Overall Physical Strength Demands
Overall Physical Strength Demands S=Sedentary - Exerting up to 10 lbs. occasionally or small weights frequently; sitting most of the time.

Physical Demands
C = Continuously- 2/3 or more of the time. F = Frequently- From 1/3 to 2/3 of the time. O = Occasionally- Up to 1/3 of the time. R = Rarely- Less than 1 hour per week. N = Never- Never occurs.
Standing Rarely- Less than 1 hour per week.

Sitting Continuously- 2/3 or more of the time.

Walking Rarely- Less than 1 hour per week.

Lifting Rarely- Less than 1 hour per week.

Lifting Amount Exerting up to 10 lbs

Carrying Rarely- Less than 1 hour per week.

Carrying Weight Exerting up to 10 lbs

Pushing/Pulling Rarely- Less than 1 hour per week.

Pushing/Pulling Weight Exerting up to 10 lbs

Reaching Rarely- Less than 1 hour per week.

Handling Rarely- Less than 1 hour per week.

Fine Dexterity Rarely- Less than 1 hour per week.

Kneeling Rarely- Less than 1 hour per week.

Crouching Never- Never occurs.

Crawling Rarely- Less than 1 hour per week.

Bending Rarely- Less than 1 hour per week.

Twisting Rarely- Less than 1 hour per week.

Climbing Rarely- Less than 1 hour per week.

Balancing Rarely- Less than 1 hour per week.

Vision Continuously- 2/3 or more of the time.

Hearing Frequently- From 1/3 to 2/3 of the time.

Talking Frequently- From 1/3 to 2/3 of the time.

Foot Controls Rarely- Less than 1 hour per week.

Machines, Tools, Equipment and Work Aids Used

Protective Equipment Required

Health and Safety
D = Daily W = Several Times Per Week M = Several Times Per Month S = Seasonally N = Never
Mechanical Hazards N = Never

Chemical Hazards N = Never

Electrical Hazards N = Never

Fire Hazards N = Never

Explosives N = Never

Communicable Diseases N = Never

Physical Danger or Abuse N = Never

Other N = Never

If Other, Description

Environmental Factors
D = Daily W = Several Times Per Week M = Several Times Per Month S = Seasonally N = Never
Dirt and Dust N = Never

Extreme Temperatures N = Never

Noise and Vibration N = Never

Fumes and Odors N = Never

Wetness/Humidity N = Never

Darkness or Poor Lighting N = Never

Primary Work Location Office Environment

Non-Physical Demands
C = Continuously- 2/3 or more of the time. F = Frequently- From 1/3 to 2/3 of the time. O = Occasionally- Up to 1/3 of the time. R = Rarely- Less than 1 hour per week. N = Never- Never occurs.
Time Pressures Occasionally- Up to 1/3 of the time.

Emergency Situations Rarely- Less than 1 hour per week.

Frequent Change of Tasks Occasionally- Up to 1/3 of the time.

Irregular Work Schedule/Overtime Rarely- Less than 1 hour per week.

Performing Multiple Tasks Simultaneously Frequently- From 1/3 to 2/3 of the time.

Working Closely with Others as Part of a Team Frequently- From 1/3 to 2/3 of the time.

Tedious or Exacting Work Occasionally- Up to 1/3 of the time.

Noisy/Distracting Environment Rarely- Less than 1 hour per week.

Other Rarely- Less than 1 hour per week.

If Other, Description

Can anyone assist the employee in performing the primary tasks assigned to this position? If yes, identify the eligible task(s)
No

Reference Requests
References
Minimum Requests 2

Maximum Requests 4",https://www.indeed.com/rc/clk?jk=9e4185dd23a370c1&fccid=dace3c8809b994d4&vjs=3,"Chesapeake, VA 23322 23322 (Great Bridge area)",Software Engineer I - Data Analyst,Data Engineer
243,National University,"SUMMARY:
This position creates, implements, and maintains a design for the storage and maintenance of various on prem and off prem database systems. This position is also responsible for participating in the development and deployment activities for those systems. Create scripts for data maintenance and bug fixes. Works with various teams to provide information, knowledge, coordination, and tools that support the growth and continued success of NEP. Relies on knowledge and professional discretion to achieve goals. Significant ingenuity and flexibility is expected.

ESSENTIAL FUNCTIONS:
Develop and maintain data extraction, transfer and loading (ETL) from various sources using Python, SSIS or other tools.Implement appropriate security measures throughout the data pipeline.Responsible for maintaining metadata management and data quality activities so that data are accurate, reliable and documented.Provide support as needed to production systems ensuring that both internal and external client’s needs are met.Facilitate the transmission and understanding of data to enable fact-based decisions to various stakeholders, such as leadership, faculty and staff.Provide reliable and timely data that supports strategic planning, student success initiates, and educational and operational effectiveness.Utilize various software tools and reporting services to deliver actionable data to end-users in a digestible formPossess a positive and constructive attitudeReasonable and consistent Attendance to fulfill requirement of the position

KNOWLEDGE, SKILLS, & ABILITIES:
T-SQL or other SQL language Databases: MSSQL, PostgreSQL, Python, R, SSIS, SSRS, TableauAzure or Google Cloud Platform or AWS/On-Prem Hard Skills: Build, test, and improve/maintain ETLProficient with Microsoft Word and other applications in the Microsoft Office SuiteDeploy/Rollback DB schema changes Create/Alter tables, views, stored procedures, functions, indexesImprove data availability, usability, integrity and securityEnjoys data wrangling. Eager to understand internal/external business reporting needs. Ability to communicate effectively and work with business stakeholders to arrive at the appropriate solution.Comfortable working across departments to gather/spread necessary knowledge to complete a project. Willingness to learn new languages/technologies as needed for the job.

EDUCATION & EXPERIENCE:
Bachelor’s degree in a related discipline
One to three (1-3) years of recent professional experience in data analysis and/or data engineering or equivalent educational experience.Experience using Azure or Google Cloud Platform or AWS/On-Prem Hard Skills: Build, test, and improve/maintain ETL
Experience in higher education preferredExperience working in a technology-driven enterprise preferredAll skills, abilities and education will be considered for minimum qualifications

WORKING CONDITIONS:
This position operates in a remote, home office environment. This role routinely uses standard office equipment such as computers, printers, and phones.
Good working environment with the absence of disagreeable conditions.
The noise level in the work environment is usually moderate.Regular and reliable attendance is required.",https://www.indeed.com/rc/clk?jk=4a1d4526b213abcb&fccid=f0a51b1dc14462e9&vjs=3,"Salt Lake City, UT 84101 84101 (People's Freeway area)+5 locations",Data Engineer,Data Engineer
244,Blue State,"The role

As a Data Engineer at Blue State, you'll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations. You'll design, build, and manage the systems and processes which form the underpinning of Blue State's analytics work, supporting and working alongside data analysts and campaign strategists. But you'll also work directly with Blue State's clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations.

Day-to-day responsibilities:

Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects
Work with Blue State's client organizations to solve difficult data migration, management, and integration challenges
Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes
Automate workflows and look for further opportunities to improve efficiency in our work
Always maintain a high level of data security and privacy
The team

You will be a part of the global Web and Product Development team working primarily with our creative agency on client projects. You'll work in either the NY or DC office.

Once Blue State offices reopen, on-site presence is strongly preferred at a minimum of two days a week. To enter our US offices or attend Blue State events, staff and visitors must be fully vaccinated against COVID-19, including with a booster shot when eligible. Exceptions for protected grounds will be reviewed on a case-by-case basis.

Top things we're looking for
Good foundational understanding of statistical analysis
Extensive experience working with SQL databases in an analytics or business intelligence context
Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools
Experience with ETL/ELT tools, processes, and best practices
Strong Python experience:
Python should be your go-to tool for solving problems. If the first thing you want to do when you have to do the same thing twice is write a Python script to automate it - we want you!
Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus
Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks
Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:
Designing data warehouse schemas for cross-channel marketing analytics
Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data
Building and running automated tasks within the GCP environment - e.g. Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler
Comfortable managing GCP IAM policies across projects and teams
Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if you've extended Google Sheets using Google Apps Script
Familiarity with Git and maintains good habits around code maintenance
Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)
Good at teaching others what you know.

At Blue State, diversity is a necessity, not a nice-to-have. We encourage those from underrepresented communities — women, people of color, LGBTQIA+, immigrants, indigenous folks, those with disabilities and people at all the intersections in between — to apply. Even if you don't think your current skill set checks every box below, but this role seems to align with your strengths, we want to hear from you.

The minimum starting salary for this position is $90,000; compensation is otherwise commensurate with experience.

The company

Blue State is a values-led creative and campaigns agency that partners with leading causes, companies, and campaigns to build better organizations for a better world. We drive real change, make good trouble, put people first and are constantly curious.

We believe that there is no force more powerful than people taking collective action on the things they care about. We don't pretend to have all the answers, but we know where to find them: in people. We listen, learn, and uncover new insights that often surprise us and our clients — and move us toward better results. Across clients including UNHCR, Amnesty International, Google, Tesco, Nesta, and Tate. We have offices in New York City, Washington DC, London, Oakland and Chicago.",https://www.indeed.com/rc/clk?jk=7359029ebacaafc6&fccid=4d9339102788fdca&vjs=3,"Washington, DC 20005 20005 (Downtown area)+2 locations",Data Engineer,Data Engineer
245,Netflix,"Remote, United States
Data Science and Engineering
Netflix is revolutionizing entertainment itself and pushing the limits of what it means to be a subscription business. From the Netflix Second Quarter 2021 Letter to Shareholders: ""In Q2, revenue increased 19% year over year to $7.3 billion, while operating income rose 36% year over
year to $1.8 billion. We finished the quarter with over 209m paid memberships, slightly ahead of our forecast."" As part of the Membership Data Engineering team, you help the entire company build on top of this most mission-critical data: our 200M+ members.

This role requires folks who enjoy the business complexity that emerges as we manage the member lifecycle by interfacing with membership and billing systems to report our earnings release numbers accurately and enable forecasts as well as analytical insights. As part of the Membership Data Engineering team, you will play a vital role in creating reliable distributed data pipelines that will allow self-service. We expect you to show thought leadership and partner effectively with our business and engineering teams to develop better metrics and more elegant insights.
Location of work: For this role, we are considering both candidates who are willing to relocate to Los Gatos, California, and fully-remote candidates (remote in the US with occasional visits to Los Gatos). For fully-remote work, you should be a remote-work expert who will help uplevel the team’s collaboration skills!
Who are you:
You are proficient in SQL (any variant) and at least one major language (e.g., Java, Scala, Python). You strive to write beautiful code, and you're comfortable with picking up new technologies.
You have strong communication skills to partner with data scientists and engineering stakeholders effectively. In addition, you love being the bridge between Finance and Engineering teams.
You LOVE data of all sorts, big and small! You enjoy helping teams push the boundaries of extracting business insights from our data.
You have a strong background in data pipelining, distributed data processing, software engineering components, and data modeling concepts.
You are always looking for opportunities to simplify, automate tasks, and build reusable components reusable across multiple use cases and teams.
You have an eye for detail and realize where accuracy is critical. You like to spark joy in internal partners with high-quality data products that have great documentation and are easy to understand.
You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback.
You have preferably worked with Financial Accounting teams and understand their terminology and processes. You can help Netflix meet the compliance requirements of these teams (e.g., SOX)
Learn more about the team, technologies, and the immediate team members you’d get to work with! If you’d like us to make changes to the interview process to improve the odds of your sailing through it with flying colors, please share your thoughts with us - we promise to do whatever is feasible to accommodate.
APPLY NOW
Share this listing:
LINK COPIED",https://www.indeed.com/rc/clk?jk=1699b30e2ac8cf27&fccid=66403b30a2c0d89c&vjs=3,+4 locationsRemote,"Senior Data Engineer, Membership",Data Engineer
246,Snapdocs,"Snapdocs is a rapidly growing company backed by investors like Sequoia, Y Combinator, F-Prime and Tiger Global. We're an innovative team taking on the extensive mortgage market, bringing scalable and sophisticated software to a pillar of the US economy that still relies on fax machines and manila envelopes.
As a Lead Data Engineer, you will lead the Data Engineering team in building our data warehouse, data catalog and externally facing data products. This position will be hands-on, in addition to having 2 direct reports, with a focus on coaching and technical development. Partnering with our Head of Data, you will play a major role in shaping the Data Engineering roadmap and strategy for all of Snapdocs. As a whole, you will own the architecture of our data systems, while making key technical and business decisions. The first project will be a complete overhaul of the data warehouse, from there scope expands to architecting and building the data catalog to building new products from scratch, so there is huge opportunity for impact.

Ideally, you have 5+ years of experience in the field of Data Engineering, including previous work architecting, building data warehouses, and shipping data products. Since you will have direct reports, you should have some experience maintaining a ""player-coach"" mentality while remaining involved technically in objectives and mentoring the team. This team collaborates very closely with our Data Science, BI, and Security teams so prior experience working cross-functionally is a must. From a technology perspective, our Data Engineering team is working with Python, SQL, and Airflow. Bonus points for those who consider themselves an architecting expert and have worked with diverse tools!

Colorado applicants: This role pays $150,000.00 - $178,000.00 annually plus equity in the company (pay for other locations may vary).

#LI-LW1 #LI-remote
Snapdocs strongly values diversity and drive. We want to work with people of different backgrounds and different paths in life, and we trust our team to make smart decisions. This means we value independent work as well as collaboration. We provide outstanding benefits (listed below) and while we have hubs in both San Francisco and Denver, we're an extremely remote-friendly company with over a third of our staff outside of those two hubs!

Our benefits include (but are not limited to):

Excellent health, dental, and vision benefits
401(k) with up to 4% company match
16 weeks paid parental leave (regardless of gender)
Flexible time off policy
Flexible spending account for healthcare and dependent care
Galileo, Modern Health, Urban Sitter, and Northstar Financial memberships
Life and disability insurance
Commuter benefits
10 year exercise window on your equity (!!)

Snapdocs is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know.

California residents applying for positions at Snapdocs are subject to our candidate privacy policy.",https://www.indeed.com/rc/clk?jk=2626944ddb8b3ab0&fccid=628f32dda01beb0f&vjs=3,"Remote in San Francisco, CA 94102 94102",Lead Data Engineer,Data Engineer
247,KIPP SoCal Public Schools,"Who We Are
KIPP SoCal Public Schools is a network of 22 high-performing, college-preparatory public charter schools. We educate more than 9,800 students in grades pre-K through eighth in the greater Los Angeles area and San Diego. Additionally, we support approximately 5,100 alumni on their journey to and through high school and college.
What We Believe
KIPP SoCal believes that the purpose of education is for liberation. This means that we see, develop, and inspire the limitless potential in each and all of our KIPPsters.
What We Do
At KIPP SoCal we build trusting relationships that are strengthened by high expectations and collaboration alongside laughter and joy. We set ambitious goals and hold each other accountable for students achieving their greatest potential. We work together with each other and in partnership with families and communities in the active pursuit of a more equitable world. We do the right thing, even when it's not always comfortable standing up and speaking out against anti-blackness, racism, oppression, and injustice. We persevere through obstacles and treat failure as an opportunity to learn and grow.
The Opportunity
The Data Engineer is responsible for the region’s data warehouse, internal data dashboard website (KASTLE), and system integrations. This position will maintain and improve the organization’s data warehouse, data pipelines, and other data analytic solutions with a focus on effectiveness and sustainability. This role is critical to KIPP SoCal’s ability to access and analyze data to drive decisions in the best interests of our students for years to come. Success in this role requires tenacious problem solving, application of technical expertise, adaptability, strong organizational and prioritization skills, and continuous self-directed learning.
This is a telecommuting role where most of the work can be done remotely but some on-site meetings are required. Reporting to the Chief of Staff, the Data Engineer will lead work in the following areas:
Data Warehouse Management & Development
Lead the design and management of KIPP SoCal’s data warehouse, identifying new data pipelines and new database tables to create
Update and create database objects (tables, views, indexes, stored procedures, etc.) to ensure optimal performance and organization
Review database tables to ensure data accuracy
Prep and import survey (TNTP, family, student) and state (state assessments, California Dashboard, student enrollment, etc.) data into the data warehouse
Create and maintain ETL scripts to fulfill the data needs of analytics
Perform regular database monitoring duties to ensure databases, servers, and workflow management systems (Apache Airflow) remain stable, accessible, and secure
Independently analyze and solve for database or server related issues in real-time
Manage Amazon Web Services subscriptions, account provisioning, and drive technology investments to ensure that cloudware/software needs are maintained and improved over time
System Integrations
Create and update scripts that integrate information across platforms using SQL, APIs, ETL processes, and other methods
Review scripts for integrations written by Analytics Team
Manage user accounts in Whetstone via an automated script, supplemented by manual additions requested through help desk tickets
Work with stakeholders to understand source systems and gather specifications for integrations
Code Review
Lead the use of Git applications on the data team for version control and collaboration
Develop and implement the code review process for purposes of learning and improvement, code sustainability, and finding errors
Data Analytics Web Platform (KASTLE)
Review other team members’ dashboards before publishing to KASTLE, checking for data accuracy and data visualization best practices
Update context content on KASTLE (announcements, pop-ups, FAQ page, etc.)Respond to comments submitted by KASTLE users through the feedback forms on KASTLE
Manage the Analytics Team’s WordPress site, including user account creation and custom HTML/CSS and PHP widgets
Ensure that all KIPP SoCal employees have access to KASTLE
Support the development and maintenance of organizational dashboards in Tableau
Documentation
Develop and maintain technical documentation for all areas of responsibility, including data flow diagrams and data dictionaries
Create documentation of system integrations for non-technical audiences
You'll Be Qualified If You Have
Experience managing a data warehouse (back-end architecture, front-end interface, tables, views, stored procedures, etc.)
Strong knowledge and experience with SQL and at least one other programmatic language to wrangle and clean data (e.g. Python, R)
Experience with cloud computing platforms and Apache Airflow (or similar)
Strong quantitative, analytical, and critical thinking skills with demonstrated proficiency in managing large and/or challenging data sets
Knowledge of and experience working with K-12 educational data
Experience with business intelligence technologies (e.g. Tableau)
Strong communication skills and customer service orientation
Attention to detail and commitment to data integrity
Ability to juggle multiple projects simultaneously
Deep commitment to KIPP SoCal’s mission
Model the values of KIPP SoCal (lead with love, strength in the collective, equity, and excellence, curiosity and integrity)
Joining KIPP SoCal Means the Following
Working at an organization committed to the hard work necessary to ensure education is for liberation
Spending time with students and families in the best communities of LA, Compton, and San Diego
Support and care from fellow regional team members
Ongoing professional development and coaching from manager
Working with Us Means the Following Perks
Salary range of $68,000 - $85,000 per year
Employer-paid KIPP SoCal life insurance
401(k) plan with up to 6% match by KIPP SoCal
Voluntary Benefits (Pet Insurance, Supplemental Life, Accident, Short Term Disability, etc.)
Generous paid time off program includes 22 holidays, sick leave, and vacation
Other great benefits (Flexible spending account, EAP, college tuition benefits program through Guardian Dental plan, etc.)
** PLEASE NOTE: KIPP SoCal Public Schools requires that all team members be fully vaccinated against COVID-19 for the safety of our students, family, and staff. Team members may apply to seek a medical exemption. Regardless of your current vaccination status, our team can advise you on how to fulfill this important requirement so that you can start the position in a timely way. We’re here to help. **
KIPP SoCal Public Schools is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, age, national origin, veteran status, disability, sexual orientation/gender identity, or any other characteristic protected by applicable law.
KIPP SoCal strives to ensure that our careers website is accessible to all, including individuals with disabilities. If you require reasonable accommodation for any part of the application or hiring process due to a disability, please contact us. You can find our contact information on our website, http://www.kippsocal.org/. Information will be sent to a talent acquisition representative who will provide assistance to ensure appropriate consideration in the hiring process.",https://www.indeed.com/rc/clk?jk=f9cefddd0b658f26&fccid=7340fb5bffd8c4f7&vjs=3,"Remote in Los Angeles, CA 90063 90063",Data Engineer,Data Engineer
248,CareFirst BlueCross BlueShield,"Resp & Qualifications
PURPOSE:
Understands data needs and advise company on technological resources. Aggregate and analyze various data sets to provide actionable insight. Develop reports, dashboards, and tools for business-users. Develop technical solutions to improve access to data and data usage. Develops and executes ETL code and ensures that the data loading processes are reliable and data quality remains high.

ESSENTIAL FUNCTIONS:
Writes and executes ETL code, primarily complex SQL/NoSQL script, but with a strong ability to leverage other technologies.
Analyzes and validates existing routines and communicates impacts. Assists with data transformation, manipulation, and presentation to users.
Works with Business Analysts, Business Intelligence Developers, Infrastructure Architects, and key business users to review, design, and develop data models and ETL processes.
Supports the Software Engineering team with data analysis and validation with extensive SQL queries.
Designs, configures, implements, monitors, and manages all aspects of Data Integration Framework.
Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, Denormalized Data Structures, OLAP, and Data Warehousing concepts.
Defines Data Integration best practices for the data management environment.
SUPERVISORY RESPONSIBILITY:
This position has no direct reports, however, may informally lead teams in a matrix environment.

QUALIFICATIONS:
Education Level: Bachelor's Degree in Information Technology or Computer Science or Engineering

Experience: 3 years Hands-on experience with database management systems, ELT/ETL systems, business intelligence tools, defect management and system testing OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of IICS including, but not limited to, general administration, roles and access policies, CI/CD integration etc.
Knowledge and understating of troubleshooting and resolving IICS issues
Knowledge and understanding of IICS integration with services on cloud platforms (Azure preferred) and third party tools
Knowledge and understanding of DevOps best practices with for IICS
Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python)
Knowledge and understanding of data exchange formats
Knowledge and understanding of database design and implementation concepts
Ability to recognize, analyze, and solve a variety of problems
Knowledge and understanding of data movement concepts
Excellent communication skills both written and verbal
Must be able to effectively work in a fast-paced environment with frequently changing priorities, deadlines, and workloads that can be variable for long periods of time. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.
Department
Department: Data Engineering
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
Please visit our website to apply: www.carefirst.com/careers
Closing Date
Please apply before: 11.29.2021
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship",https://www.indeed.com/rc/clk?jk=2a3ce2c570e14d89&fccid=5c58e93da349f0c6&vjs=3,"Washington, DC 20002 20002 (Capitol Hill area)+3 locations",Data Engineer,Data Engineer
249,Unify Consulting,"We are a magnet firm for superstar consultants, respectfully disrupting the consulting industries in Seattle and Bay Areas. We deliver data-driven people-powered solutions because our consultants are versatile; blending management wisdom with technical chops. We are growing our meritocracy, free of middlemen, with fun, relationship-oriented optimists who value camaraderie.We are currently seeking a Data Engineer to join our team in San Francisco. The Data Engineer is responsible for implementing scalable and sustainable solutions that impact the flow and analysis of data. These solutions may be addressed by traditional database and scripting technologies, or by the introduction of new systems and/or work flows.The successful candidate will be a strong individual contributor with the ability to execute specialized data solutions and make departures from traditional approaches, if needed. They will possess skills in design, development, implementation, documentation and management of data solutions, and be expected to clearly articulate and present technical information to clients and stakeholders.Responsibilities: · Understanding of how to model data to optimize report and dashboard creation· Working knowledge of cloud-based data repositories and associated data processing tools· Working knowledge of data quality approaches and techniques· Familiarity implementing good security practices for sensitive dataRequired Qualifications: · 5+ years experience in data integration, data warehousing, and data modeling· 5+ years experience with relational database technologies (SQL Server, Oracle, Teradata, or similar) and very strong SQL skills· 5+ years experience with ETL/ELT tools such as Informatica PowerCenter/BDM, Talend, Attunity, Datastage, Apache Spark, or similar· Experience with data visualization tools such as PowerBI or Tableau· Programming/scripting language experience (Python, Java, Powershell, etc.)· Excellent communication skills with the ability to relay information between technical and non-technical teams· Soft skills· Display curiosity about new technologies and stays current on BI trends· Ability to work independently on multiple projects and prioritize workload· Sharp skills in critical thinking and problem solving· Adapt to change and manage ambiguity· BS Degree or equivalent required in Computer Science or similar area of studyPreferred Qualifications: · Demonstrated presentation skills· Experience working with big data technologies, preferably cloud-based, such as Redshift, Snowflake, Databricks, Azure SQL Data Warehouse, MongoDB, Cosmos, or similarJob Type: Full-timePay: $115,125.00 - $224,044.00 per yearBenefits:401(k)Dental insuranceFlexible spending accountHealth insuranceHealth savings accountPaid time offVision insuranceSchedule:Monday to FridayWork Location: Remote",https://www.indeed.com/company/Unify-Consulting/jobs/Data-Engineer-ff721a03114ec1de?fccid=e1e5ee5e82327809&vjs=3,Remote,Data Engineer,Data Engineer
250,AstraZeneca,"Senior Data Engineer - Wilmington, DE or Gaithersburg, MD
At AstraZeneca, patients are at the heart of everything we do. A global company that pushes the boundaries of science, we all share a big purpose: to discover and develop life-changing medicines that save lives.
In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). As a Senior Data Engineer you will design, build and support data pipelines consuming data from multiple source systems and transforming it into valuable and insightful information. You will have the opportunity to work closely with business and contribute to data science solutions and the business segments to build batch and real-time insights. The role will be part of a global team supporting our Commercial Internal & Analytics OBU and BBU teams.
ABOUT OUR TEAM

We lead AstraZeneca’s transformation, fusing our digital and data capabilities with backing from the business to make it happen. Applying our skills for good, we feel a pride like no other as we positively impact patients across the world. Accelerate the business forward as we power the core functions to work better and faster. By partnering across the business, we’re leveraging our leading technologies and exploring data to make improved decisions and help the business to reach the right outcomes, quicker. Working with new technologies in our ever-changing environment we never stand still. We challenge, innovate and break away from the norm to find bold new ways of approaching the everyday. Always focused on leading the way for sustainable digital healthcare. This cannot be done alone. Empowerment and working together as one, go in hand in hand. Giving our diverse team of specialists the freedom to explore and express their ideas and views. There’s no better place to continuously expand knowledge and develop with our two-way feedback loop and novel roles. A place to realise the true impact data and digital can make.
AstraZeneca is looking for a savvy Senior Data Engineer to join our team of analytics experts in either Wilmington, DE or Gaithersburg, MD. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will be an expert data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. You will need to be self-sufficient and comfortable supporting the data needs of multiple teams.
ABOUT THE ROLE
As a senior data engineer, you will be
Onboard and translate raw data into actionable data.
Building fully automated reusable data pipelines and operationalizing them.
Maintain and build complex Data Orchestration systems
Advocate and advance modern, agile development practices and evangelize great engineering culture
You’ll take part in Kanban framework to estimate requirements and deliver tailored, complete solutions in a way that is operationally stable and sustainable.
You’ll collaborate with colleagues across IT and within Business Data teams to understand specific project requirements and provide tailored, complete solutions that fit their needs.
Brainstorming and rapid ideation of new concepts.
As a support team member, you will be responsible for taking the full ownership of the engineering issues and solving the problems effectively.
Rendering DataOps and MLOps support.
Define and promote best practices and data standards. Adherence to strict governance process and practices.
ESSENTIAL SKILLS & EXPERIENCE REQUIRED
At least 6 years of ETL experience building complex data models/pipelines/products is required.
At least two year experience in building Spark or PySpark based data pipeline orchestrations is required.
At least 3 years of hands on advanced Python programming experience required.
Experience working with Data Lakehouse or Datalake architectural solution is preferred.
Experience working with Jupiter Notebooks is preferred.
At least 6 years experience working with complex SQL based workloads is required. Experience optimizing Spark SQL is preferred.
Experience working with Databricks is a big plus.
Experience working AWS ecosystem is preferred.
Experience in at least petabyte scale data ecosystem is preferred.
Experience building and/or supporting Predictive Models is a plus.
Experience working with Commercial IQVIA datasets is a plus.
Familiarity with Kimball's data warehouse principles is required.
High School diploma or GED required, Bachelor’s degree in related field preferred
Why AstraZeneca?
At AstraZeneca, we 're dedicated to being a Great Place to Work. Where you are empowered to push the boundaries of science and unleash your entrepreneurial spirit. There’s no better place to make a difference to medicine, patients and society. An inclusive culture that champions diversity and collaboration, and always committed to lifelong learning, growth and development. We’re on an exciting journey to pioneer the future of healthcare.
AstraZeneca has taken critical steps to protecting colleagues, loved ones, and patients from COVID-19 and its variants. COVID-19 vaccination is required for all US employees, and weekly PCR COVID-19 testing is required for those who are unvaccinated or have an approved accommodation.
So, what’s next?
Are you already imagining yourself joining our team? Good, because we can’t wait to hear from you. Apply today!
Where can I find out more?
Our Social Media, Follow AstraZeneca on LinkedIn https://www.linkedin.com/company/1603/
Follow AstraZeneca on Facebook https://www.facebook.com/astrazenecacareers/
Follow AstraZeneca on Instagram https://www.instagram.com/astrazeneca_careers/?hl=en",https://www.indeed.com/rc/clk?jk=032030791cf205ea&fccid=003656df63cede32&vjs=3,"Gaithersburg, MD 20878 20878+2 locations",Senior Data Engineer,Data Engineer
251,Bosch Group,"Company Description
The Bosch Center for Artificial Intelligence provides services in AI technologies to Bosch’s business units and plants. Artificial intelligence technologies are impacting Bosch’s products and services in many domains: manufacturing, predictive maintenance, vehicle diagnostics, supply chain, large-scale simulations, etc. This is a technical position for someone who is skilled at bringing together disparate technologies to solve business problems.
Our team is responsible for streaming Bosch data to centralized analytics platforms and building data-based services for a wide variety of Bosch engineering and research teams
We are looking for a talented Data Engineer who is passionate about building fault-tolerant data services and analytics tools in the public cloud environments. Your work will be used by hundreds of Bosch engineers and have global impact by improving the quality and value of Bosch products.

Job Description
Primary Responsibilities:
Extend and improve the existing data platform which serves hundreds of users from assembly plants worldwide.
Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding.
Share knowledge by clearly articulating results and ideas to customers, managers, and key decision-makers.
Stay current with the latest research and technology and communicate your knowledge throughout the company.
Up to 5% travel may be required (post-COVID-19, domestic and international).

Qualifications
Basic Qualifications:
Bachelor's Degree in Computer Science or a related technical field.
2+ years industry experience in building and operating distributed data systems.
2+ years of programming experience in Scala, Java or Python.
1+ years industry experience with SQL.
Preferred Qualifications:
Experience with Big Data technologies, such as Apache Spark.
Proficiency in building data pipelines to stream and process datasets at low latencies.
Working experience with public clouds is a plus (Azure or similar).
Familiarity with our technical stack (Kubernetes, Prometheus, Grafana, Airflow, Jenkins)
Experience operating large data warehouses or data lakes.
Proficiency in a scripting language - bash or similar.
Additional Information
BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives
FIRST Robotics (For Inspiration and Recognition of Science and Technology)
AWIM (A World In Motion)
By choice, we are committed to a diverse workforce – EOE/Protected Veteran/Disabled.
For more information on our culture and benefits, please visit:
Culture and Benefits | Bosch in the USA",https://www.indeed.com/rc/clk?jk=b78937134b72b207&fccid=a2faf1301ac6ad4b&vjs=3,"Sunnyvale, CA 94085 94085 (East Murphy area)",Data Engineer,Data Engineer
252,KIPP SoCal Public Schools,"Who We Are
KIPP SoCal Public Schools is a network of 22 high-performing, college-preparatory public charter schools. We educate more than 9,800 students in grades pre-K through eighth in the greater Los Angeles area and San Diego. Additionally, we support approximately 5,100 alumni on their journey to and through high school and college.
What We Believe
KIPP SoCal believes that the purpose of education is for liberation. This means that we see, develop, and inspire the limitless potential in each and all of our KIPPsters.
What We Do
At KIPP SoCal we build trusting relationships that are strengthened by high expectations and collaboration alongside laughter and joy. We set ambitious goals and hold each other accountable for students achieving their greatest potential. We work together with each other and in partnership with families and communities in the active pursuit of a more equitable world. We do the right thing, even when it's not always comfortable standing up and speaking out against anti-blackness, racism, oppression, and injustice. We persevere through obstacles and treat failure as an opportunity to learn and grow.
The Opportunity
The Data Engineer is responsible for the region’s data warehouse, internal data dashboard website (KASTLE), and system integrations. This position will maintain and improve the organization’s data warehouse, data pipelines, and other data analytic solutions with a focus on effectiveness and sustainability. This role is critical to KIPP SoCal’s ability to access and analyze data to drive decisions in the best interests of our students for years to come. Success in this role requires tenacious problem solving, application of technical expertise, adaptability, strong organizational and prioritization skills, and continuous self-directed learning.
This is a telecommuting role where most of the work can be done remotely but some on-site meetings are required. Reporting to the Chief of Staff, the Data Engineer will lead work in the following areas:
Data Warehouse Management & Development
Lead the design and management of KIPP SoCal’s data warehouse, identifying new data pipelines and new database tables to create
Update and create database objects (tables, views, indexes, stored procedures, etc.) to ensure optimal performance and organization
Review database tables to ensure data accuracy
Prep and import survey (TNTP, family, student) and state (state assessments, California Dashboard, student enrollment, etc.) data into the data warehouse
Create and maintain ETL scripts to fulfill the data needs of analytics
Perform regular database monitoring duties to ensure databases, servers, and workflow management systems (Apache Airflow) remain stable, accessible, and secure
Independently analyze and solve for database or server related issues in real-time
Manage Amazon Web Services subscriptions, account provisioning, and drive technology investments to ensure that cloudware/software needs are maintained and improved over time
System Integrations
Create and update scripts that integrate information across platforms using SQL, APIs, ETL processes, and other methods
Review scripts for integrations written by Analytics Team
Manage user accounts in Whetstone via an automated script, supplemented by manual additions requested through help desk tickets
Work with stakeholders to understand source systems and gather specifications for integrations
Code Review
Lead the use of Git applications on the data team for version control and collaboration
Develop and implement the code review process for purposes of learning and improvement, code sustainability, and finding errors
Data Analytics Web Platform (KASTLE)
Review other team members’ dashboards before publishing to KASTLE, checking for data accuracy and data visualization best practices
Update context content on KASTLE (announcements, pop-ups, FAQ page, etc.)Respond to comments submitted by KASTLE users through the feedback forms on KASTLE
Manage the Analytics Team’s WordPress site, including user account creation and custom HTML/CSS and PHP widgets
Ensure that all KIPP SoCal employees have access to KASTLE
Support the development and maintenance of organizational dashboards in Tableau
Documentation
Develop and maintain technical documentation for all areas of responsibility, including data flow diagrams and data dictionaries
Create documentation of system integrations for non-technical audiences
You'll Be Qualified If You Have
Experience managing a data warehouse (back-end architecture, front-end interface, tables, views, stored procedures, etc.)
Strong knowledge and experience with SQL and at least one other programmatic language to wrangle and clean data (e.g. Python, R)
Experience with cloud computing platforms and Apache Airflow (or similar)
Strong quantitative, analytical, and critical thinking skills with demonstrated proficiency in managing large and/or challenging data sets
Knowledge of and experience working with K-12 educational data
Experience with business intelligence technologies (e.g. Tableau)
Strong communication skills and customer service orientation
Attention to detail and commitment to data integrity
Ability to juggle multiple projects simultaneously
Deep commitment to KIPP SoCal’s mission
Model the values of KIPP SoCal (lead with love, strength in the collective, equity, and excellence, curiosity and integrity)
Joining KIPP SoCal Means the Following
Working at an organization committed to the hard work necessary to ensure education is for liberation
Spending time with students and families in the best communities of LA, Compton, and San Diego
Support and care from fellow regional team members
Ongoing professional development and coaching from manager
Working with Us Means the Following Perks
Salary range of $68,000 - $85,000 per year
Employer-paid KIPP SoCal life insurance
401(k) plan with up to 6% match by KIPP SoCal
Voluntary Benefits (Pet Insurance, Supplemental Life, Accident, Short Term Disability, etc.)
Generous paid time off program includes 22 holidays, sick leave, and vacation
Other great benefits (Flexible spending account, EAP, college tuition benefits program through Guardian Dental plan, etc.)
** PLEASE NOTE: KIPP SoCal Public Schools requires that all team members be fully vaccinated against COVID-19 for the safety of our students, family, and staff. Team members may apply to seek a medical exemption. Regardless of your current vaccination status, our team can advise you on how to fulfill this important requirement so that you can start the position in a timely way. We’re here to help. **
KIPP SoCal Public Schools is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, age, national origin, veteran status, disability, sexual orientation/gender identity, or any other characteristic protected by applicable law.
KIPP SoCal strives to ensure that our careers website is accessible to all, including individuals with disabilities. If you require reasonable accommodation for any part of the application or hiring process due to a disability, please contact us. You can find our contact information on our website, http://www.kippsocal.org/. Information will be sent to a talent acquisition representative who will provide assistance to ensure appropriate consideration in the hiring process.",https://www.indeed.com/rc/clk?jk=f9cefddd0b658f26&fccid=7340fb5bffd8c4f7&vjs=3,"Remote in Los Angeles, CA 90063 90063",Data Engineer,Data Engineer
253,AstraZeneca,"Senior Data Engineer - Wilmington, DE or Gaithersburg, MD
At AstraZeneca, patients are at the heart of everything we do. A global company that pushes the boundaries of science, we all share a big purpose: to discover and develop life-changing medicines that save lives.
In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). As a Senior Data Engineer you will design, build and support data pipelines consuming data from multiple source systems and transforming it into valuable and insightful information. You will have the opportunity to work closely with business and contribute to data science solutions and the business segments to build batch and real-time insights. The role will be part of a global team supporting our Commercial Internal & Analytics OBU and BBU teams.
ABOUT OUR TEAM

We lead AstraZeneca’s transformation, fusing our digital and data capabilities with backing from the business to make it happen. Applying our skills for good, we feel a pride like no other as we positively impact patients across the world. Accelerate the business forward as we power the core functions to work better and faster. By partnering across the business, we’re leveraging our leading technologies and exploring data to make improved decisions and help the business to reach the right outcomes, quicker. Working with new technologies in our ever-changing environment we never stand still. We challenge, innovate and break away from the norm to find bold new ways of approaching the everyday. Always focused on leading the way for sustainable digital healthcare. This cannot be done alone. Empowerment and working together as one, go in hand in hand. Giving our diverse team of specialists the freedom to explore and express their ideas and views. There’s no better place to continuously expand knowledge and develop with our two-way feedback loop and novel roles. A place to realise the true impact data and digital can make.
AstraZeneca is looking for a savvy Senior Data Engineer to join our team of analytics experts in either Wilmington, DE or Gaithersburg, MD. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will be an expert data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. You will need to be self-sufficient and comfortable supporting the data needs of multiple teams.
ABOUT THE ROLE
As a senior data engineer, you will be
Onboard and translate raw data into actionable data.
Building fully automated reusable data pipelines and operationalizing them.
Maintain and build complex Data Orchestration systems
Advocate and advance modern, agile development practices and evangelize great engineering culture
You’ll take part in Kanban framework to estimate requirements and deliver tailored, complete solutions in a way that is operationally stable and sustainable.
You’ll collaborate with colleagues across IT and within Business Data teams to understand specific project requirements and provide tailored, complete solutions that fit their needs.
Brainstorming and rapid ideation of new concepts.
As a support team member, you will be responsible for taking the full ownership of the engineering issues and solving the problems effectively.
Rendering DataOps and MLOps support.
Define and promote best practices and data standards. Adherence to strict governance process and practices.
ESSENTIAL SKILLS & EXPERIENCE REQUIRED
At least 6 years of ETL experience building complex data models/pipelines/products is required.
At least two year experience in building Spark or PySpark based data pipeline orchestrations is required.
At least 3 years of hands on advanced Python programming experience required.
Experience working with Data Lakehouse or Datalake architectural solution is preferred.
Experience working with Jupiter Notebooks is preferred.
At least 6 years experience working with complex SQL based workloads is required. Experience optimizing Spark SQL is preferred.
Experience working with Databricks is a big plus.
Experience working AWS ecosystem is preferred.
Experience in at least petabyte scale data ecosystem is preferred.
Experience building and/or supporting Predictive Models is a plus.
Experience working with Commercial IQVIA datasets is a plus.
Familiarity with Kimball's data warehouse principles is required.
High School diploma or GED required, Bachelor’s degree in related field preferred
Why AstraZeneca?
At AstraZeneca, we 're dedicated to being a Great Place to Work. Where you are empowered to push the boundaries of science and unleash your entrepreneurial spirit. There’s no better place to make a difference to medicine, patients and society. An inclusive culture that champions diversity and collaboration, and always committed to lifelong learning, growth and development. We’re on an exciting journey to pioneer the future of healthcare.
AstraZeneca has taken critical steps to protecting colleagues, loved ones, and patients from COVID-19 and its variants. COVID-19 vaccination is required for all US employees, and weekly PCR COVID-19 testing is required for those who are unvaccinated or have an approved accommodation.
So, what’s next?
Are you already imagining yourself joining our team? Good, because we can’t wait to hear from you. Apply today!
Where can I find out more?
Our Social Media, Follow AstraZeneca on LinkedIn https://www.linkedin.com/company/1603/
Follow AstraZeneca on Facebook https://www.facebook.com/astrazenecacareers/
Follow AstraZeneca on Instagram https://www.instagram.com/astrazeneca_careers/?hl=en",https://www.indeed.com/rc/clk?jk=032030791cf205ea&fccid=003656df63cede32&vjs=3,"Gaithersburg, MD 20878 20878+2 locations",Senior Data Engineer,Data Engineer
254,Grote Industries,"The Data Engineer will leverage their expertise in engineering solutions that work with structured and unstructured datasets to optimize company-wide business problems, Machine Learning Models, and bring data-driven solutions into production to make real business impact. The Data Engineer will build data pipelines that drive analytic solutions. This role requires deep understanding of data architecture, data engineering and a basic understanding of data analytics & data science techniques and workflows.

Responsibility & Customer-Focused:

Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals.
Have working experience with the Big Data technologies like Azure, AWS etc.
Advanced working SQL knowledge and experience working with relational databases, working familiarity with the variety of databases.
Advanced working Python / R knowledge.
Basic understanding of Machine Learning techniques.
Solve complex data problems to deliver insights that helps the organization’s business to achieve their goals.
Create data products for analytics team.
Prepare data for prescriptive and predictive modeling.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Advice, consult, mentor and coach other data and analytic professionals on data standards and practices.
Foster a culture of sharing, re-use, design for scale stability and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve the organization’s productivity as a team.
Experience supporting and working with cross-functional teams in dynamic environment.
Partner with business analysts and solution architects to develop technical architectures for strategic enterprise projects and initiatives.
Responsible to follow and carry out corporate procedures, policies, guidelines, legal requirements, etc.

Knowledge in:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases such as DB2.
Advanced programming experience with Python or R
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Good project management and organizational skills.
Experience supporting and working with cross-functional teams.

Skills in:

Attention to detail
Complex Problem solving
Work as a productive member of a team
Work independently with direction
Multi-tasking
Organization
Excellent written and oral communication

Demonstrated ability to:

Manages multiple assignments simultaneously and demonstrate strong organizational skills
Ability to interact with all levels of employees
Work with user community and other IS personnel to gather requirements.
Work at a highly motivated level with a sense of urgency and dedication to accomplishing tasks
Ability to type on computer keyboard for extended periods of time
Adapt to frequent changes in work environment & prioritization.
Work with general or minimal supervision
Leverage existing and potential training

Experience with:

Big Data Tools
Relational & NoSQL database
Data pipeline and workflow management tools.
Azure cloud services.
Object oriented languages like Python or R.
Conflict resolution
In-depth problem identification, analysis and resolution
Prioritizing and executing multiple projects

Other:

Experience of working with ERP systems
Experience with Visual Studio

",https://www.indeed.com/rc/clk?jk=487636742f2cc591&fccid=6a235b16cffe4f73&vjs=3,"Madison, IN 47250 47250",Data Engineer,Data Engineer
255,Armament Research Company,"Job Title: Data EngineerLocation:  Washington, D.C. Region / RemoteReports To: Data Science LeadReporting to this Position: N/AWHO WE ARE:  Armaments Research Company (ARC) is a startup dedicated to bringing real time situational awareness and battlefield logistics capabilities to both the DoD and law enforcement personnel. You will be an integral part of the ARC team, and will be creating innovative and powerful IoT devices that feed data to the ARC technology stack. Our core offering is an embedded device designed in-house that pushes real time data to the cloud and back down to tactical users and central dispatch on mobile device or web browsers. Our device provides information on firearms usage, escalation of force alerting, discharge, personnel location/status, line-of-fire, and rounds of ammunition remaining. The information our device provides enables battlefield/tactical awareness for our customers that does not exist today. ARC technologies were initially developed in support of the US Defense Advanced Research Projects Agency (DARPA) and the National Science Foundation (NSF) and leverage internet-of-things (IoT) technology and machine learning (ML) to develop actionable insights and transform security responses.ARC’s core technology gathers data related to key weapon platform outputs which requires a suite of sensors that are transparent to the end user. The ARC products enable aggregated insights over big data sets that will be used to save lives on the frontline. This novel technology will be deployed at scale on future US Military and Law Enforcement weapons to enhance safety, decision making, and transparency for ground personnel.Position Description:  ARC is looking for a motivated, enthusiastic data engineer to lead the charge on data storage, organization, and exploitation. As a data-driven company, ARC takes data management seriously: your work will directly facilitate major product efforts. You will be able to bring your passion for data and problem solving to an IoT company focused on solving critical life saving problems within DoD and Law Enforcement market segments. To be successful, a willingness to communicate and collaborate is required, as you will be a central player that works across Data Science and Software organizations. Attention to detail and pride in your work is key. Your skills and knowledge will be critical to helping ARC build scalable data solutions that stay one step ahead of our customers’ needs.You will be heavily involved in the organization, ETL, warehousing, and pipelining of data at ARC. ARC’s infrastructure, algorithm development process, and test will all utilize the foundations of data architecture that you provide in collaboration with Software and Data Science. Opportunities exist to develop brand new tools and optimize existing flows, as well as forming the architecture of ARC’s data lakes in AWS S3 to uncover brand new insights from data gathered by battlefield IoT sensors.LOCATION: 4445 Willard Avenue, Chevy Chase MD 20815EDUCATION: University degree in the field of computer science or similar technical focus. An advanced degree in one of these fields is a plus.Position Responsibilities: Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexityDesigns and implements systems to ensure data quality, troubleshoots data quality issues as they ariseManages and maintains data storage in collaboration with broader engineering team to maximize data accessibility and utilityWrites unit/integration tests for data infrastructure and APIsPerforms data analysis to identify trends in data and optimize data handling for business usesIdentifies opportunities for automation/reducing manual touchpoints in data lifecycle and develops infrastructure to support such automationEssential Skills and Experience Bachelor’s Degree in Computer Science, Information Systems or similar fieldExperience with ETL best practicesExperience with or knowledge of Agile software development methodologies3+ years of Python development experience or similar2+ years of experience designing and maintaining data processing systemsExperience with applied supervised machine learning processes and data needsCritical thinking, innovation, and problem solving to succeed in unstructured problem spacesDiscipline to thrive in a virtual environment when requiredValued but not required Skills and Experience: Master / PhD degree in Computer Science, Information Systems or similar field2+ years experience with SQLFamiliarity with Amazon Web ServicesUS Security ClearanceMilitary and/or Law Enforcement experienceWork Environment:  ARC is a fast paced startup, focused on people with attention to avoiding burnout. You will be able to take advantage of an unlimited PTO policy and twelve scheduled holidays in your pursuit of mission success. This position is primarily remote, with the possibility of traveling to the office approximately once a week post-COVID-19 (or more, but we will always have a liberal work from home policy!). You'll be supported by an enthusiastic and friendly team willing to go the extra mile even when you're not physically in the same (home) office space. No clearance is required for this position, but ARC is able to sponsor your own TS clearance (or hold a current one). Work is all unclassified, enabling you to leverage all available commercial tools at your disposal.General Sign-Off:  US Citizenship is required due to ARC’s government contracts. ARC is an equal opportunity employer.An Equal Employment Employer, ARC is committed to inclusion and diversity in the workforce. All qualified candidates will be considered for employment without regard to race, ethnicity, gender, sexual orientation, national origin, genetics, age, marital or family status, protected veteran status, disability, or religion. Job Type: Full-timePay: $100,000.00 - $120,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offReferral programRetirement planVision insuranceSchedule:Monday to FridayApplication Question(s):ARC requires US Citizenship for employment due to it's government contracts. Do you currently have US Citizenship?Work Location: One location",https://www.indeed.com/company/Armament-Research-Company/jobs/Data-Engineer-61f0d7736dc9af82?fccid=788d9e4c958936f8&vjs=3,"Chevy Chase, MD 20815 20815",Data Engineer,Data Engineer
256,Saama Technologies Inc,"The Collibra Analyst/Modeler will support the development of the organization’s metadata operating model
This position will design the build and collaborate to enhance and maintain the data catalog, policy and standards library, data lineage, data quality, privacy, and risk dashboards
The Collibra Analyst/Modeler will also translate metadata usage into business-enabled processes
This role will promote data accountability, validate compliance, and track data governance effectiveness through scorecards and status reports
Create data management processes to be automated in Collibra
Perform business requirements analysis for business rules, business processes, and documents associated with population, maintenance, and completeness of metadata
Design, maintain and support application, business glossary, business intelligence, data integration, data modeling, and database management resources in Collibra
Design Collibra Privacy model and processes
Design processes and metamodel linkage between technical Business Glossaries and Collibra assets
Validate completeness of data lineage and design processes for implementation to certify Tier 1 CDEs and Reports
Provide ongoing support for metadata implementation, testing and integration
Design and maintain metadata standards and procedures
Create requirements and solution architecture for all Collibra implemented and manual processes, including issues, catalog, lineage, privacy data quality, and risk
Devise improvements to current procedures and develop methods for increasing efficiency, accuracy, and performance of metadata solutions
Designing Dashboard and Tableau integrations for Collibra platform
Conduct periodic reviews of the data catalogue to ensure compliance with data governance standards and policies
Participate in a variety of activities related to data governance, data strategy implementation, data modeling, data requirements, risk assessment, identification of data challenges, modeling, data manipulation, and/or requirement elicitation",https://www.indeed.com/rc/clk?jk=79cdafec3a78fc6b&fccid=f7cbfaf9ddc3fe8a&vjs=3,"Phoenix, AZ 85021 85021 (North Mountain area)+3 locations",Data Engineer - Collibra,Data Engineer
257,DBAce Tech LLC,"Title : AWS Data EngineerLocation : Herndon, Virginia ( Initially Remote )Duration : 12+ MonthsStart date : ASAPKey skills required for the job are: 7+ years of relevant experience.Must have AWS data AND application experiencePySpark/SparkPQL skillsBig DataStrong Python or Java skillsAWS experienceDatabase systems (SQL and NoSQL)Data warehousing solutionsETL toolsData APIs.Understanding the basics of distributed systems.Knowledge of algorithms and data structures.Desired qualificationsKnowledge of Machine Learning concepts such as KNN, Random Forest, Naïve Bayes, Neural Networks, and deploying them on Sagemaker is a plus.EDUCATIONAL REQUIREMENTBachelor degree in Computer Science, Information Systems or related fieldSkills: Big DataETLJavaPythonJob Types: Full-time, ContractPay: $60.00 - $70.00 per hourSchedule:8 hour shiftAbility to commute/relocate:Herndon, VA: Reliably commute or planning to relocate before starting work (Required)Experience:AWS: 5 years (Preferred)SQL: 5 years (Preferred)Data warehouse: 5 years (Preferred)Python: 5 years (Preferred)Work Location: One location",https://www.indeed.com/company/DBAce-Tech-LLC/jobs/Aws-Data-Engineer-c152204e9219fa73?fccid=879f52a3c537a9f8&vjs=3,"Herndon, VA","AWS Data Engineer / Herndon, Virginia ( Initially Remote )",Data Engineer
258,FATHOM,"Fathom is a Series A startup on a mission to understand and structure the world's medical data. With an engineering team out of organizations like Google, Facebook, Snap, and Twitch, we are starting by using deep learning to structure the data contained within physician notes in order to automate medical coding which is currently a process performed by 125,000 FTEs and costs the US healthcare system almost $10B annually.

We are looking for a Software Engineer, Data to work on data products that drive the core of our business. We want to work with teammates in the Bay area, who are excited about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration. If you are a data expert able to unify data, and build systems that scale from both an operational and an organizational perspective, Fathom might be the right fit for you.

Your role and responsibilities will include:
Developing data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs
Building performant and expressive interfaces to the data
Creating infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning

We are looking for a teammate with:
2+ years of development experience in a company/production setting
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark and the ability to evaluate which tools to use on the job
A unique combination of creative and analytic skills apt of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology

Bonus points if you have:
Know-how of developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python's NLTK
Expertise with wrangling healthcare data and/or HIPAA
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive",https://www.indeed.com/rc/clk?jk=b11f2f361d3aea8a&fccid=292ac9c0130b47e0&vjs=3,"San Francisco, CA","Software Engineer, Data",Data Engineer
259,Remote,"About Remote

Remote is solving global remote organizations' biggest challenge: employing anyone anywhere compliantly. We make it possible for businesses big and small to employ a global team by handling global payroll, benefits, taxes, and compliance (learn more about how it works). We're backed by A+ investors and our team is world-class, literally and figuratively, as we're all scattered around the world.

Please check out our public handbook to learn more about our culture. We encourage folks from all ethnic groups, genders, sexuality, age and abilities to apply. You can also check out independent reviews by other candidates on Glassdoor. If this job description resonates with you, we want to hear from you!

All of our positions are fully remote. You do not have to relocate to join us!

How we work

We love working async and this means you get to do your own schedule.

The Remote Engineering team uses a simples approach to productivity and task management and you can read more about it in The Remote Flow.

We empower ownership and proactivity and when in doubt default to action instead of waiting.

The position

As a data engineer, you will be the link between data producers and data consumers at Remote. You'll primarily focus on building out our data pipeline to unify our various data sources in a compliant manner. That being said, you should also be able to jump in as needed and help deliver consumable data to internal users.

Requirements

Must have (professional experience)

3+ years of experience with SQL (we use PostgreSQL at Remote)
3+ years experience with data pipeline tools, e.g. Meltano or Stitch
Experience with BI Tools e.g. Metabase, Looker, or Tableau
Key responsibilities
Maintain our data pipeline by scheduling extractors within Meltano, and handling errors.
Writing custom extractors in Python for our Meltano ELT pipeline.
Writing transformations using DBT.
Identify and address data quality issues.
Build documentation around our tools.
Work with stakeholders to get them the data they need while maintaining safe data access.
Work with stakeholders to build the necessary data pipelines to get people the data they need.
Building a clear vision for the needs of the data team, and how to improve our process.
Remote Compensation Philosophy

Remote's Total Rewards philosophy is to ensure fair unbiased compensation and fair pay along with competitive benefits in all locations in which we operate. We do not agree to or encourage cheap-labour practices and therefore pay a minimum annual salary of USD 40,000 per year, in all locations throughout the world. Actual compensation may vary based upon geographical location, experience, and/or skill level. However, it will never be below our minimum global compensation mentioned.

Benefits

You can learn more about the benefits we're offering to all internal employees at Remote by visiting our public Benefits & Perks Handbook page.

Practicals
You'll report to: Head of Automation
Team: Automation
Location: Anywhere in the World
Start date: As soon as possible
Application process
(async) Profile review
Interview with recruiter
Interview with future manager
(async) Small challenge
(async) Challenge Review
Interview with team members (no managers present)
 Prior employment verification check(s)
(async) Offer
How to apply

Please fill out the form below. Thank you!",https://www.indeed.com/rc/clk?jk=89d9adce43db4a62&fccid=dceb3c3fdc10eef9&vjs=3,"Remote in Washington, DC",Data Engineer,Data Engineer
260,Warby Parker,"At Warby Parker, we've proven that businesses can scale, be profitable, and do good in the world. Now, we're searching for a motivated Senior Data Engineer to join our in-house Engineering team to help take this mission to the next step.

In this role, you'll work collaboratively to develop and implement data models that act as the eyes and ears of a complex business that spans e-commerce, supply chain, marketing, retail, customer service, product development, and more. Your efforts to collect, organize, and share data will impact nearly all of Team Warby and will be highly visible across the organization. Sound like the job for you? Keep reading!

What you'll do:

Work collaboratively with a product manager, engineering manager, business stakeholders, and other engineers in an Agile environment
Design and implement dimensional data models that capture company-wide business processes
Ask thoughtful questions and listen carefully to get to know your business partners' vocabulary, problems, goals, and constraints
Contribute to the technical leadership of the team through writing, technical guidance, planning, mentorship, and other efforts to help guide the team to success

Who you are:

4+ years of experience designing and deploying dimensional models
Fluency in SQL
The ability to learn and work comfortably with the tools of modern software engineering—whether that's working at the command line, using version control, writing tests, or performing code reviews
Top-notch communication skills—you're an empathetic listener and a proactive communicator both in person and in writing
You are not on the Office of Inspector General's List of Excluded Individuals/Entities (LEIE)

 Some benefits and perks of working at Warby Parker:

Health, vision, and dental insurance
Flexible ""My Time"" vacation policy
Retirement savings plan with a company match
Parental leave (non-birthing parents included)
Cell phone plan reimbursement
Free eyewear, plus discounts for friends and family
And more—just ask!

About Us:

Warby Parker was founded with a mission: to inspire and impact the world with vision, purpose, and style.

We're constantly asking ourselves how we can do more and make a greater impact—and that starts by reimagining everything that a company and industry can be. We want to demonstrate that a business can scale, be profitable, and do good in the world—without charging a premium for it. And we've learned that it takes creativity, empathy, and innovation to achieve that goal.

Since the day we launched in 2010, we've pioneered ideas, designed products, and developed technologies that help people see. We offer everything our customers need for happier eyes at a price that leaves them with money in their pockets, from designer-quality glasses and contacts to eye exams and vision tests.

Ultimately, we believe in vision for all, which is why for every pair of glasses or sunglasses we sell, a pair of glasses is distributed to someone in need through our Buy a Pair, Give a Pair program. Over eight million pairs of glasses have been distributed in over 50 countries; that means eight million people now have the glasses they need to learn, work, and achieve better economic outcomes.

At Warby Parker, you can look forward to company outings and events, volunteering and learning opportunities, and just great company filled with curious, kind folks. Dreaming up and sharing ideas aren't responsibilities reserved for certain teams or leaders; the challenge (a really fun one) of innovation is on all of our shoulders. Teammates can also connect around common interests, backgrounds, and identities, no matter their home base, through our various employee resource groups. (We're happy to say that the Human Rights Campaign has named us a Best Place to Work for LGBTQ+ employees!) That sense of community and belonging keeps us excited to walk through the door every day, wherever that door may be.

We're driven to continue building a workplace, based on inclusive behaviors and equitable systems, where all employees can bring their authentic selves, feel engaged, and share their perspectives as a valued member of Team Warby. Transparency is what we're all about, and our annual Impact Report and Racial Equity Strategy lay out how we're sticking to these values.",https://www.indeed.com/rc/clk?jk=fe7fff04d934c518&fccid=9129ee712d1ee91f&vjs=3,"Remote in New York, NY 10281 10281",Staff Data Engineer (Remote Optional),Data Engineer
261,Primesoftinc,"Role: Sr Azure Data EngineerExp: 10+ yearsLocation: LA(remote for now)-need people in PSTVisa: Other Than Opt'sMOI: Phone + SkypeSkills Required: *Must have good experience and working knowledge with –*Data engineering on Azure, ADF platform*Experience in Snowflake. Financial Services experience is plus.*Develop ingestion pipelines to consume data from various sources – structured and unstructured*Design and develop the data services and reusable data frameworks for Investment Product data to enable easy access for all the consuming platforms*Design and implementation of highly complex data solution(s) end-to-end from ingestion to curation to development of the operational & analytical areas of a data platform*Play a key role in architecting future integrated data platform solutions*Co-ordinate with other internal IT teams to release code changes and as needed to resolve system / technical issues for the team*This role will be leaned on as a subject matter-expert when it comes to all-things on the investment product data*This role will be able to balance multiple initiatives with fast iteration and coordinate with the rest of the team. Therefore, a commitment to collaborative problem solving is very important*Work closely with key stakeholders to understand use cases, design solutions and lead from inception to production*Collaborate with various user experience channels enabling enhanced data consumption experience.RegardsSundeep.B732-790-5650sundeep.b (at) primesoftinc (dotcom)Job Types: Full-time, Part-time, ContractSchedule:8 hour shiftExperience:Total IT: 9 years (Preferred)Azure: 4 years (Preferred)Data warehouse: 3 years (Preferred)Work Location: One location",https://www.indeed.com/company/Primesoftinc/jobs/Azure-Data-Engineer-8599965191286ea3?fccid=0540b5b387d37378&vjs=3,"Remote in Los Angeles, CA+1 location",Azure Data Engineer,Data Engineer
262,Swish Analytics,"Company Overview

Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. We're looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumer/enterprise clients.

Job Description

The Swish Analytics team is seeking Data Engineers to have direct impact on the infrastructure and delivery of our core consumer and enterprise data offerings. We're a team passionate about accurate predictions and real-time data, and hope you find satisfaction in building new products with the latest and greatest technologies.

Duties

Architect low-latency, real-time analytics systems including raw data collection, feature development and endpoint production
Build new sports betting data products and predictions offerings Integrate large and complex real-time datasets into new consumer and enterprise products
Develop production-level predictive analytics into enterprise-grade APIs
Contribute to the design and implementation of new, fully-automated sports data delivery frameworks

Requirements

BS/BA degree in Mathematics, Computer Science, or related STEM field
Experience writing production level code
Proficiency in Python (scikit-learn, pandas, NumPy, SciPy)
Expertise in database management, preferably SQL
Experience with NoSql databases (Cassandra, MongoDb)
Experience building ETL pipelines
Experience with data mining and analysis
Experience utilizing REST APIs
Experience with version control (git), continuous integration and deployment, shell scripting, and cloud-computing infrastructures (AWS)
Experience with web scraping and cleaning unstructured data
Experience with Distributed Computing
A strong interest for US sports and sports betting. You love sports, particularly the NFL, NBA, MLB and NHL, and can use your knowledge of the sport to inform your work with complex datasets
Benefits
Health: Medical, Dental, and Vision coverage for all
Vacation: Flexible PTO for whatever life brings your way
Sports: Watch parties, World Series tickets, pickup basketball...and everything in between",https://www.indeed.com/rc/clk?jk=6c1da2915bb93dde&fccid=e632785cd75868e9&vjs=3,"Remote in San Francisco, CA 94133 94133",Data Engineer,Data Engineer
263,EVOTEK,"EVOTEK™ is North America's premier enabler of digital business with a focus on innovation. With technology offerings in data center and cloud, EVOTEK is uniquely equipped to enable customers with the industry shift from traditional IT computing to secure multi-cloud. With services practices in cybersecurity, mobility, platform engineering and AIOps, EVOTEK is moving up the value chain, closer to the part of digital business that matters most. EVOTEK was named to Inc. Magazine’s “Best Places to Work” in 2018 and 2020. For five consecutive years, from 2016-2020, EVOTEK was listed in The San Diego Business Journal's “Best Places to Work” and recognized in CRN's “Solution Provider 500” list, CRN's “Next-Generation 250” list, CRN’s “Triple Crown” and highlighted as CRN's “Top 150 Growth Companies”, holding the #1 spot in 2017 as the fastest growing system integrator in the country. In 2020, EVOTEK was named to the Inc. 5000 list as one of the fastest growing companies in America.As a Data Center Solutions Engineer this individual will work with our growing and expanding team, building, and maintaining relationships with various domains, practices, and customers. The ideal candidate will combine sales and technical skills to address client needs, drive business outcomes and partner with internal teams to help build customer relationships. They will have a seasoned background in a variety of storage platforms, possessing excellent customer consulting abilities driving advisement, implementation, and management of solutions.Responsible for driving pre-sales activities and the ability to drive business opportunities from opportunity identification, scoping process, and booking.Work closely with clients to fully understand internal needs and concerns offering solutions and options to remediate their environments.Build and maintain relationships with clients and prospects while designing, communicating, and advisement of company solutions.Communicate clearly, and concisely the capabilities of data center offerings, advisory services, and delivery capabilities.Assists the sales team with design, sales engineering, and proposal assimilation.Responsible and capable of speaking in front of large groups, as well as delivering excellent and relevant information to clientele and technical teams.Responsible for creating presentations, appropriate documentation and whiteboarding abilities.Experience in disseminating information to customers and internally (i.e., training, cross training, customer education).Confidence and ability to lead project launch and project design needs effectively and independently.Responsible for design lifecycle creation and management of standard to complex Data Center solutions.Ability to lead complex projects to completion on-time and on-budget.Actively participate and support technical teams throughout all phases of a project including project launch workshops, kickoff calls and meetings, as well as lessons learned.RequirementsPrior experience working with clients in a pre-sale capacity providing guidance around data center solutions.Prior experience managing proposal processes including SOW creation.Ability to clearly articulate company’s value, and solutions to client stakeholders.Ability to research and provide updates on current technology solutions.Background/familiarity with NetApp, Cohesity, Nutanix, Pure, and others.Background/familiarity with Dell, HPE, Intel, SuperMicro and others.Understanding of Intel/AMD processor generations and competitive landscape.Knowledge of compute storage platforms to include but not limited to: HDD, SSD, NVME, and Optane.Proof of Concept capabilities and a broader skillset in the Data Center space.Ability to capture inputs, results and present findings to the customers and internal teams.Understanding of backup, virtualization technologies, and SAN/NAS.Ideally this person has had experience in the past building a program, compute focused or otherwise.Certifications from NetApp, Dell, HPE, or others preferred but not requiredBenefitsStrong company culture.Competitive compensation.Benefits package that includes 100% paid medical, dental and vision for the employee.401(k) with employer match.Flexible PTO policy.Flexible working arrangements.Annual company overnight retreat (employee + significant other).Equal Opportunity EmployerEVOTEK believes that everyone has the ability to make an impact, and we are proud to be an equal opportunity employer committed to providing employment opportunity regardless of sex, race, creed, color, gender, religion, marital status, domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition, sexual orientation, pregnancy, military or veteran status, citizenship status, and genetic information.Job Type: Full-timePay: $170,000.00 - $185,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceHealth insurancePaid time offVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payWork Location: Remote",https://www.indeed.com/company/3RP/jobs/Data-Center-Solution-Engineer-e6faaf4de5316793?fccid=8499db643ed1ba18&vjs=3,Remote,Data Center Solutions Engineer (Pre-Sales),Data Engineer
264,Hoverstate,"Job Summary
This is a remote, work from home role. We're looking for a Data Engineer who is ready for an amazing opportunity to design, develop, and deliver our data platform from the ground up.
Responsibilities and Duties
Design, develop, and deliver/implement data solutions to include: architecture design, prototyping of concepts to proof of concept, development of standards, design, and development of test plans, code, and module design, development and testing, data solution debugging, and design and implementation
Optimize existing data pipelines and create new ones to manage data sets while learning the platforms from which we extract data; Develop and maintain third-party API processes for data pipelines
Design, implement, and manage data warehouse solutions
Support and maintain data and database systems to meet business delivery specifications and needs.
Document structure and processes
Work with Data Science to create and launch new models Work with various business and engineering teams to ensure reliable, scalable, robust architecture for our data platform
Qualifications and Skills
Experience programming in one or more general-purpose programming languages, including but not limited to Python, Java, or Javascript Experience programming in SQL (No-SQL is a plus)
Experience with Snowflake or other managed solutions such as BigQuery, Redshift
Expertise in dimensional data modeling skills with experience tuning and optimizing
Experience designing and operating data services & data pipelines Familiarity with data tooling (e.g. Airflow/MWAA, BigQuery, HIVE) or similar cloud big data tooling
Bachelor’s Degree in Computer Engineering, Computer Science, related field, or similar experience
5+ Years of Software Engineering Experience with a focus on Data
SnowPro Certification is preferred
Company Overview
Hoverstate is a Digital Consultancy with offices in California (Los Angeles, Oakland), Texas (Lubbock), Utah (Lehi) and Italy (Milan, Turin) specializing in Health and Life Sciences. We are a digital consultancy specializing in: delivering customer-centric transformation, innovative digital products, forward-thinking technology strategies, leveraging compliant technologies, developing rock-solid code, testing our products with a robust QA team/automation, then backing it all with marketing support. If there is a problem to be solved in the digital space, Hoverstate can solve it.
Since 2008, Hoverstate has counted world-renowned health insurance and life sciences companies among our roster of clients. Our focused expertise keeps our clients on top of changes in policy, privacy rules and usability for their customers. Our mission is to continually create best-in-class user experiences through our robust design and testing practices, backed by some pretty impressive feats of technology, to bring this slow-to-innovate sector into the 21st century.
Hoverstate + Pega: A Long History. Hoverstate has been a Certified Pega partner since 2013. Every day we leverage Pega’s adaptable architecture to deliver applications to our clients in record time while creating outstanding user experiences with meaningful business results. Thanks to our extensive experience, and proven ability to execute Pega projects the ""Pega Way"", we were chosen as one of the first Pega Ventures funded partners in 2017. That means we have the full backing and endorsement of Pegasystems.
What Hoverstate Offers
Pay | competitive salary and bonus plan
Health | generous healthcare benefits
PTO | unlimited vacation, all holidays, sick and bereavement leave
401K | company contributions
Relocation | relocation assistance
Continuing Education | yearly stipend for your choice of ongoing education, conference attendance, training or certification
Growth | join Hoverstate for the opportunity to join a rapidly growing, well-established business that offers accelerated recognition and rewards, where you’ll see your contributions and great ideas directly impact our business
Environment | fun, friendly, collaborative, innovative, fast, yet professional!
Networking | regular, personal interaction with the Hoverstate senior leadership team and senior leadership of our clients.
Zfr0YQakLv",https://www.indeed.com/rc/clk?jk=7e57fdb1a7585d41&fccid=ab85f9ea8ff6d0ac&vjs=3,+2 locationsRemote,Lead Snowflake Data Engineer,Data Engineer
265,Experian,"Company Description
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been name in the 100 “World’s Most Innovative Companies” by Forbes Magazine. In this era of disruptive digital transformation, delivering new and innovative customer experiences is a top priority for businesses worldwide. At Experian, a worldwide decisioning leader, we make this a reality by uniting data, advanced analytics and decision strategies with real-time operational execution and strong decision governance to deliver meaningful customer outcomes and positive business results.

Job Description

Carry out the extraction, formulation and manipulation of data to create the structured samples necessary to address business requirement.
Interpret data specifications
Ensure quality and accuracy of own work
Produced analysis is accurate and completed to agreed timescales, resulting in positive feedback from client/customer
Follow Business Line procedures and processes throughout the data extraction, formulation and manipulation process, ensuring all work is produced to the agreed specification, and meets all requirements
Take responsibility for the production of high quality results documentation
This role is responsible for carrying out elements of data manipulation and reporting, quality assurance, documentation and internal/client communication. The role holder is able to work independently.
Identify and resolve data problems, producing appropriate analysis to verify results
Identify potential gaps in own technical/business knowledge that might compromise the deadlines. Work with Project Managers or Consultant to plan timely steps to fill gaps
Challenge existing processes and recommend improvements
Ensure the quality of all data sent to clients/customers (e.g. project development samples; retrospective bureau data and Bureau Score data (if applicable))
Analyze and confirm the integrity of source data to be evaluated
Input into the review and assessment of client/customer requirements to produce a data project specification
With direction, produce data specifications
Liaise with other internal and external project management groups to ensure project is delivered as part of complete solution
Input into the accurate assessment of the work that needs to be undertaken to define and maintain a project plan
Plan, organize and complete own workload to meet project commitments within agreed schedules
Provide pro-actively status reports and progress updates to Project Manager or Consultant as required, for all projects/tasks
Contribute to the preparation of regular project status reports to both internal and external parties.
Proactively identify potential risks in achieving project timescales, taking pro-active action to minimize risk
Recognize when issues or problems need to be escalated
Maintain accurate time recording to projects in the appropriate internal time tracker software
Effectively use project management/planning tools
Proactively use time recording data to identify inefficiencies in project work and recommend improvements
Implement appropriate quality control procedures for all analysis reporting and scoring
Reduce error rate and re-run frequency. Meeting both the analytical and business objective

Qualifications

Willingness to learn data extraction, formulation and manipulation methods
Willingness to develop strong investigation skills. Ability to identify and correct data problems
PC/MF literate with experience in IT packages. Knowledge of statistical (e.g., SAS) packages desirable
Knowledge of Excel VBA
An innovative and inquisitive mind, focused on addressing and solving data problems
Common sense/logical approach to mathematical reasoning
Strong communication skills, including proficiency in writing reports and presenting technical work
Strong personal planning and time management skills
Strong interpersonal skills
Strong collaboration and teamwork
Commercially focused
Language in relevant marketplace
English language good knowledge
Additional Information

During this pandemic, all Experian employees are working remotely. Once it’s safe to do so, we’ll slowly return to our offices however we are in no rush to do this. The safety of our employees is of utmost importance.
Competitive pay and comprehensive benefits package
Flexible work schedule and relaxed dress code
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU and it ensures that we live what we believe.
Experian U.S. employees are required to be fully vaccinated for COVID-19.
Experian Careers - Creating a better tomorrow together
Find out what its like to work for Experian by clicking here",https://www.indeed.com/rc/clk?jk=314cbd35d87dd902&fccid=75a3a5a15b202084&vjs=3,"Costa Mesa, CA 92626 92626+1 location",Data Engineer,Data Engineer
266,IT Alliances Inc,"HiThis is Prince from IT-Alliances Inc I have a job requirement on full-time/permanent. Interested can send me the updated resume.Title: - Data Engineer.Location: - Dallas, TX.Duration: - Fulltime/Permanent.Job Description:Strong background in data processing & software engineering and can build high-quality, scalable data oriented products.Industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, EMR, etc..) for building efficient, large-scale data pipelines.Strong Software Engineering experience with in depth understanding of Python, Scala, Java or equivalentStrong understanding of data architecture, modeling and infrastructureExperience with building workflows (ETL pipelines)Experience with SQL and optimizing queries.Problem solver with attention to detail who can see complex problems in the data space through end to endWillingness to work in a fast paced environmentMS/BS in Computer Science or relevant industry experience.Strongly recommended (but optional)Experience building scalable applications on the Cloud (Amazon AWS, Google Cloud, etc..)Experience building stream-processing applications (Spark streaming, Apache-Flink, Kafka, etc..)Job Summary & ResponsibilitiesDesign and develop data ingest and transform processesDevelop data models to provide standardized reporting solutions to the firmDevelop automation, governance and reporting solutions to provide firm and regulatory mandated controlsWork as part of a global team using Agile software methodologiesPartner with Marcus risk, product, acquisition and servicing teamsUse Marcus data to drive change throughout the Marcus businessSkills And Experience We Are Looking For: -Bachelor’s degree or equivalent required.Minimum 5 years of relevant professional experience.Experience with SQL and relational databases.Self-starter, motivated, and good communication skills.Strong sense of ownership and driven to manage tasks to completionPreferred Qualifications: -Experience with data modellingProficiency in Python, Spark and the Hadoop ecosystemJob Type: Full-timeSalary: $100,000.00 - $140,000.00 per yearSupplemental Pay:Bonus payWork Location: RemoteSpeak with the employer+91 732.666.0012",https://www.indeed.com/company/IT-Alliances-Inc/jobs/Data-Engineer-2485cff79ebe217c?fccid=1703af95672f1f50&vjs=3,Remote,Data Engineer,Data Engineer
